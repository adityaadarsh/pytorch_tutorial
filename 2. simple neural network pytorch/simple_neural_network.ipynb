{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Course Reference: https://learn.microsoft.com/en-us/training/modules/intro-machine-learning-pytorch/\n",
    "#### Learning objectives\n",
    "\n",
    "In this module you will:\n",
    "\n",
    "* Learn how create a asimple NN using pytorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transformations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Fully Connected Network\n",
    "\n",
    "We define our neural network by subclassing nn.Module, and initialize the neural network layers in __init__. Every nn.Module subclass implements the operations on input data in the forward method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, input_size, n_classes):\n",
    "        super(NN, self).__init__()\n",
    "\n",
    "        self.fc1= nn.Linear(input_size, 50)\n",
    "        self.fc2= nn.Linear(50, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60, 10])\n"
     ]
    }
   ],
   "source": [
    "# check the network graph\n",
    "model = NN(784, 10)\n",
    "\n",
    "# create a random variable and pass it to the model to check the network graph\n",
    "x = torch.randn(60, 784)\n",
    "print(model(x).shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 784\n",
    "n_class = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and load the data from pytorch sample datasets\n",
    "# https://pytorch.org/vision/0.8/datasets.html\n",
    "\n",
    "train_dataset = datasets.MNIST(root='../dataset/', train=True, transform=transformations.ToTensor(), download=True)\n",
    "train_datloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels batch shape: torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 4\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Understand the dataloader\n",
    "plt.title('sample image')\n",
    "# Display image and label.\n",
    "train_features, train_labels = next(iter(train_datloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset shape: torch.Size([60000, 28, 28])\n",
      "['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n"
     ]
    }
   ],
   "source": [
    "# train data shape and class labels\n",
    "print('train_dataset shape:', train_dataset.data.shape)\n",
    "print(train_dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_dataset shape: torch.Size([10000, 28, 28])\n",
      "['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n"
     ]
    }
   ],
   "source": [
    "test_dataset = datasets.MNIST(root='../dataset/', train=False, transform=transformations.ToTensor(), download=True)\n",
    "test_datloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print('test_dataset shape:', test_dataset.data.shape)\n",
    "print(test_dataset.classes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN(\n",
      "  (fc1): Linear(in_features=784, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NN(input_dim, n_class)\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise Loss And Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps to train the model\n",
    "1. For each epoch, iterate through the batch\n",
    "2. For each batch\n",
    "    * feed forward the input and target data of train to the model\n",
    "    * Calculate the loss and score\n",
    "    * Backpropogate the loss\n",
    "    * optimise the loss using optimiser() (gradient descent is one such optimiser) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch: 0, loss: 2.323420286178589\n",
      "epoch: 0, batch: 1, loss: 2.295718193054199\n",
      "epoch: 0, batch: 2, loss: 2.230379819869995\n",
      "epoch: 0, batch: 3, loss: 2.236008644104004\n",
      "epoch: 0, batch: 4, loss: 2.230621814727783\n",
      "epoch: 0, batch: 5, loss: 2.151639461517334\n",
      "epoch: 0, batch: 6, loss: 2.1377763748168945\n",
      "epoch: 0, batch: 7, loss: 2.1138293743133545\n",
      "epoch: 0, batch: 8, loss: 2.072591543197632\n",
      "epoch: 0, batch: 9, loss: 2.054612874984741\n",
      "epoch: 0, batch: 10, loss: 2.056401014328003\n",
      "epoch: 0, batch: 11, loss: 2.0291805267333984\n",
      "epoch: 0, batch: 12, loss: 1.9473990201950073\n",
      "epoch: 0, batch: 13, loss: 1.9610120058059692\n",
      "epoch: 0, batch: 14, loss: 1.8364129066467285\n",
      "epoch: 0, batch: 15, loss: 1.8503538370132446\n",
      "epoch: 0, batch: 16, loss: 1.821225881576538\n",
      "epoch: 0, batch: 17, loss: 1.7982161045074463\n",
      "epoch: 0, batch: 18, loss: 1.7077645063400269\n",
      "epoch: 0, batch: 19, loss: 1.775176763534546\n",
      "epoch: 0, batch: 20, loss: 1.776419758796692\n",
      "epoch: 0, batch: 21, loss: 1.7028098106384277\n",
      "epoch: 0, batch: 22, loss: 1.6737171411514282\n",
      "epoch: 0, batch: 23, loss: 1.6231111288070679\n",
      "epoch: 0, batch: 24, loss: 1.5296591520309448\n",
      "epoch: 0, batch: 25, loss: 1.6173357963562012\n",
      "epoch: 0, batch: 26, loss: 1.572590947151184\n",
      "epoch: 0, batch: 27, loss: 1.4923460483551025\n",
      "epoch: 0, batch: 28, loss: 1.57818603515625\n",
      "epoch: 0, batch: 29, loss: 1.3595194816589355\n",
      "epoch: 0, batch: 30, loss: 1.4923819303512573\n",
      "epoch: 0, batch: 31, loss: 1.4710568189620972\n",
      "epoch: 0, batch: 32, loss: 1.377854824066162\n",
      "epoch: 0, batch: 33, loss: 1.3702796697616577\n",
      "epoch: 0, batch: 34, loss: 1.3208526372909546\n",
      "epoch: 0, batch: 35, loss: 1.3062814474105835\n",
      "epoch: 0, batch: 36, loss: 1.1868841648101807\n",
      "epoch: 0, batch: 37, loss: 1.3426955938339233\n",
      "epoch: 0, batch: 38, loss: 1.3343254327774048\n",
      "epoch: 0, batch: 39, loss: 1.1809431314468384\n",
      "epoch: 0, batch: 40, loss: 1.156886100769043\n",
      "epoch: 0, batch: 41, loss: 1.2870128154754639\n",
      "epoch: 0, batch: 42, loss: 1.2886509895324707\n",
      "epoch: 0, batch: 43, loss: 1.258434534072876\n",
      "epoch: 0, batch: 44, loss: 1.058054804801941\n",
      "epoch: 0, batch: 45, loss: 1.1143133640289307\n",
      "epoch: 0, batch: 46, loss: 1.102681040763855\n",
      "epoch: 0, batch: 47, loss: 0.885812520980835\n",
      "epoch: 0, batch: 48, loss: 1.0161662101745605\n",
      "epoch: 0, batch: 49, loss: 1.0657981634140015\n",
      "epoch: 0, batch: 50, loss: 0.997428834438324\n",
      "epoch: 0, batch: 51, loss: 1.0272269248962402\n",
      "epoch: 0, batch: 52, loss: 0.9583359360694885\n",
      "epoch: 0, batch: 53, loss: 0.9636778831481934\n",
      "epoch: 0, batch: 54, loss: 0.8638226985931396\n",
      "epoch: 0, batch: 55, loss: 1.0218489170074463\n",
      "epoch: 0, batch: 56, loss: 0.9064701795578003\n",
      "epoch: 0, batch: 57, loss: 0.8368288278579712\n",
      "epoch: 0, batch: 58, loss: 1.0136566162109375\n",
      "epoch: 0, batch: 59, loss: 0.7623704671859741\n",
      "epoch: 0, batch: 60, loss: 0.8255118727684021\n",
      "epoch: 0, batch: 61, loss: 0.7326850295066833\n",
      "epoch: 0, batch: 62, loss: 0.8892238140106201\n",
      "epoch: 0, batch: 63, loss: 0.9194903373718262\n",
      "epoch: 0, batch: 64, loss: 0.7581034898757935\n",
      "epoch: 0, batch: 65, loss: 0.7830852270126343\n",
      "epoch: 0, batch: 66, loss: 0.7078776955604553\n",
      "epoch: 0, batch: 67, loss: 0.8914394974708557\n",
      "epoch: 0, batch: 68, loss: 0.7358497977256775\n",
      "epoch: 0, batch: 69, loss: 0.7238073945045471\n",
      "epoch: 0, batch: 70, loss: 0.7939420938491821\n",
      "epoch: 0, batch: 71, loss: 0.8641967177391052\n",
      "epoch: 0, batch: 72, loss: 0.6305825114250183\n",
      "epoch: 0, batch: 73, loss: 0.6706076860427856\n",
      "epoch: 0, batch: 74, loss: 0.8854058384895325\n",
      "epoch: 0, batch: 75, loss: 0.7695334553718567\n",
      "epoch: 0, batch: 76, loss: 0.7552183270454407\n",
      "epoch: 0, batch: 77, loss: 0.5901557207107544\n",
      "epoch: 0, batch: 78, loss: 0.6853863000869751\n",
      "epoch: 0, batch: 79, loss: 0.7250466346740723\n",
      "epoch: 0, batch: 80, loss: 0.5650571584701538\n",
      "epoch: 0, batch: 81, loss: 0.5947922468185425\n",
      "epoch: 0, batch: 82, loss: 0.7424746155738831\n",
      "epoch: 0, batch: 83, loss: 0.6703943014144897\n",
      "epoch: 0, batch: 84, loss: 0.5663454532623291\n",
      "epoch: 0, batch: 85, loss: 0.4660135805606842\n",
      "epoch: 0, batch: 86, loss: 0.5865134000778198\n",
      "epoch: 0, batch: 87, loss: 0.6517990231513977\n",
      "epoch: 0, batch: 88, loss: 0.7150551676750183\n",
      "epoch: 0, batch: 89, loss: 0.6295667886734009\n",
      "epoch: 0, batch: 90, loss: 0.588947057723999\n",
      "epoch: 0, batch: 91, loss: 0.7875666618347168\n",
      "epoch: 0, batch: 92, loss: 0.5987852215766907\n",
      "epoch: 0, batch: 93, loss: 0.8196829557418823\n",
      "epoch: 0, batch: 94, loss: 0.7864100933074951\n",
      "epoch: 0, batch: 95, loss: 0.5897790789604187\n",
      "epoch: 0, batch: 96, loss: 0.9092787504196167\n",
      "epoch: 0, batch: 97, loss: 0.6095929145812988\n",
      "epoch: 0, batch: 98, loss: 0.425114244222641\n",
      "epoch: 0, batch: 99, loss: 0.587458074092865\n",
      "epoch: 0, batch: 100, loss: 0.5990225672721863\n",
      "epoch: 0, batch: 101, loss: 0.5986756086349487\n",
      "epoch: 0, batch: 102, loss: 0.5421567559242249\n",
      "epoch: 0, batch: 103, loss: 0.49391669034957886\n",
      "epoch: 0, batch: 104, loss: 0.6163917779922485\n",
      "epoch: 0, batch: 105, loss: 0.6507806777954102\n",
      "epoch: 0, batch: 106, loss: 0.5345987677574158\n",
      "epoch: 0, batch: 107, loss: 0.6687564253807068\n",
      "epoch: 0, batch: 108, loss: 0.5434464812278748\n",
      "epoch: 0, batch: 109, loss: 0.5132619738578796\n",
      "epoch: 0, batch: 110, loss: 0.504311203956604\n",
      "epoch: 0, batch: 111, loss: 0.5018064975738525\n",
      "epoch: 0, batch: 112, loss: 0.5458673238754272\n",
      "epoch: 0, batch: 113, loss: 0.5072839260101318\n",
      "epoch: 0, batch: 114, loss: 0.4782358705997467\n",
      "epoch: 0, batch: 115, loss: 0.6122207045555115\n",
      "epoch: 0, batch: 116, loss: 0.5537896156311035\n",
      "epoch: 0, batch: 117, loss: 0.6153919696807861\n",
      "epoch: 0, batch: 118, loss: 0.6021637916564941\n",
      "epoch: 0, batch: 119, loss: 0.412556916475296\n",
      "epoch: 0, batch: 120, loss: 0.6021324396133423\n",
      "epoch: 0, batch: 121, loss: 0.5612131953239441\n",
      "epoch: 0, batch: 122, loss: 0.5920607447624207\n",
      "epoch: 0, batch: 123, loss: 0.46680185198783875\n",
      "epoch: 0, batch: 124, loss: 0.4649934768676758\n",
      "epoch: 0, batch: 125, loss: 0.42653024196624756\n",
      "epoch: 0, batch: 126, loss: 0.5476415753364563\n",
      "epoch: 0, batch: 127, loss: 0.5499188899993896\n",
      "epoch: 0, batch: 128, loss: 0.5482203364372253\n",
      "epoch: 0, batch: 129, loss: 0.46510228514671326\n",
      "epoch: 0, batch: 130, loss: 0.5942763090133667\n",
      "epoch: 0, batch: 131, loss: 0.43872931599617004\n",
      "epoch: 0, batch: 132, loss: 0.601411759853363\n",
      "epoch: 0, batch: 133, loss: 0.40408486127853394\n",
      "epoch: 0, batch: 134, loss: 0.3454321026802063\n",
      "epoch: 0, batch: 135, loss: 0.43996480107307434\n",
      "epoch: 0, batch: 136, loss: 0.5107191801071167\n",
      "epoch: 0, batch: 137, loss: 0.4675716161727905\n",
      "epoch: 0, batch: 138, loss: 0.5466050505638123\n",
      "epoch: 0, batch: 139, loss: 0.3564381003379822\n",
      "epoch: 0, batch: 140, loss: 0.4446169435977936\n",
      "epoch: 0, batch: 141, loss: 0.4786858558654785\n",
      "epoch: 0, batch: 142, loss: 0.5800370573997498\n",
      "epoch: 0, batch: 143, loss: 0.3325164318084717\n",
      "epoch: 0, batch: 144, loss: 0.4561229944229126\n",
      "epoch: 0, batch: 145, loss: 0.34858185052871704\n",
      "epoch: 0, batch: 146, loss: 0.5361301898956299\n",
      "epoch: 0, batch: 147, loss: 0.5183570384979248\n",
      "epoch: 0, batch: 148, loss: 0.43820109963417053\n",
      "epoch: 0, batch: 149, loss: 0.494434118270874\n",
      "epoch: 0, batch: 150, loss: 0.4555983245372772\n",
      "epoch: 0, batch: 151, loss: 0.4882490038871765\n",
      "epoch: 0, batch: 152, loss: 0.5523474216461182\n",
      "epoch: 0, batch: 153, loss: 0.6322342753410339\n",
      "epoch: 0, batch: 154, loss: 0.39764732122421265\n",
      "epoch: 0, batch: 155, loss: 0.45822158455848694\n",
      "epoch: 0, batch: 156, loss: 0.43656712770462036\n",
      "epoch: 0, batch: 157, loss: 0.4951198399066925\n",
      "epoch: 0, batch: 158, loss: 0.546647310256958\n",
      "epoch: 0, batch: 159, loss: 0.4188395142555237\n",
      "epoch: 0, batch: 160, loss: 0.41024118661880493\n",
      "epoch: 0, batch: 161, loss: 0.4363713264465332\n",
      "epoch: 0, batch: 162, loss: 0.7263841032981873\n",
      "epoch: 0, batch: 163, loss: 0.36738795042037964\n",
      "epoch: 0, batch: 164, loss: 0.40508410334587097\n",
      "epoch: 0, batch: 165, loss: 0.4597705900669098\n",
      "epoch: 0, batch: 166, loss: 0.4511249363422394\n",
      "epoch: 0, batch: 167, loss: 0.3987058401107788\n",
      "epoch: 0, batch: 168, loss: 0.41947925090789795\n",
      "epoch: 0, batch: 169, loss: 0.3133141100406647\n",
      "epoch: 0, batch: 170, loss: 0.4790661334991455\n",
      "epoch: 0, batch: 171, loss: 0.7232482433319092\n",
      "epoch: 0, batch: 172, loss: 0.41352149844169617\n",
      "epoch: 0, batch: 173, loss: 0.4279542565345764\n",
      "epoch: 0, batch: 174, loss: 0.5605654716491699\n",
      "epoch: 0, batch: 175, loss: 0.39293521642684937\n",
      "epoch: 0, batch: 176, loss: 0.3893732726573944\n",
      "epoch: 0, batch: 177, loss: 0.555789589881897\n",
      "epoch: 0, batch: 178, loss: 0.3583572208881378\n",
      "epoch: 0, batch: 179, loss: 0.4005548655986786\n",
      "epoch: 0, batch: 180, loss: 0.5406959652900696\n",
      "epoch: 0, batch: 181, loss: 0.34096774458885193\n",
      "epoch: 0, batch: 182, loss: 0.385545015335083\n",
      "epoch: 0, batch: 183, loss: 0.35781368613243103\n",
      "epoch: 0, batch: 184, loss: 0.3610575497150421\n",
      "epoch: 0, batch: 185, loss: 0.45043784379959106\n",
      "epoch: 0, batch: 186, loss: 0.26167839765548706\n",
      "epoch: 0, batch: 187, loss: 0.41076961159706116\n",
      "epoch: 0, batch: 188, loss: 0.37343430519104004\n",
      "epoch: 0, batch: 189, loss: 0.3048650324344635\n",
      "epoch: 0, batch: 190, loss: 0.6911061406135559\n",
      "epoch: 0, batch: 191, loss: 0.27974388003349304\n",
      "epoch: 0, batch: 192, loss: 0.5579227805137634\n",
      "epoch: 0, batch: 193, loss: 0.4042685031890869\n",
      "epoch: 0, batch: 194, loss: 0.46142590045928955\n",
      "epoch: 0, batch: 195, loss: 0.4656146764755249\n",
      "epoch: 0, batch: 196, loss: 0.28236106038093567\n",
      "epoch: 0, batch: 197, loss: 0.22220763564109802\n",
      "epoch: 0, batch: 198, loss: 0.35460585355758667\n",
      "epoch: 0, batch: 199, loss: 0.27383726835250854\n",
      "epoch: 0, batch: 200, loss: 0.37457478046417236\n",
      "epoch: 0, batch: 201, loss: 0.5008593797683716\n",
      "epoch: 0, batch: 202, loss: 0.3650398850440979\n",
      "epoch: 0, batch: 203, loss: 0.3065654933452606\n",
      "epoch: 0, batch: 204, loss: 0.26035502552986145\n",
      "epoch: 0, batch: 205, loss: 0.37804651260375977\n",
      "epoch: 0, batch: 206, loss: 0.34714171290397644\n",
      "epoch: 0, batch: 207, loss: 0.43503114581108093\n",
      "epoch: 0, batch: 208, loss: 0.33232206106185913\n",
      "epoch: 0, batch: 209, loss: 0.3559863865375519\n",
      "epoch: 0, batch: 210, loss: 0.45029446482658386\n",
      "epoch: 0, batch: 211, loss: 0.5213592648506165\n",
      "epoch: 0, batch: 212, loss: 0.7016337513923645\n",
      "epoch: 0, batch: 213, loss: 0.39977124333381653\n",
      "epoch: 0, batch: 214, loss: 0.28254178166389465\n",
      "epoch: 0, batch: 215, loss: 0.36625444889068604\n",
      "epoch: 0, batch: 216, loss: 0.26072344183921814\n",
      "epoch: 0, batch: 217, loss: 0.6370715498924255\n",
      "epoch: 0, batch: 218, loss: 0.48535779118537903\n",
      "epoch: 0, batch: 219, loss: 0.45798084139823914\n",
      "epoch: 0, batch: 220, loss: 0.3801266551017761\n",
      "epoch: 0, batch: 221, loss: 0.4760490953922272\n",
      "epoch: 0, batch: 222, loss: 0.3932528495788574\n",
      "epoch: 0, batch: 223, loss: 0.4189908802509308\n",
      "epoch: 0, batch: 224, loss: 0.41201671957969666\n",
      "epoch: 0, batch: 225, loss: 0.43975597620010376\n",
      "epoch: 0, batch: 226, loss: 0.4136403799057007\n",
      "epoch: 0, batch: 227, loss: 0.23111847043037415\n",
      "epoch: 0, batch: 228, loss: 0.34595435857772827\n",
      "epoch: 0, batch: 229, loss: 0.678500235080719\n",
      "epoch: 0, batch: 230, loss: 0.3490484654903412\n",
      "epoch: 0, batch: 231, loss: 0.3750278949737549\n",
      "epoch: 0, batch: 232, loss: 0.3096773326396942\n",
      "epoch: 0, batch: 233, loss: 0.5843486189842224\n",
      "epoch: 0, batch: 234, loss: 0.434481680393219\n",
      "epoch: 0, batch: 235, loss: 0.19431886076927185\n",
      "epoch: 0, batch: 236, loss: 0.41604429483413696\n",
      "epoch: 0, batch: 237, loss: 0.5091897249221802\n",
      "epoch: 0, batch: 238, loss: 0.28228139877319336\n",
      "epoch: 0, batch: 239, loss: 0.34570419788360596\n",
      "epoch: 0, batch: 240, loss: 0.3442392349243164\n",
      "epoch: 0, batch: 241, loss: 0.5536143183708191\n",
      "epoch: 0, batch: 242, loss: 0.4530504047870636\n",
      "epoch: 0, batch: 243, loss: 0.3677230477333069\n",
      "epoch: 0, batch: 244, loss: 0.37923094630241394\n",
      "epoch: 0, batch: 245, loss: 0.476938933134079\n",
      "epoch: 0, batch: 246, loss: 0.3032124936580658\n",
      "epoch: 0, batch: 247, loss: 0.4305485785007477\n",
      "epoch: 0, batch: 248, loss: 0.28774717450141907\n",
      "epoch: 0, batch: 249, loss: 0.2892952263355255\n",
      "epoch: 0, batch: 250, loss: 0.4068237543106079\n",
      "epoch: 0, batch: 251, loss: 0.36730220913887024\n",
      "epoch: 0, batch: 252, loss: 0.5379568338394165\n",
      "epoch: 0, batch: 253, loss: 0.4155210554599762\n",
      "epoch: 0, batch: 254, loss: 0.23916465044021606\n",
      "epoch: 0, batch: 255, loss: 0.300868421792984\n",
      "epoch: 0, batch: 256, loss: 0.4935944080352783\n",
      "epoch: 0, batch: 257, loss: 0.27151498198509216\n",
      "epoch: 0, batch: 258, loss: 0.44315627217292786\n",
      "epoch: 0, batch: 259, loss: 0.4623512923717499\n",
      "epoch: 0, batch: 260, loss: 0.35649409890174866\n",
      "epoch: 0, batch: 261, loss: 0.37933966517448425\n",
      "epoch: 0, batch: 262, loss: 0.44932442903518677\n",
      "epoch: 0, batch: 263, loss: 0.43097132444381714\n",
      "epoch: 0, batch: 264, loss: 0.259685754776001\n",
      "epoch: 0, batch: 265, loss: 0.4351498484611511\n",
      "epoch: 0, batch: 266, loss: 0.3472788333892822\n",
      "epoch: 0, batch: 267, loss: 0.5663791298866272\n",
      "epoch: 0, batch: 268, loss: 0.2702128291130066\n",
      "epoch: 0, batch: 269, loss: 0.3480251729488373\n",
      "epoch: 0, batch: 270, loss: 0.3186399042606354\n",
      "epoch: 0, batch: 271, loss: 0.4031664729118347\n",
      "epoch: 0, batch: 272, loss: 0.31310001015663147\n",
      "epoch: 0, batch: 273, loss: 0.42095059156417847\n",
      "epoch: 0, batch: 274, loss: 0.44725996255874634\n",
      "epoch: 0, batch: 275, loss: 0.22963163256645203\n",
      "epoch: 0, batch: 276, loss: 0.35758447647094727\n",
      "epoch: 0, batch: 277, loss: 0.3986520767211914\n",
      "epoch: 0, batch: 278, loss: 0.2034737467765808\n",
      "epoch: 0, batch: 279, loss: 0.23728018999099731\n",
      "epoch: 0, batch: 280, loss: 0.29634976387023926\n",
      "epoch: 0, batch: 281, loss: 0.42386749386787415\n",
      "epoch: 0, batch: 282, loss: 0.3220099210739136\n",
      "epoch: 0, batch: 283, loss: 0.4993192255496979\n",
      "epoch: 0, batch: 284, loss: 0.33252525329589844\n",
      "epoch: 0, batch: 285, loss: 0.35252535343170166\n",
      "epoch: 0, batch: 286, loss: 0.26677826046943665\n",
      "epoch: 0, batch: 287, loss: 0.33562755584716797\n",
      "epoch: 0, batch: 288, loss: 0.3749932646751404\n",
      "epoch: 0, batch: 289, loss: 0.3855980634689331\n",
      "epoch: 0, batch: 290, loss: 0.37515923380851746\n",
      "epoch: 0, batch: 291, loss: 0.32200050354003906\n",
      "epoch: 0, batch: 292, loss: 0.41071295738220215\n",
      "epoch: 0, batch: 293, loss: 0.4357137680053711\n",
      "epoch: 0, batch: 294, loss: 0.2547297477722168\n",
      "epoch: 0, batch: 295, loss: 0.19536496698856354\n",
      "epoch: 0, batch: 296, loss: 0.2213459312915802\n",
      "epoch: 0, batch: 297, loss: 0.344165563583374\n",
      "epoch: 0, batch: 298, loss: 0.32753705978393555\n",
      "epoch: 0, batch: 299, loss: 0.31192561984062195\n",
      "epoch: 0, batch: 300, loss: 0.3215721547603607\n",
      "epoch: 0, batch: 301, loss: 0.35000959038734436\n",
      "epoch: 0, batch: 302, loss: 0.3498537242412567\n",
      "epoch: 0, batch: 303, loss: 0.5642476081848145\n",
      "epoch: 0, batch: 304, loss: 0.28850531578063965\n",
      "epoch: 0, batch: 305, loss: 0.3213548958301544\n",
      "epoch: 0, batch: 306, loss: 0.278259813785553\n",
      "epoch: 0, batch: 307, loss: 0.3619084060192108\n",
      "epoch: 0, batch: 308, loss: 0.5057530999183655\n",
      "epoch: 0, batch: 309, loss: 0.19908931851387024\n",
      "epoch: 0, batch: 310, loss: 0.3124501407146454\n",
      "epoch: 0, batch: 311, loss: 0.36646413803100586\n",
      "epoch: 0, batch: 312, loss: 0.4488711357116699\n",
      "epoch: 0, batch: 313, loss: 0.5048748850822449\n",
      "epoch: 0, batch: 314, loss: 0.15836352109909058\n",
      "epoch: 0, batch: 315, loss: 0.35218966007232666\n",
      "epoch: 0, batch: 316, loss: 0.2531702518463135\n",
      "epoch: 0, batch: 317, loss: 0.5592409372329712\n",
      "epoch: 0, batch: 318, loss: 0.23876504600048065\n",
      "epoch: 0, batch: 319, loss: 0.34595543146133423\n",
      "epoch: 0, batch: 320, loss: 0.5777155756950378\n",
      "epoch: 0, batch: 321, loss: 0.2867561876773834\n",
      "epoch: 0, batch: 322, loss: 0.2850523293018341\n",
      "epoch: 0, batch: 323, loss: 0.37868964672088623\n",
      "epoch: 0, batch: 324, loss: 0.4360499382019043\n",
      "epoch: 0, batch: 325, loss: 0.3641834557056427\n",
      "epoch: 0, batch: 326, loss: 0.41692039370536804\n",
      "epoch: 0, batch: 327, loss: 0.2192247211933136\n",
      "epoch: 0, batch: 328, loss: 0.3466692864894867\n",
      "epoch: 0, batch: 329, loss: 0.5345599055290222\n",
      "epoch: 0, batch: 330, loss: 0.48961180448532104\n",
      "epoch: 0, batch: 331, loss: 0.43155109882354736\n",
      "epoch: 0, batch: 332, loss: 0.2981018126010895\n",
      "epoch: 0, batch: 333, loss: 0.40398484468460083\n",
      "epoch: 0, batch: 334, loss: 0.2632525861263275\n",
      "epoch: 0, batch: 335, loss: 0.28970155119895935\n",
      "epoch: 0, batch: 336, loss: 0.257445752620697\n",
      "epoch: 0, batch: 337, loss: 0.5305512547492981\n",
      "epoch: 0, batch: 338, loss: 0.40070292353630066\n",
      "epoch: 0, batch: 339, loss: 0.6357221007347107\n",
      "epoch: 0, batch: 340, loss: 0.5544657707214355\n",
      "epoch: 0, batch: 341, loss: 0.3304252624511719\n",
      "epoch: 0, batch: 342, loss: 0.30926841497421265\n",
      "epoch: 0, batch: 343, loss: 0.33343642950057983\n",
      "epoch: 0, batch: 344, loss: 0.3134261965751648\n",
      "epoch: 0, batch: 345, loss: 0.4426511824131012\n",
      "epoch: 0, batch: 346, loss: 0.3682110011577606\n",
      "epoch: 0, batch: 347, loss: 0.29378169775009155\n",
      "epoch: 0, batch: 348, loss: 0.2995118796825409\n",
      "epoch: 0, batch: 349, loss: 0.3509644865989685\n",
      "epoch: 0, batch: 350, loss: 0.21327735483646393\n",
      "epoch: 0, batch: 351, loss: 0.36638930439949036\n",
      "epoch: 0, batch: 352, loss: 0.1936759501695633\n",
      "epoch: 0, batch: 353, loss: 0.29068368673324585\n",
      "epoch: 0, batch: 354, loss: 0.23114918172359467\n",
      "epoch: 0, batch: 355, loss: 0.4984612762928009\n",
      "epoch: 0, batch: 356, loss: 0.371042400598526\n",
      "epoch: 0, batch: 357, loss: 0.28574806451797485\n",
      "epoch: 0, batch: 358, loss: 0.5239101648330688\n",
      "epoch: 0, batch: 359, loss: 0.45424818992614746\n",
      "epoch: 0, batch: 360, loss: 0.2842320203781128\n",
      "epoch: 0, batch: 361, loss: 0.27511462569236755\n",
      "epoch: 0, batch: 362, loss: 0.3691955506801605\n",
      "epoch: 0, batch: 363, loss: 0.2598235011100769\n",
      "epoch: 0, batch: 364, loss: 0.33324676752090454\n",
      "epoch: 0, batch: 365, loss: 0.43239617347717285\n",
      "epoch: 0, batch: 366, loss: 0.4723800718784332\n",
      "epoch: 0, batch: 367, loss: 0.23530040681362152\n",
      "epoch: 0, batch: 368, loss: 0.28901228308677673\n",
      "epoch: 0, batch: 369, loss: 0.44272035360336304\n",
      "epoch: 0, batch: 370, loss: 0.2048792839050293\n",
      "epoch: 0, batch: 371, loss: 0.254548043012619\n",
      "epoch: 0, batch: 372, loss: 0.4829300045967102\n",
      "epoch: 0, batch: 373, loss: 0.30064666271209717\n",
      "epoch: 0, batch: 374, loss: 0.6330927014350891\n",
      "epoch: 0, batch: 375, loss: 0.20771896839141846\n",
      "epoch: 0, batch: 376, loss: 0.27946779131889343\n",
      "epoch: 0, batch: 377, loss: 0.4806159734725952\n",
      "epoch: 0, batch: 378, loss: 0.19639991223812103\n",
      "epoch: 0, batch: 379, loss: 0.5015402436256409\n",
      "epoch: 0, batch: 380, loss: 0.3306720554828644\n",
      "epoch: 0, batch: 381, loss: 0.33680981397628784\n",
      "epoch: 0, batch: 382, loss: 0.33755797147750854\n",
      "epoch: 0, batch: 383, loss: 0.41543275117874146\n",
      "epoch: 0, batch: 384, loss: 0.35930535197257996\n",
      "epoch: 0, batch: 385, loss: 0.37865903973579407\n",
      "epoch: 0, batch: 386, loss: 0.2739413380622864\n",
      "epoch: 0, batch: 387, loss: 0.44097936153411865\n",
      "epoch: 0, batch: 388, loss: 0.23980994522571564\n",
      "epoch: 0, batch: 389, loss: 0.3468715250492096\n",
      "epoch: 0, batch: 390, loss: 0.49691182374954224\n",
      "epoch: 0, batch: 391, loss: 0.2721192538738251\n",
      "epoch: 0, batch: 392, loss: 0.4144384562969208\n",
      "epoch: 0, batch: 393, loss: 0.2747648060321808\n",
      "epoch: 0, batch: 394, loss: 0.19256456196308136\n",
      "epoch: 0, batch: 395, loss: 0.31124770641326904\n",
      "epoch: 0, batch: 396, loss: 0.2922123372554779\n",
      "epoch: 0, batch: 397, loss: 0.28750938177108765\n",
      "epoch: 0, batch: 398, loss: 0.1497437059879303\n",
      "epoch: 0, batch: 399, loss: 0.2688482999801636\n",
      "epoch: 0, batch: 400, loss: 0.20091022551059723\n",
      "epoch: 0, batch: 401, loss: 0.4141940176486969\n",
      "epoch: 0, batch: 402, loss: 0.46576595306396484\n",
      "epoch: 0, batch: 403, loss: 0.14894665777683258\n",
      "epoch: 0, batch: 404, loss: 0.3558673858642578\n",
      "epoch: 0, batch: 405, loss: 0.20443344116210938\n",
      "epoch: 0, batch: 406, loss: 0.2318575233221054\n",
      "epoch: 0, batch: 407, loss: 0.2407996654510498\n",
      "epoch: 0, batch: 408, loss: 0.3186839818954468\n",
      "epoch: 0, batch: 409, loss: 0.25895363092422485\n",
      "epoch: 0, batch: 410, loss: 0.3459843695163727\n",
      "epoch: 0, batch: 411, loss: 0.27188578248023987\n",
      "epoch: 0, batch: 412, loss: 0.2793249189853668\n",
      "epoch: 0, batch: 413, loss: 0.5172914862632751\n",
      "epoch: 0, batch: 414, loss: 0.25421565771102905\n",
      "epoch: 0, batch: 415, loss: 0.2136450856924057\n",
      "epoch: 0, batch: 416, loss: 0.1959277093410492\n",
      "epoch: 0, batch: 417, loss: 0.416917622089386\n",
      "epoch: 0, batch: 418, loss: 0.24729333817958832\n",
      "epoch: 0, batch: 419, loss: 0.2673233449459076\n",
      "epoch: 0, batch: 420, loss: 0.2714928686618805\n",
      "epoch: 0, batch: 421, loss: 0.2685377895832062\n",
      "epoch: 0, batch: 422, loss: 0.4973640441894531\n",
      "epoch: 0, batch: 423, loss: 0.39435258507728577\n",
      "epoch: 0, batch: 424, loss: 0.281425416469574\n",
      "epoch: 0, batch: 425, loss: 0.2657073438167572\n",
      "epoch: 0, batch: 426, loss: 0.27974122762680054\n",
      "epoch: 0, batch: 427, loss: 0.5035153031349182\n",
      "epoch: 0, batch: 428, loss: 0.30250266194343567\n",
      "epoch: 0, batch: 429, loss: 0.2586286664009094\n",
      "epoch: 0, batch: 430, loss: 0.23212388157844543\n",
      "epoch: 0, batch: 431, loss: 0.2315872609615326\n",
      "epoch: 0, batch: 432, loss: 0.3908633589744568\n",
      "epoch: 0, batch: 433, loss: 0.3936510384082794\n",
      "epoch: 0, batch: 434, loss: 0.19132886826992035\n",
      "epoch: 0, batch: 435, loss: 0.4254075884819031\n",
      "epoch: 0, batch: 436, loss: 0.47489210963249207\n",
      "epoch: 0, batch: 437, loss: 0.31942999362945557\n",
      "epoch: 0, batch: 438, loss: 0.2649741470813751\n",
      "epoch: 0, batch: 439, loss: 0.2972545027732849\n",
      "epoch: 0, batch: 440, loss: 0.1996493637561798\n",
      "epoch: 0, batch: 441, loss: 0.3559454679489136\n",
      "epoch: 0, batch: 442, loss: 0.3028222322463989\n",
      "epoch: 0, batch: 443, loss: 0.37751200795173645\n",
      "epoch: 0, batch: 444, loss: 0.3399142324924469\n",
      "epoch: 0, batch: 445, loss: 0.21907900273799896\n",
      "epoch: 0, batch: 446, loss: 0.4400891065597534\n",
      "epoch: 0, batch: 447, loss: 0.45360517501831055\n",
      "epoch: 0, batch: 448, loss: 0.3154705762863159\n",
      "epoch: 0, batch: 449, loss: 0.27488014101982117\n",
      "epoch: 0, batch: 450, loss: 0.313422828912735\n",
      "epoch: 0, batch: 451, loss: 0.5184601545333862\n",
      "epoch: 0, batch: 452, loss: 0.3204101324081421\n",
      "epoch: 0, batch: 453, loss: 0.33965253829956055\n",
      "epoch: 0, batch: 454, loss: 0.38174787163734436\n",
      "epoch: 0, batch: 455, loss: 0.29639729857444763\n",
      "epoch: 0, batch: 456, loss: 0.5686377286911011\n",
      "epoch: 0, batch: 457, loss: 0.3942905068397522\n",
      "epoch: 0, batch: 458, loss: 0.25668394565582275\n",
      "epoch: 0, batch: 459, loss: 0.3242749273777008\n",
      "epoch: 0, batch: 460, loss: 0.3551652431488037\n",
      "epoch: 0, batch: 461, loss: 0.2885996997356415\n",
      "epoch: 0, batch: 462, loss: 0.38529229164123535\n",
      "epoch: 0, batch: 463, loss: 0.17548926174640656\n",
      "epoch: 0, batch: 464, loss: 0.22891487181186676\n",
      "epoch: 0, batch: 465, loss: 0.2012394666671753\n",
      "epoch: 0, batch: 466, loss: 0.30272606015205383\n",
      "epoch: 0, batch: 467, loss: 0.3134762942790985\n",
      "epoch: 0, batch: 468, loss: 0.20184999704360962\n",
      "epoch: 0, batch: 469, loss: 0.18983188271522522\n",
      "epoch: 0, batch: 470, loss: 0.44670772552490234\n",
      "epoch: 0, batch: 471, loss: 0.230354905128479\n",
      "epoch: 0, batch: 472, loss: 0.39925891160964966\n",
      "epoch: 0, batch: 473, loss: 0.29250067472457886\n",
      "epoch: 0, batch: 474, loss: 0.2843145728111267\n",
      "epoch: 0, batch: 475, loss: 0.2894713878631592\n",
      "epoch: 0, batch: 476, loss: 0.32212719321250916\n",
      "epoch: 0, batch: 477, loss: 0.21183976531028748\n",
      "epoch: 0, batch: 478, loss: 0.5932391881942749\n",
      "epoch: 0, batch: 479, loss: 0.29370585083961487\n",
      "epoch: 0, batch: 480, loss: 0.32873350381851196\n",
      "epoch: 0, batch: 481, loss: 0.26728275418281555\n",
      "epoch: 0, batch: 482, loss: 0.2515198588371277\n",
      "epoch: 0, batch: 483, loss: 0.22491121292114258\n",
      "epoch: 0, batch: 484, loss: 0.44700807332992554\n",
      "epoch: 0, batch: 485, loss: 0.3621312379837036\n",
      "epoch: 0, batch: 486, loss: 0.2498273104429245\n",
      "epoch: 0, batch: 487, loss: 0.6156169176101685\n",
      "epoch: 0, batch: 488, loss: 0.18989244103431702\n",
      "epoch: 0, batch: 489, loss: 0.21791335940361023\n",
      "epoch: 0, batch: 490, loss: 0.19954462349414825\n",
      "epoch: 0, batch: 491, loss: 0.2178073674440384\n",
      "epoch: 0, batch: 492, loss: 0.7492039203643799\n",
      "epoch: 0, batch: 493, loss: 0.1993417590856552\n",
      "epoch: 0, batch: 494, loss: 0.4652690589427948\n",
      "epoch: 0, batch: 495, loss: 0.4324589669704437\n",
      "epoch: 0, batch: 496, loss: 0.2374309003353119\n",
      "epoch: 0, batch: 497, loss: 0.29102879762649536\n",
      "epoch: 0, batch: 498, loss: 0.3602694571018219\n",
      "epoch: 0, batch: 499, loss: 0.2405053973197937\n",
      "epoch: 0, batch: 500, loss: 0.30651891231536865\n",
      "epoch: 0, batch: 501, loss: 0.1731441169977188\n",
      "epoch: 0, batch: 502, loss: 0.20874835550785065\n",
      "epoch: 0, batch: 503, loss: 0.19175691902637482\n",
      "epoch: 0, batch: 504, loss: 0.26564592123031616\n",
      "epoch: 0, batch: 505, loss: 0.4655286967754364\n",
      "epoch: 0, batch: 506, loss: 0.18388718366622925\n",
      "epoch: 0, batch: 507, loss: 0.2949678897857666\n",
      "epoch: 0, batch: 508, loss: 0.33575400710105896\n",
      "epoch: 0, batch: 509, loss: 0.4537917673587799\n",
      "epoch: 0, batch: 510, loss: 0.15267357230186462\n",
      "epoch: 0, batch: 511, loss: 0.47498005628585815\n",
      "epoch: 0, batch: 512, loss: 0.2548537254333496\n",
      "epoch: 0, batch: 513, loss: 0.15509772300720215\n",
      "epoch: 0, batch: 514, loss: 0.24843189120292664\n",
      "epoch: 0, batch: 515, loss: 0.28183621168136597\n",
      "epoch: 0, batch: 516, loss: 0.351054310798645\n",
      "epoch: 0, batch: 517, loss: 0.4736102521419525\n",
      "epoch: 0, batch: 518, loss: 0.2837597131729126\n",
      "epoch: 0, batch: 519, loss: 0.302223265171051\n",
      "epoch: 0, batch: 520, loss: 0.23054812848567963\n",
      "epoch: 0, batch: 521, loss: 0.192889004945755\n",
      "epoch: 0, batch: 522, loss: 0.1984570026397705\n",
      "epoch: 0, batch: 523, loss: 0.328439861536026\n",
      "epoch: 0, batch: 524, loss: 0.45527613162994385\n",
      "epoch: 0, batch: 525, loss: 0.5174762606620789\n",
      "epoch: 0, batch: 526, loss: 0.2210008054971695\n",
      "epoch: 0, batch: 527, loss: 0.22015069425106049\n",
      "epoch: 0, batch: 528, loss: 0.25456780195236206\n",
      "epoch: 0, batch: 529, loss: 0.1276029646396637\n",
      "epoch: 0, batch: 530, loss: 0.20978404581546783\n",
      "epoch: 0, batch: 531, loss: 0.1897154152393341\n",
      "epoch: 0, batch: 532, loss: 0.3447802662849426\n",
      "epoch: 0, batch: 533, loss: 0.27935877442359924\n",
      "epoch: 0, batch: 534, loss: 0.21741259098052979\n",
      "epoch: 0, batch: 535, loss: 0.33747637271881104\n",
      "epoch: 0, batch: 536, loss: 0.24003192782402039\n",
      "epoch: 0, batch: 537, loss: 0.3475528061389923\n",
      "epoch: 0, batch: 538, loss: 0.3710155785083771\n",
      "epoch: 0, batch: 539, loss: 0.3360285460948944\n",
      "epoch: 0, batch: 540, loss: 0.26474902033805847\n",
      "epoch: 0, batch: 541, loss: 0.4867652952671051\n",
      "epoch: 0, batch: 542, loss: 0.30179309844970703\n",
      "epoch: 0, batch: 543, loss: 0.2967488765716553\n",
      "epoch: 0, batch: 544, loss: 0.21923312544822693\n",
      "epoch: 0, batch: 545, loss: 0.3272814154624939\n",
      "epoch: 0, batch: 546, loss: 0.35267096757888794\n",
      "epoch: 0, batch: 547, loss: 0.287161260843277\n",
      "epoch: 0, batch: 548, loss: 0.13564957678318024\n",
      "epoch: 0, batch: 549, loss: 0.2942259609699249\n",
      "epoch: 0, batch: 550, loss: 0.2679234743118286\n",
      "epoch: 0, batch: 551, loss: 0.28981325030326843\n",
      "epoch: 0, batch: 552, loss: 0.4915727376937866\n",
      "epoch: 0, batch: 553, loss: 0.19764232635498047\n",
      "epoch: 0, batch: 554, loss: 0.2907819151878357\n",
      "epoch: 0, batch: 555, loss: 0.2376517951488495\n",
      "epoch: 0, batch: 556, loss: 0.23352965712547302\n",
      "epoch: 0, batch: 557, loss: 0.4000129997730255\n",
      "epoch: 0, batch: 558, loss: 0.19546285271644592\n",
      "epoch: 0, batch: 559, loss: 0.25495511293411255\n",
      "epoch: 0, batch: 560, loss: 0.2512262463569641\n",
      "epoch: 0, batch: 561, loss: 0.2698952555656433\n",
      "epoch: 0, batch: 562, loss: 0.37308311462402344\n",
      "epoch: 0, batch: 563, loss: 0.1934809535741806\n",
      "epoch: 0, batch: 564, loss: 0.3072962164878845\n",
      "epoch: 0, batch: 565, loss: 0.27140548825263977\n",
      "epoch: 0, batch: 566, loss: 0.18748556077480316\n",
      "epoch: 0, batch: 567, loss: 0.17751231789588928\n",
      "epoch: 0, batch: 568, loss: 0.3640969693660736\n",
      "epoch: 0, batch: 569, loss: 0.3101955056190491\n",
      "epoch: 0, batch: 570, loss: 0.3269862234592438\n",
      "epoch: 0, batch: 571, loss: 0.1190660297870636\n",
      "epoch: 0, batch: 572, loss: 0.17163622379302979\n",
      "epoch: 0, batch: 573, loss: 0.21334975957870483\n",
      "epoch: 0, batch: 574, loss: 0.3843391239643097\n",
      "epoch: 0, batch: 575, loss: 0.19462141394615173\n",
      "epoch: 0, batch: 576, loss: 0.28740307688713074\n",
      "epoch: 0, batch: 577, loss: 0.3117959499359131\n",
      "epoch: 0, batch: 578, loss: 0.260125994682312\n",
      "epoch: 0, batch: 579, loss: 0.4747277796268463\n",
      "epoch: 0, batch: 580, loss: 0.19863826036453247\n",
      "epoch: 0, batch: 581, loss: 0.32268309593200684\n",
      "epoch: 0, batch: 582, loss: 0.22921234369277954\n",
      "epoch: 0, batch: 583, loss: 0.21706466376781464\n",
      "epoch: 0, batch: 584, loss: 0.26819732785224915\n",
      "epoch: 0, batch: 585, loss: 0.23592540621757507\n",
      "epoch: 0, batch: 586, loss: 0.19440099596977234\n",
      "epoch: 0, batch: 587, loss: 0.2557319700717926\n",
      "epoch: 0, batch: 588, loss: 0.35326266288757324\n",
      "epoch: 0, batch: 589, loss: 0.32282012701034546\n",
      "epoch: 0, batch: 590, loss: 0.2740715742111206\n",
      "epoch: 0, batch: 591, loss: 0.34429556131362915\n",
      "epoch: 0, batch: 592, loss: 0.3816712200641632\n",
      "epoch: 0, batch: 593, loss: 0.23454321920871735\n",
      "epoch: 0, batch: 594, loss: 0.24417459964752197\n",
      "epoch: 0, batch: 595, loss: 0.35060253739356995\n",
      "epoch: 0, batch: 596, loss: 0.15989533066749573\n",
      "epoch: 0, batch: 597, loss: 0.29167720675468445\n",
      "epoch: 0, batch: 598, loss: 0.23990491032600403\n",
      "epoch: 0, batch: 599, loss: 0.19827434420585632\n",
      "epoch: 0, batch: 600, loss: 0.37273502349853516\n",
      "epoch: 0, batch: 601, loss: 0.16869693994522095\n",
      "epoch: 0, batch: 602, loss: 0.2593107521533966\n",
      "epoch: 0, batch: 603, loss: 0.3546583950519562\n",
      "epoch: 0, batch: 604, loss: 0.28197231888771057\n",
      "epoch: 0, batch: 605, loss: 0.38545873761177063\n",
      "epoch: 0, batch: 606, loss: 0.20567134022712708\n",
      "epoch: 0, batch: 607, loss: 0.35978013277053833\n",
      "epoch: 0, batch: 608, loss: 0.19972391426563263\n",
      "epoch: 0, batch: 609, loss: 0.15934231877326965\n",
      "epoch: 0, batch: 610, loss: 0.3708881437778473\n",
      "epoch: 0, batch: 611, loss: 0.17183741927146912\n",
      "epoch: 0, batch: 612, loss: 0.37356171011924744\n",
      "epoch: 0, batch: 613, loss: 0.22030755877494812\n",
      "epoch: 0, batch: 614, loss: 0.19454342126846313\n",
      "epoch: 0, batch: 615, loss: 0.32098332047462463\n",
      "epoch: 0, batch: 616, loss: 0.20156824588775635\n",
      "epoch: 0, batch: 617, loss: 0.3908400237560272\n",
      "epoch: 0, batch: 618, loss: 0.28448179364204407\n",
      "epoch: 0, batch: 619, loss: 0.24579094350337982\n",
      "epoch: 0, batch: 620, loss: 0.21097980439662933\n",
      "epoch: 0, batch: 621, loss: 0.3122239112854004\n",
      "epoch: 0, batch: 622, loss: 0.3175843358039856\n",
      "epoch: 0, batch: 623, loss: 0.3559075891971588\n",
      "epoch: 0, batch: 624, loss: 0.3567056953907013\n",
      "epoch: 0, batch: 625, loss: 0.21991896629333496\n",
      "epoch: 0, batch: 626, loss: 0.36871424317359924\n",
      "epoch: 0, batch: 627, loss: 0.26936739683151245\n",
      "epoch: 0, batch: 628, loss: 0.4571245610713959\n",
      "epoch: 0, batch: 629, loss: 0.14537379145622253\n",
      "epoch: 0, batch: 630, loss: 0.34424787759780884\n",
      "epoch: 0, batch: 631, loss: 0.26085394620895386\n",
      "epoch: 0, batch: 632, loss: 0.36152398586273193\n",
      "epoch: 0, batch: 633, loss: 0.1941932886838913\n",
      "epoch: 0, batch: 634, loss: 0.2214532494544983\n",
      "epoch: 0, batch: 635, loss: 0.21287137269973755\n",
      "epoch: 0, batch: 636, loss: 0.2864335775375366\n",
      "epoch: 0, batch: 637, loss: 0.25120922923088074\n",
      "epoch: 0, batch: 638, loss: 0.24150334298610687\n",
      "epoch: 0, batch: 639, loss: 0.18155254423618317\n",
      "epoch: 0, batch: 640, loss: 0.21564322710037231\n",
      "epoch: 0, batch: 641, loss: 0.34304365515708923\n",
      "epoch: 0, batch: 642, loss: 0.24497108161449432\n",
      "epoch: 0, batch: 643, loss: 0.2475425750017166\n",
      "epoch: 0, batch: 644, loss: 0.2087271362543106\n",
      "epoch: 0, batch: 645, loss: 0.3097946345806122\n",
      "epoch: 0, batch: 646, loss: 0.21079394221305847\n",
      "epoch: 0, batch: 647, loss: 0.3596956431865692\n",
      "epoch: 0, batch: 648, loss: 0.23201631009578705\n",
      "epoch: 0, batch: 649, loss: 0.23146945238113403\n",
      "epoch: 0, batch: 650, loss: 0.4518194794654846\n",
      "epoch: 0, batch: 651, loss: 0.17156268656253815\n",
      "epoch: 0, batch: 652, loss: 0.32017743587493896\n",
      "epoch: 0, batch: 653, loss: 0.2554326057434082\n",
      "epoch: 0, batch: 654, loss: 0.22087426483631134\n",
      "epoch: 0, batch: 655, loss: 0.27914413809776306\n",
      "epoch: 0, batch: 656, loss: 0.3906288146972656\n",
      "epoch: 0, batch: 657, loss: 0.1886017918586731\n",
      "epoch: 0, batch: 658, loss: 0.238983616232872\n",
      "epoch: 0, batch: 659, loss: 0.27345868945121765\n",
      "epoch: 0, batch: 660, loss: 0.2699320614337921\n",
      "epoch: 0, batch: 661, loss: 0.18868938088417053\n",
      "epoch: 0, batch: 662, loss: 0.19044971466064453\n",
      "epoch: 0, batch: 663, loss: 0.2488609254360199\n",
      "epoch: 0, batch: 664, loss: 0.16309109330177307\n",
      "epoch: 0, batch: 665, loss: 0.26837223768234253\n",
      "epoch: 0, batch: 666, loss: 0.3470851480960846\n",
      "epoch: 0, batch: 667, loss: 0.34672829508781433\n",
      "epoch: 0, batch: 668, loss: 0.15913091599941254\n",
      "epoch: 0, batch: 669, loss: 0.15633288025856018\n",
      "epoch: 0, batch: 670, loss: 0.3779468238353729\n",
      "epoch: 0, batch: 671, loss: 0.2382972538471222\n",
      "epoch: 0, batch: 672, loss: 0.2673279345035553\n",
      "epoch: 0, batch: 673, loss: 0.36710941791534424\n",
      "epoch: 0, batch: 674, loss: 0.2948709726333618\n",
      "epoch: 0, batch: 675, loss: 0.2507361173629761\n",
      "epoch: 0, batch: 676, loss: 0.16188746690750122\n",
      "epoch: 0, batch: 677, loss: 0.22211863100528717\n",
      "epoch: 0, batch: 678, loss: 0.19495409727096558\n",
      "epoch: 0, batch: 679, loss: 0.3145036995410919\n",
      "epoch: 0, batch: 680, loss: 0.2260155826807022\n",
      "epoch: 0, batch: 681, loss: 0.4841300845146179\n",
      "epoch: 0, batch: 682, loss: 0.3625164031982422\n",
      "epoch: 0, batch: 683, loss: 0.2990144193172455\n",
      "epoch: 0, batch: 684, loss: 0.2627710700035095\n",
      "epoch: 0, batch: 685, loss: 0.25890737771987915\n",
      "epoch: 0, batch: 686, loss: 0.33289769291877747\n",
      "epoch: 0, batch: 687, loss: 0.27430784702301025\n",
      "epoch: 0, batch: 688, loss: 0.362540602684021\n",
      "epoch: 0, batch: 689, loss: 0.3310324549674988\n",
      "epoch: 0, batch: 690, loss: 0.18142284452915192\n",
      "epoch: 0, batch: 691, loss: 0.5878459811210632\n",
      "epoch: 0, batch: 692, loss: 0.2069758176803589\n",
      "epoch: 0, batch: 693, loss: 0.18378528952598572\n",
      "epoch: 0, batch: 694, loss: 0.2683643102645874\n",
      "epoch: 0, batch: 695, loss: 0.2486543208360672\n",
      "epoch: 0, batch: 696, loss: 0.4462224841117859\n",
      "epoch: 0, batch: 697, loss: 0.17360427975654602\n",
      "epoch: 0, batch: 698, loss: 0.2788187563419342\n",
      "epoch: 0, batch: 699, loss: 0.2203594595193863\n",
      "epoch: 0, batch: 700, loss: 0.43500053882598877\n",
      "epoch: 0, batch: 701, loss: 0.3229495882987976\n",
      "epoch: 0, batch: 702, loss: 0.28042080998420715\n",
      "epoch: 0, batch: 703, loss: 0.34606772661209106\n",
      "epoch: 0, batch: 704, loss: 0.3016183078289032\n",
      "epoch: 0, batch: 705, loss: 0.2645063102245331\n",
      "epoch: 0, batch: 706, loss: 0.20765061676502228\n",
      "epoch: 0, batch: 707, loss: 0.20529890060424805\n",
      "epoch: 0, batch: 708, loss: 0.24319975078105927\n",
      "epoch: 0, batch: 709, loss: 0.3060830533504486\n",
      "epoch: 0, batch: 710, loss: 0.3044608235359192\n",
      "epoch: 0, batch: 711, loss: 0.17598192393779755\n",
      "epoch: 0, batch: 712, loss: 0.3463056683540344\n",
      "epoch: 0, batch: 713, loss: 0.26975664496421814\n",
      "epoch: 0, batch: 714, loss: 0.2781617343425751\n",
      "epoch: 0, batch: 715, loss: 0.28847187757492065\n",
      "epoch: 0, batch: 716, loss: 0.246670663356781\n",
      "epoch: 0, batch: 717, loss: 0.22194594144821167\n",
      "epoch: 0, batch: 718, loss: 0.34526950120925903\n",
      "epoch: 0, batch: 719, loss: 0.31298139691352844\n",
      "epoch: 0, batch: 720, loss: 0.19534388184547424\n",
      "epoch: 0, batch: 721, loss: 0.1706390678882599\n",
      "epoch: 0, batch: 722, loss: 0.3690115809440613\n",
      "epoch: 0, batch: 723, loss: 0.2569887936115265\n",
      "epoch: 0, batch: 724, loss: 0.25741642713546753\n",
      "epoch: 0, batch: 725, loss: 0.19741703569889069\n",
      "epoch: 0, batch: 726, loss: 0.27350914478302\n",
      "epoch: 0, batch: 727, loss: 0.2070254534482956\n",
      "epoch: 0, batch: 728, loss: 0.2250867784023285\n",
      "epoch: 0, batch: 729, loss: 0.13182660937309265\n",
      "epoch: 0, batch: 730, loss: 0.11580917984247208\n",
      "epoch: 0, batch: 731, loss: 0.2833169400691986\n",
      "epoch: 0, batch: 732, loss: 0.20109374821186066\n",
      "epoch: 0, batch: 733, loss: 0.4089052379131317\n",
      "epoch: 0, batch: 734, loss: 0.2776057720184326\n",
      "epoch: 0, batch: 735, loss: 0.20706039667129517\n",
      "epoch: 0, batch: 736, loss: 0.3619856834411621\n",
      "epoch: 0, batch: 737, loss: 0.2743498682975769\n",
      "epoch: 0, batch: 738, loss: 0.1741032600402832\n",
      "epoch: 0, batch: 739, loss: 0.4620473086833954\n",
      "epoch: 0, batch: 740, loss: 0.49831387400627136\n",
      "epoch: 0, batch: 741, loss: 0.1932888627052307\n",
      "epoch: 0, batch: 742, loss: 0.11249105632305145\n",
      "epoch: 0, batch: 743, loss: 0.11918473988771439\n",
      "epoch: 0, batch: 744, loss: 0.32439446449279785\n",
      "epoch: 0, batch: 745, loss: 0.2295933961868286\n",
      "epoch: 0, batch: 746, loss: 0.34027355909347534\n",
      "epoch: 0, batch: 747, loss: 0.42194172739982605\n",
      "epoch: 0, batch: 748, loss: 0.12636277079582214\n",
      "epoch: 0, batch: 749, loss: 0.3784010708332062\n",
      "epoch: 0, batch: 750, loss: 0.3956475555896759\n",
      "epoch: 0, batch: 751, loss: 0.19342485070228577\n",
      "epoch: 0, batch: 752, loss: 0.2450646311044693\n",
      "epoch: 0, batch: 753, loss: 0.23397521674633026\n",
      "epoch: 0, batch: 754, loss: 0.2984195649623871\n",
      "epoch: 0, batch: 755, loss: 0.2404930144548416\n",
      "epoch: 0, batch: 756, loss: 0.14497630298137665\n",
      "epoch: 0, batch: 757, loss: 0.23792541027069092\n",
      "epoch: 0, batch: 758, loss: 0.2457299679517746\n",
      "epoch: 0, batch: 759, loss: 0.32837730646133423\n",
      "epoch: 0, batch: 760, loss: 0.37571579217910767\n",
      "epoch: 0, batch: 761, loss: 0.35075658559799194\n",
      "epoch: 0, batch: 762, loss: 0.41163843870162964\n",
      "epoch: 0, batch: 763, loss: 0.22853586077690125\n",
      "epoch: 0, batch: 764, loss: 0.10767403990030289\n",
      "epoch: 0, batch: 765, loss: 0.1925872266292572\n",
      "epoch: 0, batch: 766, loss: 0.09617044776678085\n",
      "epoch: 0, batch: 767, loss: 0.4131789207458496\n",
      "epoch: 0, batch: 768, loss: 0.20662257075309753\n",
      "epoch: 0, batch: 769, loss: 0.3439174294471741\n",
      "epoch: 0, batch: 770, loss: 0.16017459332942963\n",
      "epoch: 0, batch: 771, loss: 0.32839295268058777\n",
      "epoch: 0, batch: 772, loss: 0.2811821699142456\n",
      "epoch: 0, batch: 773, loss: 0.2915421426296234\n",
      "epoch: 0, batch: 774, loss: 0.0958419144153595\n",
      "epoch: 0, batch: 775, loss: 0.43277469277381897\n",
      "epoch: 0, batch: 776, loss: 0.44571611285209656\n",
      "epoch: 0, batch: 777, loss: 0.6339119076728821\n",
      "epoch: 0, batch: 778, loss: 0.25519561767578125\n",
      "epoch: 0, batch: 779, loss: 0.09833526611328125\n",
      "epoch: 0, batch: 780, loss: 0.38031595945358276\n",
      "epoch: 0, batch: 781, loss: 0.34341415762901306\n",
      "epoch: 0, batch: 782, loss: 0.3146151006221771\n",
      "epoch: 0, batch: 783, loss: 0.2743399739265442\n",
      "epoch: 0, batch: 784, loss: 0.15597178041934967\n",
      "epoch: 0, batch: 785, loss: 0.3985638916492462\n",
      "epoch: 0, batch: 786, loss: 0.34901162981987\n",
      "epoch: 0, batch: 787, loss: 0.18949618935585022\n",
      "epoch: 0, batch: 788, loss: 0.19471856951713562\n",
      "epoch: 0, batch: 789, loss: 0.20378540456295013\n",
      "epoch: 0, batch: 790, loss: 0.24302080273628235\n",
      "epoch: 0, batch: 791, loss: 0.3463161289691925\n",
      "epoch: 0, batch: 792, loss: 0.26251986622810364\n",
      "epoch: 0, batch: 793, loss: 0.16840104758739471\n",
      "epoch: 0, batch: 794, loss: 0.3426743447780609\n",
      "epoch: 0, batch: 795, loss: 0.3066847622394562\n",
      "epoch: 0, batch: 796, loss: 0.18444427847862244\n",
      "epoch: 0, batch: 797, loss: 0.29667359590530396\n",
      "epoch: 0, batch: 798, loss: 0.20406240224838257\n",
      "epoch: 0, batch: 799, loss: 0.2312815934419632\n",
      "epoch: 0, batch: 800, loss: 0.2627125680446625\n",
      "epoch: 0, batch: 801, loss: 0.12707634270191193\n",
      "epoch: 0, batch: 802, loss: 0.3288038969039917\n",
      "epoch: 0, batch: 803, loss: 0.4128667414188385\n",
      "epoch: 0, batch: 804, loss: 0.40941694378852844\n",
      "epoch: 0, batch: 805, loss: 0.30580127239227295\n",
      "epoch: 0, batch: 806, loss: 0.19552378356456757\n",
      "epoch: 0, batch: 807, loss: 0.2657632827758789\n",
      "epoch: 0, batch: 808, loss: 0.1818559467792511\n",
      "epoch: 0, batch: 809, loss: 0.19095975160598755\n",
      "epoch: 0, batch: 810, loss: 0.27409061789512634\n",
      "epoch: 0, batch: 811, loss: 0.1976310908794403\n",
      "epoch: 0, batch: 812, loss: 0.23888447880744934\n",
      "epoch: 0, batch: 813, loss: 0.25714126229286194\n",
      "epoch: 0, batch: 814, loss: 0.18990078568458557\n",
      "epoch: 0, batch: 815, loss: 0.28117087483406067\n",
      "epoch: 0, batch: 816, loss: 0.6231582760810852\n",
      "epoch: 0, batch: 817, loss: 0.2929316759109497\n",
      "epoch: 0, batch: 818, loss: 0.22018323838710785\n",
      "epoch: 0, batch: 819, loss: 0.2512681186199188\n",
      "epoch: 0, batch: 820, loss: 0.24267928302288055\n",
      "epoch: 0, batch: 821, loss: 0.2438768595457077\n",
      "epoch: 0, batch: 822, loss: 0.1683439016342163\n",
      "epoch: 0, batch: 823, loss: 0.33018869161605835\n",
      "epoch: 0, batch: 824, loss: 0.1607518196105957\n",
      "epoch: 0, batch: 825, loss: 0.11900923401117325\n",
      "epoch: 0, batch: 826, loss: 0.13929422199726105\n",
      "epoch: 0, batch: 827, loss: 0.22521820664405823\n",
      "epoch: 0, batch: 828, loss: 0.18845830857753754\n",
      "epoch: 0, batch: 829, loss: 0.2051711082458496\n",
      "epoch: 0, batch: 830, loss: 0.39848315715789795\n",
      "epoch: 0, batch: 831, loss: 0.1598469316959381\n",
      "epoch: 0, batch: 832, loss: 0.16645145416259766\n",
      "epoch: 0, batch: 833, loss: 0.2710447311401367\n",
      "epoch: 0, batch: 834, loss: 0.39726799726486206\n",
      "epoch: 0, batch: 835, loss: 0.28226447105407715\n",
      "epoch: 0, batch: 836, loss: 0.2803899049758911\n",
      "epoch: 0, batch: 837, loss: 0.13124805688858032\n",
      "epoch: 0, batch: 838, loss: 0.2004639208316803\n",
      "epoch: 0, batch: 839, loss: 0.18189287185668945\n",
      "epoch: 0, batch: 840, loss: 0.25709861516952515\n",
      "epoch: 0, batch: 841, loss: 0.2500971853733063\n",
      "epoch: 0, batch: 842, loss: 0.307794451713562\n",
      "epoch: 0, batch: 843, loss: 0.2465697079896927\n",
      "epoch: 0, batch: 844, loss: 0.37440672516822815\n",
      "epoch: 0, batch: 845, loss: 0.2746277451515198\n",
      "epoch: 0, batch: 846, loss: 0.3682831823825836\n",
      "epoch: 0, batch: 847, loss: 0.4445955455303192\n",
      "epoch: 0, batch: 848, loss: 0.3803388774394989\n",
      "epoch: 0, batch: 849, loss: 0.3572261929512024\n",
      "epoch: 0, batch: 850, loss: 0.3749998211860657\n",
      "epoch: 0, batch: 851, loss: 0.1671428680419922\n",
      "epoch: 0, batch: 852, loss: 0.35558804869651794\n",
      "epoch: 0, batch: 853, loss: 0.12151453644037247\n",
      "epoch: 0, batch: 854, loss: 0.21842586994171143\n",
      "epoch: 0, batch: 855, loss: 0.2685367465019226\n",
      "epoch: 0, batch: 856, loss: 0.15929263830184937\n",
      "epoch: 0, batch: 857, loss: 0.35491108894348145\n",
      "epoch: 0, batch: 858, loss: 0.4001157283782959\n",
      "epoch: 0, batch: 859, loss: 0.5519359111785889\n",
      "epoch: 0, batch: 860, loss: 0.21499112248420715\n",
      "epoch: 0, batch: 861, loss: 0.20009580254554749\n",
      "epoch: 0, batch: 862, loss: 0.2609361708164215\n",
      "epoch: 0, batch: 863, loss: 0.24145039916038513\n",
      "epoch: 0, batch: 864, loss: 0.19297629594802856\n",
      "epoch: 0, batch: 865, loss: 0.4087817072868347\n",
      "epoch: 0, batch: 866, loss: 0.2132093906402588\n",
      "epoch: 0, batch: 867, loss: 0.3343835175037384\n",
      "epoch: 0, batch: 868, loss: 0.30264967679977417\n",
      "epoch: 0, batch: 869, loss: 0.2151070386171341\n",
      "epoch: 0, batch: 870, loss: 0.2580343186855316\n",
      "epoch: 0, batch: 871, loss: 0.2851272523403168\n",
      "epoch: 0, batch: 872, loss: 0.11477266252040863\n",
      "epoch: 0, batch: 873, loss: 0.24031391739845276\n",
      "epoch: 0, batch: 874, loss: 0.297811895608902\n",
      "epoch: 0, batch: 875, loss: 0.4082910418510437\n",
      "epoch: 0, batch: 876, loss: 0.24179108440876007\n",
      "epoch: 0, batch: 877, loss: 0.2983705699443817\n",
      "epoch: 0, batch: 878, loss: 0.19078247249126434\n",
      "epoch: 0, batch: 879, loss: 0.2502759099006653\n",
      "epoch: 0, batch: 880, loss: 0.199225053191185\n",
      "epoch: 0, batch: 881, loss: 0.26031070947647095\n",
      "epoch: 0, batch: 882, loss: 0.14434196054935455\n",
      "epoch: 0, batch: 883, loss: 0.14343246817588806\n",
      "epoch: 0, batch: 884, loss: 0.38800346851348877\n",
      "epoch: 0, batch: 885, loss: 0.32653453946113586\n",
      "epoch: 0, batch: 886, loss: 0.22235918045043945\n",
      "epoch: 0, batch: 887, loss: 0.39479759335517883\n",
      "epoch: 0, batch: 888, loss: 0.27088889479637146\n",
      "epoch: 0, batch: 889, loss: 0.19935977458953857\n",
      "epoch: 0, batch: 890, loss: 0.1409032642841339\n",
      "epoch: 0, batch: 891, loss: 0.260127991437912\n",
      "epoch: 0, batch: 892, loss: 0.2300020009279251\n",
      "epoch: 0, batch: 893, loss: 0.19856707751750946\n",
      "epoch: 0, batch: 894, loss: 0.4230721890926361\n",
      "epoch: 0, batch: 895, loss: 0.18568287789821625\n",
      "epoch: 0, batch: 896, loss: 0.2308027148246765\n",
      "epoch: 0, batch: 897, loss: 0.2084958553314209\n",
      "epoch: 0, batch: 898, loss: 0.28756412863731384\n",
      "epoch: 0, batch: 899, loss: 0.24051856994628906\n",
      "epoch: 0, batch: 900, loss: 0.35273391008377075\n",
      "epoch: 0, batch: 901, loss: 0.17504583299160004\n",
      "epoch: 0, batch: 902, loss: 0.20268628001213074\n",
      "epoch: 0, batch: 903, loss: 0.1369001716375351\n",
      "epoch: 0, batch: 904, loss: 0.3880917429924011\n",
      "epoch: 0, batch: 905, loss: 0.32836639881134033\n",
      "epoch: 0, batch: 906, loss: 0.1436515599489212\n",
      "epoch: 0, batch: 907, loss: 0.4144778847694397\n",
      "epoch: 0, batch: 908, loss: 0.2447718381881714\n",
      "epoch: 0, batch: 909, loss: 0.1740361750125885\n",
      "epoch: 0, batch: 910, loss: 0.2779349386692047\n",
      "epoch: 0, batch: 911, loss: 0.17178195714950562\n",
      "epoch: 0, batch: 912, loss: 0.3089412748813629\n",
      "epoch: 0, batch: 913, loss: 0.2599995732307434\n",
      "epoch: 0, batch: 914, loss: 0.512830913066864\n",
      "epoch: 0, batch: 915, loss: 0.1830856204032898\n",
      "epoch: 0, batch: 916, loss: 0.1980268657207489\n",
      "epoch: 0, batch: 917, loss: 0.20840172469615936\n",
      "epoch: 0, batch: 918, loss: 0.33764123916625977\n",
      "epoch: 0, batch: 919, loss: 0.2673124372959137\n",
      "epoch: 0, batch: 920, loss: 0.20784689486026764\n",
      "epoch: 0, batch: 921, loss: 0.33643630146980286\n",
      "epoch: 0, batch: 922, loss: 0.15064555406570435\n",
      "epoch: 0, batch: 923, loss: 0.3175026476383209\n",
      "epoch: 0, batch: 924, loss: 0.26760146021842957\n",
      "epoch: 0, batch: 925, loss: 0.23172372579574585\n",
      "epoch: 0, batch: 926, loss: 0.303445041179657\n",
      "epoch: 0, batch: 927, loss: 0.2410367876291275\n",
      "epoch: 0, batch: 928, loss: 0.3898482024669647\n",
      "epoch: 0, batch: 929, loss: 0.22300447523593903\n",
      "epoch: 0, batch: 930, loss: 0.26500609517097473\n",
      "epoch: 0, batch: 931, loss: 0.26362767815589905\n",
      "epoch: 0, batch: 932, loss: 0.18534107506275177\n",
      "epoch: 0, batch: 933, loss: 0.21072222292423248\n",
      "epoch: 0, batch: 934, loss: 0.2166522741317749\n",
      "epoch: 0, batch: 935, loss: 0.47098639607429504\n",
      "epoch: 0, batch: 936, loss: 0.20705744624137878\n",
      "epoch: 0, batch: 937, loss: 0.18535323441028595\n",
      "CPU times: total: 8.33 s\n",
      "Wall time: 6.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# get model to cuda if possible\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, target) in enumerate((train_datloader)):\n",
    "\n",
    "        # get data to cuda if possible\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        data = data.reshape(-1,784)\n",
    "\n",
    "        # feed forward the data to model\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, target)\n",
    "        print(f\"epoch: {epoch}, batch: {batch_idx}, loss: {loss}\")\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad() # This will flush the gradients from the last iteration\n",
    "        loss.backward()\n",
    "\n",
    "        # optimise the loss (gradient descent or Adam step)\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check accuracy on train and test data (Validate model accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy on training & test to see how good our model\n",
    "def check_accuracy(loader, model):\n",
    "    \"\"\"\n",
    "    Check accuracy of our trained model given a loader and a model\n",
    "\n",
    "    Parameters:\n",
    "        loader: torch.utils.data.DataLoader\n",
    "            A loader for the dataset you want to check accuracy on\n",
    "        model: nn.Module\n",
    "            The model you want to check accuracy on\n",
    "\n",
    "    Returns:\n",
    "        acc: float\n",
    "            The accuracy of the model on the dataset given by the loader\n",
    "    \"\"\"\n",
    "\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "\n",
    "    # We don't need to keep track of gradients here so we wrap it in torch.no_grad()\n",
    "    with torch.no_grad():\n",
    "        # Loop through the data\n",
    "        for x, y in loader:\n",
    "\n",
    "            # Move data to device\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "\n",
    "            # Get to correct shape\n",
    "            x = x.reshape(x.shape[0], -1)\n",
    "\n",
    "            # Forward pass\n",
    "            scores = model(x)\n",
    "            predictions = scores.argmax(1)\n",
    "\n",
    "            # Check how many we got correct\n",
    "            num_correct += (predictions == y).sum()\n",
    "\n",
    "            # Keep track of number of samples\n",
    "            num_samples += predictions.size(0)\n",
    "\n",
    "    model.train()\n",
    "    return num_correct / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 93.26\n",
      "Accuracy on test set: 93.27\n"
     ]
    }
   ],
   "source": [
    "# Check accuracy on training & test to see how good our model\n",
    "print(f\"Accuracy on training set: {check_accuracy(train_datloader, model)*100:.2f}\")\n",
    "print(f\"Accuracy on test set: {check_accuracy(test_datloader, model)*100:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
