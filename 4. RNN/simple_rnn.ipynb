{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transformations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a RNN Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import nn module\n",
    "# Init super class\n",
    "# input  -> Time seq X embedding\n",
    "# design a RNN Layer with hidden usnits and number of layers\n",
    "# Check output size at each layer using pseudo forwaard\n",
    "# Fc1\n",
    "# Forward pass operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, imp_emb_dim, hidden_units, n_layers, output_classes):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_units = hidden_units\n",
    "\n",
    "        self.rnn = nn.RNN(input_size=imp_emb_dim,\n",
    "                               hidden_size=hidden_units,\n",
    "                               num_layers=n_layers,\n",
    "                               batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_units*imp_emb_dim, output_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Initialize the hidden state first\n",
    "        h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_units).to(device)\n",
    "        \n",
    "        # Forward Propagation\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out_flatten = torch.flatten(out, 1,-1)\n",
    "        x = self.fc(out_flatten)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "# check the network graph\n",
    "model = SimpleRNN(28, 5, 10, 10)\n",
    "\n",
    "# create a random variable and pass it to the model to check the network graph\n",
    "device = 'cpu'\n",
    "x = torch.randn(64, 28, 28)\n",
    "print(x.shape)\n",
    "print(model(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undersatnding RNN layer\n",
    "n_layer = 10\n",
    "hidden_size = 5\n",
    "emb_dim = 28\n",
    "batch_size = 64\n",
    "max_seq_length = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_cell = nn.RNN(input_size=emb_dim,\n",
    "                               hidden_size=hidden_size,\n",
    "                               num_layers=n_layer,\n",
    "                               batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 12, 28])\n",
      "torch.Size([10, 64, 5])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create a random variable and pass it to the model to check the network graph\n",
    "x = torch.randn(batch_size, max_seq_length, emb_dim)\n",
    "print(x.shape)\n",
    "\n",
    "hidden_state = torch.zeros(n_layer, batch_size, hidden_size)\n",
    "print(hidden_state.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 12, 5]), torch.Size([10, 64, 5]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch_first=True\n",
    "rnn_cell(x, hidden_state)[0].shape, rnn_cell(x, hidden_state)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 12, 5]), torch.Size([10, 12, 5]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch_first=False\n",
    "rnn_cell(x)[0].shape, rnn_cell(x, )[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_class = 10\n",
    "n_layer = 10\n",
    "hidden_size = 5\n",
    "emb_dim = 28\n",
    "batch_size = 64\n",
    "max_seq_length = 12\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and load the data from pytorch sample datasets\n",
    "# https://pytorch.org/vision/0.8/datasets.html\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    root=\"../dataset/\", train=True, transform=transformations.ToTensor(), download=True\n",
    ")\n",
    "train_datloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels batch shape: torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 6\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Understand the dataloader\n",
    "plt.title(\"sample image\")\n",
    "# Display image and label.\n",
    "train_features, train_labels = next(iter(train_datloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset shape: torch.Size([60000, 28, 28])\n",
      "['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n"
     ]
    }
   ],
   "source": [
    "# train data shape and class labels\n",
    "print(\"train_dataset shape:\", train_dataset.data.shape)\n",
    "print(train_dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_dataset shape: torch.Size([10000, 28, 28])\n",
      "['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n"
     ]
    }
   ],
   "source": [
    "test_dataset = datasets.MNIST(\n",
    "    root=\"../dataset/\", train=False, transform=transformations.ToTensor(), download=True\n",
    ")\n",
    "test_datloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(\"test_dataset shape:\", test_dataset.data.shape)\n",
    "print(test_dataset.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleRNN(\n",
      "  (rnn): RNN(28, 5, num_layers=10, batch_first=True)\n",
      "  (fc): Linear(in_features=140, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = SimpleRNN(emb_dim, hidden_size, n_layer, n_class)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise Loss And Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps to train the model\n",
    "1. For each epoch, iterate through the batch\n",
    "2. For each batch\n",
    "    * feed forward the input and target data of train to the model\n",
    "    * Calculate the loss and score\n",
    "    * Backpropogate the loss\n",
    "    * optimise the loss using optimiser() (gradient descent is one such optimiser) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch: 0, loss: 2.417743682861328\n",
      "epoch: 0, batch: 1, loss: 2.3926119804382324\n",
      "epoch: 0, batch: 2, loss: 2.345919609069824\n",
      "epoch: 0, batch: 3, loss: 2.3943912982940674\n",
      "epoch: 0, batch: 4, loss: 2.356189489364624\n",
      "epoch: 0, batch: 5, loss: 2.343998670578003\n",
      "epoch: 0, batch: 6, loss: 2.2753684520721436\n",
      "epoch: 0, batch: 7, loss: 2.307959794998169\n",
      "epoch: 0, batch: 8, loss: 2.3109803199768066\n",
      "epoch: 0, batch: 9, loss: 2.3002729415893555\n",
      "epoch: 0, batch: 10, loss: 2.3294687271118164\n",
      "epoch: 0, batch: 11, loss: 2.299365282058716\n",
      "epoch: 0, batch: 12, loss: 2.320958137512207\n",
      "epoch: 0, batch: 13, loss: 2.3113603591918945\n",
      "epoch: 0, batch: 14, loss: 2.294981002807617\n",
      "epoch: 0, batch: 15, loss: 2.333712577819824\n",
      "epoch: 0, batch: 16, loss: 2.2980194091796875\n",
      "epoch: 0, batch: 17, loss: 2.3238065242767334\n",
      "epoch: 0, batch: 18, loss: 2.2816402912139893\n",
      "epoch: 0, batch: 19, loss: 2.345109224319458\n",
      "epoch: 0, batch: 20, loss: 2.324613332748413\n",
      "epoch: 0, batch: 21, loss: 2.3083810806274414\n",
      "epoch: 0, batch: 22, loss: 2.3089072704315186\n",
      "epoch: 0, batch: 23, loss: 2.3236377239227295\n",
      "epoch: 0, batch: 24, loss: 2.3255348205566406\n",
      "epoch: 0, batch: 25, loss: 2.2875568866729736\n",
      "epoch: 0, batch: 26, loss: 2.305631160736084\n",
      "epoch: 0, batch: 27, loss: 2.310831069946289\n",
      "epoch: 0, batch: 28, loss: 2.3092498779296875\n",
      "epoch: 0, batch: 29, loss: 2.30025577545166\n",
      "epoch: 0, batch: 30, loss: 2.304708480834961\n",
      "epoch: 0, batch: 31, loss: 2.3208327293395996\n",
      "epoch: 0, batch: 32, loss: 2.2942724227905273\n",
      "epoch: 0, batch: 33, loss: 2.3008744716644287\n",
      "epoch: 0, batch: 34, loss: 2.282594919204712\n",
      "epoch: 0, batch: 35, loss: 2.266486167907715\n",
      "epoch: 0, batch: 36, loss: 2.293797016143799\n",
      "epoch: 0, batch: 37, loss: 2.3210880756378174\n",
      "epoch: 0, batch: 38, loss: 2.2900843620300293\n",
      "epoch: 0, batch: 39, loss: 2.308716058731079\n",
      "epoch: 0, batch: 40, loss: 2.3244755268096924\n",
      "epoch: 0, batch: 41, loss: 2.3298499584198\n",
      "epoch: 0, batch: 42, loss: 2.3361427783966064\n",
      "epoch: 0, batch: 43, loss: 2.3100943565368652\n",
      "epoch: 0, batch: 44, loss: 2.311326265335083\n",
      "epoch: 0, batch: 45, loss: 2.328242301940918\n",
      "epoch: 0, batch: 46, loss: 2.313999652862549\n",
      "epoch: 0, batch: 47, loss: 2.2892096042633057\n",
      "epoch: 0, batch: 48, loss: 2.310727596282959\n",
      "epoch: 0, batch: 49, loss: 2.3058905601501465\n",
      "epoch: 0, batch: 50, loss: 2.2847471237182617\n",
      "epoch: 0, batch: 51, loss: 2.314518690109253\n",
      "epoch: 0, batch: 52, loss: 2.285980701446533\n",
      "epoch: 0, batch: 53, loss: 2.311673164367676\n",
      "epoch: 0, batch: 54, loss: 2.3299572467803955\n",
      "epoch: 0, batch: 55, loss: 2.314659833908081\n",
      "epoch: 0, batch: 56, loss: 2.283543586730957\n",
      "epoch: 0, batch: 57, loss: 2.3210887908935547\n",
      "epoch: 0, batch: 58, loss: 2.2902607917785645\n",
      "epoch: 0, batch: 59, loss: 2.3101513385772705\n",
      "epoch: 0, batch: 60, loss: 2.2952799797058105\n",
      "epoch: 0, batch: 61, loss: 2.3006577491760254\n",
      "epoch: 0, batch: 62, loss: 2.314638614654541\n",
      "epoch: 0, batch: 63, loss: 2.2716660499572754\n",
      "epoch: 0, batch: 64, loss: 2.274366617202759\n",
      "epoch: 0, batch: 65, loss: 2.280487298965454\n",
      "epoch: 0, batch: 66, loss: 2.3041512966156006\n",
      "epoch: 0, batch: 67, loss: 2.314044713973999\n",
      "epoch: 0, batch: 68, loss: 2.299082040786743\n",
      "epoch: 0, batch: 69, loss: 2.2929720878601074\n",
      "epoch: 0, batch: 70, loss: 2.277287483215332\n",
      "epoch: 0, batch: 71, loss: 2.2909674644470215\n",
      "epoch: 0, batch: 72, loss: 2.331077814102173\n",
      "epoch: 0, batch: 73, loss: 2.3137974739074707\n",
      "epoch: 0, batch: 74, loss: 2.2953243255615234\n",
      "epoch: 0, batch: 75, loss: 2.2980802059173584\n",
      "epoch: 0, batch: 76, loss: 2.297755241394043\n",
      "epoch: 0, batch: 77, loss: 2.2868471145629883\n",
      "epoch: 0, batch: 78, loss: 2.2547051906585693\n",
      "epoch: 0, batch: 79, loss: 2.304335355758667\n",
      "epoch: 0, batch: 80, loss: 2.3010141849517822\n",
      "epoch: 0, batch: 81, loss: 2.2872025966644287\n",
      "epoch: 0, batch: 82, loss: 2.2637598514556885\n",
      "epoch: 0, batch: 83, loss: 2.268583059310913\n",
      "epoch: 0, batch: 84, loss: 2.2487151622772217\n",
      "epoch: 0, batch: 85, loss: 2.2508935928344727\n",
      "epoch: 0, batch: 86, loss: 2.284403085708618\n",
      "epoch: 0, batch: 87, loss: 2.248701333999634\n",
      "epoch: 0, batch: 88, loss: 2.2413737773895264\n",
      "epoch: 0, batch: 89, loss: 2.224252462387085\n",
      "epoch: 0, batch: 90, loss: 2.2025511264801025\n",
      "epoch: 0, batch: 91, loss: 2.2133848667144775\n",
      "epoch: 0, batch: 92, loss: 2.1835381984710693\n",
      "epoch: 0, batch: 93, loss: 2.1855251789093018\n",
      "epoch: 0, batch: 94, loss: 2.1767420768737793\n",
      "epoch: 0, batch: 95, loss: 2.1634128093719482\n",
      "epoch: 0, batch: 96, loss: 2.170949935913086\n",
      "epoch: 0, batch: 97, loss: 2.092710018157959\n",
      "epoch: 0, batch: 98, loss: 2.130645275115967\n",
      "epoch: 0, batch: 99, loss: 2.118314743041992\n",
      "epoch: 0, batch: 100, loss: 2.009512424468994\n",
      "epoch: 0, batch: 101, loss: 2.0772862434387207\n",
      "epoch: 0, batch: 102, loss: 2.0850186347961426\n",
      "epoch: 0, batch: 103, loss: 1.9893889427185059\n",
      "epoch: 0, batch: 104, loss: 1.9318621158599854\n",
      "epoch: 0, batch: 105, loss: 1.976322889328003\n",
      "epoch: 0, batch: 106, loss: 1.9014734029769897\n",
      "epoch: 0, batch: 107, loss: 1.978134274482727\n",
      "epoch: 0, batch: 108, loss: 1.934975504875183\n",
      "epoch: 0, batch: 109, loss: 1.9525105953216553\n",
      "epoch: 0, batch: 110, loss: 1.9084879159927368\n",
      "epoch: 0, batch: 111, loss: 1.8920462131500244\n",
      "epoch: 0, batch: 112, loss: 1.7541078329086304\n",
      "epoch: 0, batch: 113, loss: 1.8676068782806396\n",
      "epoch: 0, batch: 114, loss: 1.8178106546401978\n",
      "epoch: 0, batch: 115, loss: 1.7802258729934692\n",
      "epoch: 0, batch: 116, loss: 1.9963520765304565\n",
      "epoch: 0, batch: 117, loss: 1.721903681755066\n",
      "epoch: 0, batch: 118, loss: 1.6719523668289185\n",
      "epoch: 0, batch: 119, loss: 1.8334190845489502\n",
      "epoch: 0, batch: 120, loss: 1.7802760601043701\n",
      "epoch: 0, batch: 121, loss: 1.6419622898101807\n",
      "epoch: 0, batch: 122, loss: 1.581689715385437\n",
      "epoch: 0, batch: 123, loss: 1.5931230783462524\n",
      "epoch: 0, batch: 124, loss: 1.7696857452392578\n",
      "epoch: 0, batch: 125, loss: 1.655375599861145\n",
      "epoch: 0, batch: 126, loss: 1.7075608968734741\n",
      "epoch: 0, batch: 127, loss: 1.6888262033462524\n",
      "epoch: 0, batch: 128, loss: 1.6796791553497314\n",
      "epoch: 0, batch: 129, loss: 1.647891640663147\n",
      "epoch: 0, batch: 130, loss: 1.6458520889282227\n",
      "epoch: 0, batch: 131, loss: 1.5687566995620728\n",
      "epoch: 0, batch: 132, loss: 1.7461652755737305\n",
      "epoch: 0, batch: 133, loss: 1.4715116024017334\n",
      "epoch: 0, batch: 134, loss: 1.426406979560852\n",
      "epoch: 0, batch: 135, loss: 1.5384577512741089\n",
      "epoch: 0, batch: 136, loss: 1.5553503036499023\n",
      "epoch: 0, batch: 137, loss: 1.4802852869033813\n",
      "epoch: 0, batch: 138, loss: 1.58686363697052\n",
      "epoch: 0, batch: 139, loss: 1.5449652671813965\n",
      "epoch: 0, batch: 140, loss: 1.4846373796463013\n",
      "epoch: 0, batch: 141, loss: 1.4760829210281372\n",
      "epoch: 0, batch: 142, loss: 1.4502489566802979\n",
      "epoch: 0, batch: 143, loss: 1.5258722305297852\n",
      "epoch: 0, batch: 144, loss: 1.692649483680725\n",
      "epoch: 0, batch: 145, loss: 1.5240371227264404\n",
      "epoch: 0, batch: 146, loss: 1.4437249898910522\n",
      "epoch: 0, batch: 147, loss: 1.4808272123336792\n",
      "epoch: 0, batch: 148, loss: 1.476121187210083\n",
      "epoch: 0, batch: 149, loss: 1.5535337924957275\n",
      "epoch: 0, batch: 150, loss: 1.385575532913208\n",
      "epoch: 0, batch: 151, loss: 1.404747724533081\n",
      "epoch: 0, batch: 152, loss: 1.4392871856689453\n",
      "epoch: 0, batch: 153, loss: 1.4649730920791626\n",
      "epoch: 0, batch: 154, loss: 1.3854222297668457\n",
      "epoch: 0, batch: 155, loss: 1.7203102111816406\n",
      "epoch: 0, batch: 156, loss: 1.5110509395599365\n",
      "epoch: 0, batch: 157, loss: 1.4308607578277588\n",
      "epoch: 0, batch: 158, loss: 1.348860502243042\n",
      "epoch: 0, batch: 159, loss: 1.314276099205017\n",
      "epoch: 0, batch: 160, loss: 1.4429281949996948\n",
      "epoch: 0, batch: 161, loss: 1.5356218814849854\n",
      "epoch: 0, batch: 162, loss: 1.4422836303710938\n",
      "epoch: 0, batch: 163, loss: 1.3347290754318237\n",
      "epoch: 0, batch: 164, loss: 1.3251430988311768\n",
      "epoch: 0, batch: 165, loss: 1.3600068092346191\n",
      "epoch: 0, batch: 166, loss: 1.5892159938812256\n",
      "epoch: 0, batch: 167, loss: 1.460339069366455\n",
      "epoch: 0, batch: 168, loss: 1.4207890033721924\n",
      "epoch: 0, batch: 169, loss: 1.2630014419555664\n",
      "epoch: 0, batch: 170, loss: 1.4302210807800293\n",
      "epoch: 0, batch: 171, loss: 1.1936041116714478\n",
      "epoch: 0, batch: 172, loss: 1.5118792057037354\n",
      "epoch: 0, batch: 173, loss: 1.3673672676086426\n",
      "epoch: 0, batch: 174, loss: 1.1570186614990234\n",
      "epoch: 0, batch: 175, loss: 1.4357776641845703\n",
      "epoch: 0, batch: 176, loss: 1.3971786499023438\n",
      "epoch: 0, batch: 177, loss: 1.2239601612091064\n",
      "epoch: 0, batch: 178, loss: 1.4541351795196533\n",
      "epoch: 0, batch: 179, loss: 1.2968733310699463\n",
      "epoch: 0, batch: 180, loss: 1.4215744733810425\n",
      "epoch: 0, batch: 181, loss: 1.2246886491775513\n",
      "epoch: 0, batch: 182, loss: 1.166172742843628\n",
      "epoch: 0, batch: 183, loss: 1.3506923913955688\n",
      "epoch: 0, batch: 184, loss: 1.1995173692703247\n",
      "epoch: 0, batch: 185, loss: 1.1582213640213013\n",
      "epoch: 0, batch: 186, loss: 1.272593379020691\n",
      "epoch: 0, batch: 187, loss: 1.348692774772644\n",
      "epoch: 0, batch: 188, loss: 1.3576602935791016\n",
      "epoch: 0, batch: 189, loss: 1.2407751083374023\n",
      "epoch: 0, batch: 190, loss: 1.375362515449524\n",
      "epoch: 0, batch: 191, loss: 1.6634031534194946\n",
      "epoch: 0, batch: 192, loss: 1.2360402345657349\n",
      "epoch: 0, batch: 193, loss: 1.3376655578613281\n",
      "epoch: 0, batch: 194, loss: 1.197789192199707\n",
      "epoch: 0, batch: 195, loss: 1.2695032358169556\n",
      "epoch: 0, batch: 196, loss: 1.2447701692581177\n",
      "epoch: 0, batch: 197, loss: 1.1246821880340576\n",
      "epoch: 0, batch: 198, loss: 1.1897574663162231\n",
      "epoch: 0, batch: 199, loss: 1.3020507097244263\n",
      "epoch: 0, batch: 200, loss: 1.1884970664978027\n",
      "epoch: 0, batch: 201, loss: 1.3988114595413208\n",
      "epoch: 0, batch: 202, loss: 1.390737771987915\n",
      "epoch: 0, batch: 203, loss: 1.205249309539795\n",
      "epoch: 0, batch: 204, loss: 1.1113364696502686\n",
      "epoch: 0, batch: 205, loss: 1.2734692096710205\n",
      "epoch: 0, batch: 206, loss: 1.1973567008972168\n",
      "epoch: 0, batch: 207, loss: 1.1523441076278687\n",
      "epoch: 0, batch: 208, loss: 1.369702696800232\n",
      "epoch: 0, batch: 209, loss: 1.2317320108413696\n",
      "epoch: 0, batch: 210, loss: 1.3812854290008545\n",
      "epoch: 0, batch: 211, loss: 1.1830121278762817\n",
      "epoch: 0, batch: 212, loss: 1.3933181762695312\n",
      "epoch: 0, batch: 213, loss: 1.1541662216186523\n",
      "epoch: 0, batch: 214, loss: 1.2559400796890259\n",
      "epoch: 0, batch: 215, loss: 1.3135521411895752\n",
      "epoch: 0, batch: 216, loss: 1.0731801986694336\n",
      "epoch: 0, batch: 217, loss: 1.220023274421692\n",
      "epoch: 0, batch: 218, loss: 0.9869096279144287\n",
      "epoch: 0, batch: 219, loss: 1.238098382949829\n",
      "epoch: 0, batch: 220, loss: 1.0938996076583862\n",
      "epoch: 0, batch: 221, loss: 1.1923538446426392\n",
      "epoch: 0, batch: 222, loss: 1.201851725578308\n",
      "epoch: 0, batch: 223, loss: 1.2382662296295166\n",
      "epoch: 0, batch: 224, loss: 1.1817731857299805\n",
      "epoch: 0, batch: 225, loss: 1.3752501010894775\n",
      "epoch: 0, batch: 226, loss: 1.2778315544128418\n",
      "epoch: 0, batch: 227, loss: 1.1803239583969116\n",
      "epoch: 0, batch: 228, loss: 1.2040084600448608\n",
      "epoch: 0, batch: 229, loss: 1.318929672241211\n",
      "epoch: 0, batch: 230, loss: 1.2075868844985962\n",
      "epoch: 0, batch: 231, loss: 1.068609595298767\n",
      "epoch: 0, batch: 232, loss: 1.4524738788604736\n",
      "epoch: 0, batch: 233, loss: 1.2001701593399048\n",
      "epoch: 0, batch: 234, loss: 1.1582618951797485\n",
      "epoch: 0, batch: 235, loss: 1.088810920715332\n",
      "epoch: 0, batch: 236, loss: 1.2084993124008179\n",
      "epoch: 0, batch: 237, loss: 1.3409188985824585\n",
      "epoch: 0, batch: 238, loss: 1.180140495300293\n",
      "epoch: 0, batch: 239, loss: 1.1971527338027954\n",
      "epoch: 0, batch: 240, loss: 1.0417509078979492\n",
      "epoch: 0, batch: 241, loss: 1.1314735412597656\n",
      "epoch: 0, batch: 242, loss: 1.2411558628082275\n",
      "epoch: 0, batch: 243, loss: 1.13555109500885\n",
      "epoch: 0, batch: 244, loss: 1.2626707553863525\n",
      "epoch: 0, batch: 245, loss: 1.3097846508026123\n",
      "epoch: 0, batch: 246, loss: 1.0455114841461182\n",
      "epoch: 0, batch: 247, loss: 1.086451530456543\n",
      "epoch: 0, batch: 248, loss: 1.024060845375061\n",
      "epoch: 0, batch: 249, loss: 1.230947732925415\n",
      "epoch: 0, batch: 250, loss: 1.2745879888534546\n",
      "epoch: 0, batch: 251, loss: 1.0064752101898193\n",
      "epoch: 0, batch: 252, loss: 0.9251050353050232\n",
      "epoch: 0, batch: 253, loss: 1.2259082794189453\n",
      "epoch: 0, batch: 254, loss: 1.1527559757232666\n",
      "epoch: 0, batch: 255, loss: 1.069610357284546\n",
      "epoch: 0, batch: 256, loss: 1.0085883140563965\n",
      "epoch: 0, batch: 257, loss: 1.0462729930877686\n",
      "epoch: 0, batch: 258, loss: 1.0617945194244385\n",
      "epoch: 0, batch: 259, loss: 1.2196447849273682\n",
      "epoch: 0, batch: 260, loss: 1.1290109157562256\n",
      "epoch: 0, batch: 261, loss: 1.0862897634506226\n",
      "epoch: 0, batch: 262, loss: 1.1489050388336182\n",
      "epoch: 0, batch: 263, loss: 1.2809958457946777\n",
      "epoch: 0, batch: 264, loss: 0.9754400253295898\n",
      "epoch: 0, batch: 265, loss: 1.1191297769546509\n",
      "epoch: 0, batch: 266, loss: 1.255253791809082\n",
      "epoch: 0, batch: 267, loss: 1.0458731651306152\n",
      "epoch: 0, batch: 268, loss: 1.2763288021087646\n",
      "epoch: 0, batch: 269, loss: 1.1830588579177856\n",
      "epoch: 0, batch: 270, loss: 0.9647030830383301\n",
      "epoch: 0, batch: 271, loss: 1.083614706993103\n",
      "epoch: 0, batch: 272, loss: 1.2240334749221802\n",
      "epoch: 0, batch: 273, loss: 1.107921838760376\n",
      "epoch: 0, batch: 274, loss: 1.1142206192016602\n",
      "epoch: 0, batch: 275, loss: 1.1994223594665527\n",
      "epoch: 0, batch: 276, loss: 1.1760447025299072\n",
      "epoch: 0, batch: 277, loss: 1.191319465637207\n",
      "epoch: 0, batch: 278, loss: 1.1308879852294922\n",
      "epoch: 0, batch: 279, loss: 0.9104381799697876\n",
      "epoch: 0, batch: 280, loss: 1.3575823307037354\n",
      "epoch: 0, batch: 281, loss: 1.099124789237976\n",
      "epoch: 0, batch: 282, loss: 1.339487910270691\n",
      "epoch: 0, batch: 283, loss: 1.1153476238250732\n",
      "epoch: 0, batch: 284, loss: 0.9759754538536072\n",
      "epoch: 0, batch: 285, loss: 1.1321024894714355\n",
      "epoch: 0, batch: 286, loss: 0.8987303972244263\n",
      "epoch: 0, batch: 287, loss: 0.9033631682395935\n",
      "epoch: 0, batch: 288, loss: 1.0777456760406494\n",
      "epoch: 0, batch: 289, loss: 0.9892086982727051\n",
      "epoch: 0, batch: 290, loss: 1.0577951669692993\n",
      "epoch: 0, batch: 291, loss: 1.239567756652832\n",
      "epoch: 0, batch: 292, loss: 0.9215339422225952\n",
      "epoch: 0, batch: 293, loss: 1.2167510986328125\n",
      "epoch: 0, batch: 294, loss: 1.1044963598251343\n",
      "epoch: 0, batch: 295, loss: 1.0280567407608032\n",
      "epoch: 0, batch: 296, loss: 1.2767928838729858\n",
      "epoch: 0, batch: 297, loss: 1.0490072965621948\n",
      "epoch: 0, batch: 298, loss: 1.165932297706604\n",
      "epoch: 0, batch: 299, loss: 1.1866228580474854\n",
      "epoch: 0, batch: 300, loss: 1.0520917177200317\n",
      "epoch: 0, batch: 301, loss: 1.2974903583526611\n",
      "epoch: 0, batch: 302, loss: 0.9440492987632751\n",
      "epoch: 0, batch: 303, loss: 1.0152400732040405\n",
      "epoch: 0, batch: 304, loss: 1.0890618562698364\n",
      "epoch: 0, batch: 305, loss: 0.9839608669281006\n",
      "epoch: 0, batch: 306, loss: 1.010170340538025\n",
      "epoch: 0, batch: 307, loss: 0.9782801270484924\n",
      "epoch: 0, batch: 308, loss: 1.0482213497161865\n",
      "epoch: 0, batch: 309, loss: 0.9858582615852356\n",
      "epoch: 0, batch: 310, loss: 1.2027192115783691\n",
      "epoch: 0, batch: 311, loss: 1.0161570310592651\n",
      "epoch: 0, batch: 312, loss: 0.9096410274505615\n",
      "epoch: 0, batch: 313, loss: 1.0163865089416504\n",
      "epoch: 0, batch: 314, loss: 0.9939740300178528\n",
      "epoch: 0, batch: 315, loss: 0.916156530380249\n",
      "epoch: 0, batch: 316, loss: 1.0951566696166992\n",
      "epoch: 0, batch: 317, loss: 0.9283509254455566\n",
      "epoch: 0, batch: 318, loss: 0.929722249507904\n",
      "epoch: 0, batch: 319, loss: 1.1041145324707031\n",
      "epoch: 0, batch: 320, loss: 0.9834638833999634\n",
      "epoch: 0, batch: 321, loss: 1.1141220331192017\n",
      "epoch: 0, batch: 322, loss: 0.9857664704322815\n",
      "epoch: 0, batch: 323, loss: 0.9726471304893494\n",
      "epoch: 0, batch: 324, loss: 0.9364287853240967\n",
      "epoch: 0, batch: 325, loss: 1.0199360847473145\n",
      "epoch: 0, batch: 326, loss: 1.1428965330123901\n",
      "epoch: 0, batch: 327, loss: 0.9584127068519592\n",
      "epoch: 0, batch: 328, loss: 1.0940561294555664\n",
      "epoch: 0, batch: 329, loss: 1.1476691961288452\n",
      "epoch: 0, batch: 330, loss: 1.0592048168182373\n",
      "epoch: 0, batch: 331, loss: 1.0064516067504883\n",
      "epoch: 0, batch: 332, loss: 0.9370793104171753\n",
      "epoch: 0, batch: 333, loss: 1.087631344795227\n",
      "epoch: 0, batch: 334, loss: 0.9325010776519775\n",
      "epoch: 0, batch: 335, loss: 0.9537522196769714\n",
      "epoch: 0, batch: 336, loss: 1.0220826864242554\n",
      "epoch: 0, batch: 337, loss: 0.8809029459953308\n",
      "epoch: 0, batch: 338, loss: 0.7833883762359619\n",
      "epoch: 0, batch: 339, loss: 1.0029613971710205\n",
      "epoch: 0, batch: 340, loss: 1.238161325454712\n",
      "epoch: 0, batch: 341, loss: 0.9789218902587891\n",
      "epoch: 0, batch: 342, loss: 1.0490063428878784\n",
      "epoch: 0, batch: 343, loss: 0.8893116116523743\n",
      "epoch: 0, batch: 344, loss: 0.6994603276252747\n",
      "epoch: 0, batch: 345, loss: 1.1309620141983032\n",
      "epoch: 0, batch: 346, loss: 1.0489964485168457\n",
      "epoch: 0, batch: 347, loss: 0.9088391065597534\n",
      "epoch: 0, batch: 348, loss: 1.1771163940429688\n",
      "epoch: 0, batch: 349, loss: 0.8269777297973633\n",
      "epoch: 0, batch: 350, loss: 0.8462833762168884\n",
      "epoch: 0, batch: 351, loss: 1.277211308479309\n",
      "epoch: 0, batch: 352, loss: 0.7949548959732056\n",
      "epoch: 0, batch: 353, loss: 0.8602370619773865\n",
      "epoch: 0, batch: 354, loss: 0.8700621128082275\n",
      "epoch: 0, batch: 355, loss: 0.799207329750061\n",
      "epoch: 0, batch: 356, loss: 0.7140204906463623\n",
      "epoch: 0, batch: 357, loss: 0.9174853563308716\n",
      "epoch: 0, batch: 358, loss: 0.9371719360351562\n",
      "epoch: 0, batch: 359, loss: 1.0408439636230469\n",
      "epoch: 0, batch: 360, loss: 1.022111415863037\n",
      "epoch: 0, batch: 361, loss: 1.0928691625595093\n",
      "epoch: 0, batch: 362, loss: 0.9831546545028687\n",
      "epoch: 0, batch: 363, loss: 0.9354607462882996\n",
      "epoch: 0, batch: 364, loss: 1.1419646739959717\n",
      "epoch: 0, batch: 365, loss: 0.7763317823410034\n",
      "epoch: 0, batch: 366, loss: 0.9077453017234802\n",
      "epoch: 0, batch: 367, loss: 0.8028890490531921\n",
      "epoch: 0, batch: 368, loss: 0.9427814483642578\n",
      "epoch: 0, batch: 369, loss: 0.9750667214393616\n",
      "epoch: 0, batch: 370, loss: 1.0453354120254517\n",
      "epoch: 0, batch: 371, loss: 0.8360117673873901\n",
      "epoch: 0, batch: 372, loss: 0.9029468297958374\n",
      "epoch: 0, batch: 373, loss: 0.9303162097930908\n",
      "epoch: 0, batch: 374, loss: 0.9374539256095886\n",
      "epoch: 0, batch: 375, loss: 0.7379036545753479\n",
      "epoch: 0, batch: 376, loss: 0.9926078915596008\n",
      "epoch: 0, batch: 377, loss: 0.8314324617385864\n",
      "epoch: 0, batch: 378, loss: 1.0270521640777588\n",
      "epoch: 0, batch: 379, loss: 1.1624687910079956\n",
      "epoch: 0, batch: 380, loss: 1.120089054107666\n",
      "epoch: 0, batch: 381, loss: 1.0317918062210083\n",
      "epoch: 0, batch: 382, loss: 1.080046534538269\n",
      "epoch: 0, batch: 383, loss: 0.9137027263641357\n",
      "epoch: 0, batch: 384, loss: 1.1366702318191528\n",
      "epoch: 0, batch: 385, loss: 0.9596333503723145\n",
      "epoch: 0, batch: 386, loss: 0.9510073065757751\n",
      "epoch: 0, batch: 387, loss: 0.849695086479187\n",
      "epoch: 0, batch: 388, loss: 1.0438047647476196\n",
      "epoch: 0, batch: 389, loss: 0.8397226333618164\n",
      "epoch: 0, batch: 390, loss: 0.9797598719596863\n",
      "epoch: 0, batch: 391, loss: 0.823000431060791\n",
      "epoch: 0, batch: 392, loss: 0.8698830008506775\n",
      "epoch: 0, batch: 393, loss: 1.0028796195983887\n",
      "epoch: 0, batch: 394, loss: 1.1668169498443604\n",
      "epoch: 0, batch: 395, loss: 0.8906860947608948\n",
      "epoch: 0, batch: 396, loss: 1.0278316736221313\n",
      "epoch: 0, batch: 397, loss: 0.813861608505249\n",
      "epoch: 0, batch: 398, loss: 0.989856481552124\n",
      "epoch: 0, batch: 399, loss: 0.8515655398368835\n",
      "epoch: 0, batch: 400, loss: 0.8713679313659668\n",
      "epoch: 0, batch: 401, loss: 0.8978471755981445\n",
      "epoch: 0, batch: 402, loss: 0.9449714422225952\n",
      "epoch: 0, batch: 403, loss: 1.3088856935501099\n",
      "epoch: 0, batch: 404, loss: 1.0640990734100342\n",
      "epoch: 0, batch: 405, loss: 0.7730447053909302\n",
      "epoch: 0, batch: 406, loss: 1.133779764175415\n",
      "epoch: 0, batch: 407, loss: 0.9496052861213684\n",
      "epoch: 0, batch: 408, loss: 0.9904087781906128\n",
      "epoch: 0, batch: 409, loss: 0.9514238834381104\n",
      "epoch: 0, batch: 410, loss: 1.0170955657958984\n",
      "epoch: 0, batch: 411, loss: 0.9930346608161926\n",
      "epoch: 0, batch: 412, loss: 0.866362988948822\n",
      "epoch: 0, batch: 413, loss: 1.147711992263794\n",
      "epoch: 0, batch: 414, loss: 0.8355358242988586\n",
      "epoch: 0, batch: 415, loss: 0.9286097288131714\n",
      "epoch: 0, batch: 416, loss: 1.0308160781860352\n",
      "epoch: 0, batch: 417, loss: 0.6755229234695435\n",
      "epoch: 0, batch: 418, loss: 1.0314562320709229\n",
      "epoch: 0, batch: 419, loss: 0.8530779480934143\n",
      "epoch: 0, batch: 420, loss: 0.904349684715271\n",
      "epoch: 0, batch: 421, loss: 1.0531182289123535\n",
      "epoch: 0, batch: 422, loss: 0.9851308465003967\n",
      "epoch: 0, batch: 423, loss: 1.0377918481826782\n",
      "epoch: 0, batch: 424, loss: 0.7527057528495789\n",
      "epoch: 0, batch: 425, loss: 0.7789846658706665\n",
      "epoch: 0, batch: 426, loss: 0.8678267598152161\n",
      "epoch: 0, batch: 427, loss: 0.8706077933311462\n",
      "epoch: 0, batch: 428, loss: 0.8718061447143555\n",
      "epoch: 0, batch: 429, loss: 1.0406584739685059\n",
      "epoch: 0, batch: 430, loss: 1.320746898651123\n",
      "epoch: 0, batch: 431, loss: 1.0061036348342896\n",
      "epoch: 0, batch: 432, loss: 0.8142977356910706\n",
      "epoch: 0, batch: 433, loss: 0.862313449382782\n",
      "epoch: 0, batch: 434, loss: 0.8845114707946777\n",
      "epoch: 0, batch: 435, loss: 0.8533219695091248\n",
      "epoch: 0, batch: 436, loss: 0.9683060050010681\n",
      "epoch: 0, batch: 437, loss: 0.8416868448257446\n",
      "epoch: 0, batch: 438, loss: 0.9831222295761108\n",
      "epoch: 0, batch: 439, loss: 1.0718836784362793\n",
      "epoch: 0, batch: 440, loss: 0.7469609975814819\n",
      "epoch: 0, batch: 441, loss: 0.9269428849220276\n",
      "epoch: 0, batch: 442, loss: 1.0303232669830322\n",
      "epoch: 0, batch: 443, loss: 0.9979287385940552\n",
      "epoch: 0, batch: 444, loss: 1.0600961446762085\n",
      "epoch: 0, batch: 445, loss: 0.9591166973114014\n",
      "epoch: 0, batch: 446, loss: 1.0407123565673828\n",
      "epoch: 0, batch: 447, loss: 1.0384588241577148\n",
      "epoch: 0, batch: 448, loss: 0.8415238261222839\n",
      "epoch: 0, batch: 449, loss: 0.7995041608810425\n",
      "epoch: 0, batch: 450, loss: 1.0199754238128662\n",
      "epoch: 0, batch: 451, loss: 0.7888641357421875\n",
      "epoch: 0, batch: 452, loss: 0.932907223701477\n",
      "epoch: 0, batch: 453, loss: 0.7916914224624634\n",
      "epoch: 0, batch: 454, loss: 1.0982944965362549\n",
      "epoch: 0, batch: 455, loss: 1.0425965785980225\n",
      "epoch: 0, batch: 456, loss: 0.5448801517486572\n",
      "epoch: 0, batch: 457, loss: 0.6881915926933289\n",
      "epoch: 0, batch: 458, loss: 0.7881743311882019\n",
      "epoch: 0, batch: 459, loss: 0.9042248129844666\n",
      "epoch: 0, batch: 460, loss: 0.9378889203071594\n",
      "epoch: 0, batch: 461, loss: 0.7990541458129883\n",
      "epoch: 0, batch: 462, loss: 1.0564463138580322\n",
      "epoch: 0, batch: 463, loss: 0.8613331317901611\n",
      "epoch: 0, batch: 464, loss: 0.6262385249137878\n",
      "epoch: 0, batch: 465, loss: 0.9984445571899414\n",
      "epoch: 0, batch: 466, loss: 0.8139791488647461\n",
      "epoch: 0, batch: 467, loss: 0.8988592028617859\n",
      "epoch: 0, batch: 468, loss: 1.1186354160308838\n",
      "epoch: 0, batch: 469, loss: 0.9268708229064941\n",
      "epoch: 0, batch: 470, loss: 0.9160715937614441\n",
      "epoch: 0, batch: 471, loss: 0.9647907018661499\n",
      "epoch: 0, batch: 472, loss: 0.8220123648643494\n",
      "epoch: 0, batch: 473, loss: 0.8764214515686035\n",
      "epoch: 0, batch: 474, loss: 0.9737261533737183\n",
      "epoch: 0, batch: 475, loss: 0.8610439300537109\n",
      "epoch: 0, batch: 476, loss: 0.9885867834091187\n",
      "epoch: 0, batch: 477, loss: 1.1390769481658936\n",
      "epoch: 0, batch: 478, loss: 0.9430881142616272\n",
      "epoch: 0, batch: 479, loss: 0.8068935871124268\n",
      "epoch: 0, batch: 480, loss: 0.8815891146659851\n",
      "epoch: 0, batch: 481, loss: 0.7938604950904846\n",
      "epoch: 0, batch: 482, loss: 0.9195912480354309\n",
      "epoch: 0, batch: 483, loss: 0.6396159529685974\n",
      "epoch: 0, batch: 484, loss: 0.7825008630752563\n",
      "epoch: 0, batch: 485, loss: 1.0506542921066284\n",
      "epoch: 0, batch: 486, loss: 1.0629973411560059\n",
      "epoch: 0, batch: 487, loss: 0.7633011341094971\n",
      "epoch: 0, batch: 488, loss: 0.9850144982337952\n",
      "epoch: 0, batch: 489, loss: 0.9334777593612671\n",
      "epoch: 0, batch: 490, loss: 0.903025209903717\n",
      "epoch: 0, batch: 491, loss: 0.6455409526824951\n",
      "epoch: 0, batch: 492, loss: 0.7929397821426392\n",
      "epoch: 0, batch: 493, loss: 0.7659155130386353\n",
      "epoch: 0, batch: 494, loss: 0.8277420997619629\n",
      "epoch: 0, batch: 495, loss: 1.0409599542617798\n",
      "epoch: 0, batch: 496, loss: 0.7918431162834167\n",
      "epoch: 0, batch: 497, loss: 0.8425168395042419\n",
      "epoch: 0, batch: 498, loss: 0.7552516460418701\n",
      "epoch: 0, batch: 499, loss: 0.7814395427703857\n",
      "epoch: 0, batch: 500, loss: 0.7890602350234985\n",
      "epoch: 0, batch: 501, loss: 0.8253593444824219\n",
      "epoch: 0, batch: 502, loss: 0.8762898445129395\n",
      "epoch: 0, batch: 503, loss: 0.9304491281509399\n",
      "epoch: 0, batch: 504, loss: 0.9024643301963806\n",
      "epoch: 0, batch: 505, loss: 0.8777626752853394\n",
      "epoch: 0, batch: 506, loss: 0.575520932674408\n",
      "epoch: 0, batch: 507, loss: 1.0410548448562622\n",
      "epoch: 0, batch: 508, loss: 0.570915937423706\n",
      "epoch: 0, batch: 509, loss: 1.0893216133117676\n",
      "epoch: 0, batch: 510, loss: 0.9413384199142456\n",
      "epoch: 0, batch: 511, loss: 0.780678391456604\n",
      "epoch: 0, batch: 512, loss: 0.7250608801841736\n",
      "epoch: 0, batch: 513, loss: 0.7652267217636108\n",
      "epoch: 0, batch: 514, loss: 0.7076606750488281\n",
      "epoch: 0, batch: 515, loss: 0.8294953107833862\n",
      "epoch: 0, batch: 516, loss: 0.7362342476844788\n",
      "epoch: 0, batch: 517, loss: 0.9300170540809631\n",
      "epoch: 0, batch: 518, loss: 0.7832611203193665\n",
      "epoch: 0, batch: 519, loss: 0.8082482218742371\n",
      "epoch: 0, batch: 520, loss: 0.8979560136795044\n",
      "epoch: 0, batch: 521, loss: 0.6640951037406921\n",
      "epoch: 0, batch: 522, loss: 0.9891831874847412\n",
      "epoch: 0, batch: 523, loss: 0.7918881773948669\n",
      "epoch: 0, batch: 524, loss: 0.7911984920501709\n",
      "epoch: 0, batch: 525, loss: 0.71791672706604\n",
      "epoch: 0, batch: 526, loss: 0.6697241067886353\n",
      "epoch: 0, batch: 527, loss: 0.894403874874115\n",
      "epoch: 0, batch: 528, loss: 0.6822541356086731\n",
      "epoch: 0, batch: 529, loss: 0.756858766078949\n",
      "epoch: 0, batch: 530, loss: 0.7281082272529602\n",
      "epoch: 0, batch: 531, loss: 0.9649245142936707\n",
      "epoch: 0, batch: 532, loss: 0.8678985834121704\n",
      "epoch: 0, batch: 533, loss: 0.8610621690750122\n",
      "epoch: 0, batch: 534, loss: 0.8668437004089355\n",
      "epoch: 0, batch: 535, loss: 0.7583162188529968\n",
      "epoch: 0, batch: 536, loss: 1.0217851400375366\n",
      "epoch: 0, batch: 537, loss: 0.9989031553268433\n",
      "epoch: 0, batch: 538, loss: 1.002778172492981\n",
      "epoch: 0, batch: 539, loss: 0.8181878924369812\n",
      "epoch: 0, batch: 540, loss: 0.9845958352088928\n",
      "epoch: 0, batch: 541, loss: 0.9221452474594116\n",
      "epoch: 0, batch: 542, loss: 0.9094778895378113\n",
      "epoch: 0, batch: 543, loss: 0.7885899543762207\n",
      "epoch: 0, batch: 544, loss: 0.9213184714317322\n",
      "epoch: 0, batch: 545, loss: 0.9511034488677979\n",
      "epoch: 0, batch: 546, loss: 0.7618209719657898\n",
      "epoch: 0, batch: 547, loss: 0.6596023440361023\n",
      "epoch: 0, batch: 548, loss: 0.702617883682251\n",
      "epoch: 0, batch: 549, loss: 0.7408691048622131\n",
      "epoch: 0, batch: 550, loss: 0.6147482991218567\n",
      "epoch: 0, batch: 551, loss: 0.8394410610198975\n",
      "epoch: 0, batch: 552, loss: 0.6677073836326599\n",
      "epoch: 0, batch: 553, loss: 0.8565669059753418\n",
      "epoch: 0, batch: 554, loss: 0.8431679606437683\n",
      "epoch: 0, batch: 555, loss: 0.8983988165855408\n",
      "epoch: 0, batch: 556, loss: 1.1407594680786133\n",
      "epoch: 0, batch: 557, loss: 0.804527759552002\n",
      "epoch: 0, batch: 558, loss: 0.8279634118080139\n",
      "epoch: 0, batch: 559, loss: 1.0322657823562622\n",
      "epoch: 0, batch: 560, loss: 0.8947331309318542\n",
      "epoch: 0, batch: 561, loss: 0.7384118437767029\n",
      "epoch: 0, batch: 562, loss: 0.8559982180595398\n",
      "epoch: 0, batch: 563, loss: 0.6291146874427795\n",
      "epoch: 0, batch: 564, loss: 1.0458904504776\n",
      "epoch: 0, batch: 565, loss: 0.8249999284744263\n",
      "epoch: 0, batch: 566, loss: 0.9581740498542786\n",
      "epoch: 0, batch: 567, loss: 0.8293157815933228\n",
      "epoch: 0, batch: 568, loss: 0.7347247004508972\n",
      "epoch: 0, batch: 569, loss: 0.8073082566261292\n",
      "epoch: 0, batch: 570, loss: 1.1074037551879883\n",
      "epoch: 0, batch: 571, loss: 0.8990240097045898\n",
      "epoch: 0, batch: 572, loss: 0.8200454711914062\n",
      "epoch: 0, batch: 573, loss: 0.8415543437004089\n",
      "epoch: 0, batch: 574, loss: 0.8705065250396729\n",
      "epoch: 0, batch: 575, loss: 0.7571765184402466\n",
      "epoch: 0, batch: 576, loss: 0.8382596373558044\n",
      "epoch: 0, batch: 577, loss: 0.7064588665962219\n",
      "epoch: 0, batch: 578, loss: 0.7532588839530945\n",
      "epoch: 0, batch: 579, loss: 0.8439104557037354\n",
      "epoch: 0, batch: 580, loss: 0.8902636170387268\n",
      "epoch: 0, batch: 581, loss: 0.8143459558486938\n",
      "epoch: 0, batch: 582, loss: 1.0068440437316895\n",
      "epoch: 0, batch: 583, loss: 0.7274965047836304\n",
      "epoch: 0, batch: 584, loss: 0.7467378377914429\n",
      "epoch: 0, batch: 585, loss: 0.8814077973365784\n",
      "epoch: 0, batch: 586, loss: 0.9495718479156494\n",
      "epoch: 0, batch: 587, loss: 0.8997471928596497\n",
      "epoch: 0, batch: 588, loss: 0.5603602528572083\n",
      "epoch: 0, batch: 589, loss: 0.6375064849853516\n",
      "epoch: 0, batch: 590, loss: 0.7970537543296814\n",
      "epoch: 0, batch: 591, loss: 0.8853214979171753\n",
      "epoch: 0, batch: 592, loss: 0.627656877040863\n",
      "epoch: 0, batch: 593, loss: 0.9229231476783752\n",
      "epoch: 0, batch: 594, loss: 0.755308985710144\n",
      "epoch: 0, batch: 595, loss: 0.9352602362632751\n",
      "epoch: 0, batch: 596, loss: 1.0399463176727295\n",
      "epoch: 0, batch: 597, loss: 0.8282029628753662\n",
      "epoch: 0, batch: 598, loss: 1.0814688205718994\n",
      "epoch: 0, batch: 599, loss: 0.7133461236953735\n",
      "epoch: 0, batch: 600, loss: 0.6555771231651306\n",
      "epoch: 0, batch: 601, loss: 0.7795029878616333\n",
      "epoch: 0, batch: 602, loss: 0.5859744548797607\n",
      "epoch: 0, batch: 603, loss: 0.9692438840866089\n",
      "epoch: 0, batch: 604, loss: 0.7728330492973328\n",
      "epoch: 0, batch: 605, loss: 1.0042383670806885\n",
      "epoch: 0, batch: 606, loss: 0.923334538936615\n",
      "epoch: 0, batch: 607, loss: 0.8014257550239563\n",
      "epoch: 0, batch: 608, loss: 0.8716611266136169\n",
      "epoch: 0, batch: 609, loss: 0.6653782725334167\n",
      "epoch: 0, batch: 610, loss: 0.7757881879806519\n",
      "epoch: 0, batch: 611, loss: 0.9626696109771729\n",
      "epoch: 0, batch: 612, loss: 0.8196352124214172\n",
      "epoch: 0, batch: 613, loss: 0.8142461776733398\n",
      "epoch: 0, batch: 614, loss: 0.6315782070159912\n",
      "epoch: 0, batch: 615, loss: 0.8707149028778076\n",
      "epoch: 0, batch: 616, loss: 0.8779814839363098\n",
      "epoch: 0, batch: 617, loss: 0.6231192946434021\n",
      "epoch: 0, batch: 618, loss: 0.6960279941558838\n",
      "epoch: 0, batch: 619, loss: 0.8903135657310486\n",
      "epoch: 0, batch: 620, loss: 0.8166043758392334\n",
      "epoch: 0, batch: 621, loss: 0.7307477593421936\n",
      "epoch: 0, batch: 622, loss: 0.7097009420394897\n",
      "epoch: 0, batch: 623, loss: 0.9798139929771423\n",
      "epoch: 0, batch: 624, loss: 0.831171989440918\n",
      "epoch: 0, batch: 625, loss: 0.9505981802940369\n",
      "epoch: 0, batch: 626, loss: 0.8246939778327942\n",
      "epoch: 0, batch: 627, loss: 1.0351883172988892\n",
      "epoch: 0, batch: 628, loss: 1.0865100622177124\n",
      "epoch: 0, batch: 629, loss: 0.7774964570999146\n",
      "epoch: 0, batch: 630, loss: 0.7809275388717651\n",
      "epoch: 0, batch: 631, loss: 0.962739109992981\n",
      "epoch: 0, batch: 632, loss: 1.0215561389923096\n",
      "epoch: 0, batch: 633, loss: 0.9158127903938293\n",
      "epoch: 0, batch: 634, loss: 0.7528275847434998\n",
      "epoch: 0, batch: 635, loss: 0.7196931838989258\n",
      "epoch: 0, batch: 636, loss: 0.8352798819541931\n",
      "epoch: 0, batch: 637, loss: 0.6525390148162842\n",
      "epoch: 0, batch: 638, loss: 0.7064712047576904\n",
      "epoch: 0, batch: 639, loss: 0.9749991297721863\n",
      "epoch: 0, batch: 640, loss: 0.7292254567146301\n",
      "epoch: 0, batch: 641, loss: 0.8340240120887756\n",
      "epoch: 0, batch: 642, loss: 0.7141680121421814\n",
      "epoch: 0, batch: 643, loss: 0.7054657936096191\n",
      "epoch: 0, batch: 644, loss: 0.6547194123268127\n",
      "epoch: 0, batch: 645, loss: 0.834034264087677\n",
      "epoch: 0, batch: 646, loss: 0.9838123917579651\n",
      "epoch: 0, batch: 647, loss: 0.8229333758354187\n",
      "epoch: 0, batch: 648, loss: 1.0092132091522217\n",
      "epoch: 0, batch: 649, loss: 0.7937304973602295\n",
      "epoch: 0, batch: 650, loss: 0.7035564184188843\n",
      "epoch: 0, batch: 651, loss: 0.9530412554740906\n",
      "epoch: 0, batch: 652, loss: 0.5356487035751343\n",
      "epoch: 0, batch: 653, loss: 0.7782387137413025\n",
      "epoch: 0, batch: 654, loss: 0.9501497149467468\n",
      "epoch: 0, batch: 655, loss: 0.6992425918579102\n",
      "epoch: 0, batch: 656, loss: 0.8593283295631409\n",
      "epoch: 0, batch: 657, loss: 0.8766089081764221\n",
      "epoch: 0, batch: 658, loss: 0.7844510078430176\n",
      "epoch: 0, batch: 659, loss: 0.7690228819847107\n",
      "epoch: 0, batch: 660, loss: 0.613167941570282\n",
      "epoch: 0, batch: 661, loss: 0.8071816563606262\n",
      "epoch: 0, batch: 662, loss: 0.7809800505638123\n",
      "epoch: 0, batch: 663, loss: 0.9517295360565186\n",
      "epoch: 0, batch: 664, loss: 0.6599168181419373\n",
      "epoch: 0, batch: 665, loss: 0.7375145554542542\n",
      "epoch: 0, batch: 666, loss: 0.711941123008728\n",
      "epoch: 0, batch: 667, loss: 0.7635775804519653\n",
      "epoch: 0, batch: 668, loss: 0.6491221189498901\n",
      "epoch: 0, batch: 669, loss: 0.7500988245010376\n",
      "epoch: 0, batch: 670, loss: 0.871159553527832\n",
      "epoch: 0, batch: 671, loss: 0.6674771308898926\n",
      "epoch: 0, batch: 672, loss: 0.7585156559944153\n",
      "epoch: 0, batch: 673, loss: 0.6197798252105713\n",
      "epoch: 0, batch: 674, loss: 0.9227118492126465\n",
      "epoch: 0, batch: 675, loss: 0.659413754940033\n",
      "epoch: 0, batch: 676, loss: 0.642978847026825\n",
      "epoch: 0, batch: 677, loss: 0.8007971048355103\n",
      "epoch: 0, batch: 678, loss: 0.6373815536499023\n",
      "epoch: 0, batch: 679, loss: 0.8174226880073547\n",
      "epoch: 0, batch: 680, loss: 0.8668524622917175\n",
      "epoch: 0, batch: 681, loss: 0.6676183342933655\n",
      "epoch: 0, batch: 682, loss: 0.817425549030304\n",
      "epoch: 0, batch: 683, loss: 0.8158048391342163\n",
      "epoch: 0, batch: 684, loss: 0.6176509857177734\n",
      "epoch: 0, batch: 685, loss: 0.7332229018211365\n",
      "epoch: 0, batch: 686, loss: 0.7877936959266663\n",
      "epoch: 0, batch: 687, loss: 0.7396783232688904\n",
      "epoch: 0, batch: 688, loss: 0.7714245915412903\n",
      "epoch: 0, batch: 689, loss: 0.4780382513999939\n",
      "epoch: 0, batch: 690, loss: 0.8535798788070679\n",
      "epoch: 0, batch: 691, loss: 0.7525440454483032\n",
      "epoch: 0, batch: 692, loss: 0.7855217456817627\n",
      "epoch: 0, batch: 693, loss: 0.7521933317184448\n",
      "epoch: 0, batch: 694, loss: 0.7289924621582031\n",
      "epoch: 0, batch: 695, loss: 0.5988554954528809\n",
      "epoch: 0, batch: 696, loss: 0.709976315498352\n",
      "epoch: 0, batch: 697, loss: 0.6112115383148193\n",
      "epoch: 0, batch: 698, loss: 0.8064988255500793\n",
      "epoch: 0, batch: 699, loss: 0.5190061926841736\n",
      "epoch: 0, batch: 700, loss: 0.8285372853279114\n",
      "epoch: 0, batch: 701, loss: 0.5165160894393921\n",
      "epoch: 0, batch: 702, loss: 0.8083877563476562\n",
      "epoch: 0, batch: 703, loss: 0.8002805113792419\n",
      "epoch: 0, batch: 704, loss: 0.6919434070587158\n",
      "epoch: 0, batch: 705, loss: 0.7440934181213379\n",
      "epoch: 0, batch: 706, loss: 0.8352215886116028\n",
      "epoch: 0, batch: 707, loss: 0.8275277614593506\n",
      "epoch: 0, batch: 708, loss: 1.063103199005127\n",
      "epoch: 0, batch: 709, loss: 0.8449910283088684\n",
      "epoch: 0, batch: 710, loss: 0.7387884259223938\n",
      "epoch: 0, batch: 711, loss: 0.6705507040023804\n",
      "epoch: 0, batch: 712, loss: 0.7379853129386902\n",
      "epoch: 0, batch: 713, loss: 0.9312110543251038\n",
      "epoch: 0, batch: 714, loss: 0.6846602559089661\n",
      "epoch: 0, batch: 715, loss: 0.8029261231422424\n",
      "epoch: 0, batch: 716, loss: 0.64113849401474\n",
      "epoch: 0, batch: 717, loss: 0.7709115743637085\n",
      "epoch: 0, batch: 718, loss: 0.676788330078125\n",
      "epoch: 0, batch: 719, loss: 0.8335398435592651\n",
      "epoch: 0, batch: 720, loss: 0.8004967570304871\n",
      "epoch: 0, batch: 721, loss: 0.905368447303772\n",
      "epoch: 0, batch: 722, loss: 0.6769802570343018\n",
      "epoch: 0, batch: 723, loss: 0.6547154784202576\n",
      "epoch: 0, batch: 724, loss: 0.8174616098403931\n",
      "epoch: 0, batch: 725, loss: 0.8662122488021851\n",
      "epoch: 0, batch: 726, loss: 0.5649921298027039\n",
      "epoch: 0, batch: 727, loss: 0.8026124238967896\n",
      "epoch: 0, batch: 728, loss: 0.8621320128440857\n",
      "epoch: 0, batch: 729, loss: 0.6896370649337769\n",
      "epoch: 0, batch: 730, loss: 0.7438389658927917\n",
      "epoch: 0, batch: 731, loss: 0.6403496861457825\n",
      "epoch: 0, batch: 732, loss: 0.7396008968353271\n",
      "epoch: 0, batch: 733, loss: 0.6259081363677979\n",
      "epoch: 0, batch: 734, loss: 1.0247273445129395\n",
      "epoch: 0, batch: 735, loss: 0.8107431530952454\n",
      "epoch: 0, batch: 736, loss: 0.6707239151000977\n",
      "epoch: 0, batch: 737, loss: 0.8109413385391235\n",
      "epoch: 0, batch: 738, loss: 0.7960620522499084\n",
      "epoch: 0, batch: 739, loss: 0.659870982170105\n",
      "epoch: 0, batch: 740, loss: 0.9019023180007935\n",
      "epoch: 0, batch: 741, loss: 0.8263087868690491\n",
      "epoch: 0, batch: 742, loss: 0.945528507232666\n",
      "epoch: 0, batch: 743, loss: 0.6843252182006836\n",
      "epoch: 0, batch: 744, loss: 0.960721492767334\n",
      "epoch: 0, batch: 745, loss: 0.7395530343055725\n",
      "epoch: 0, batch: 746, loss: 0.8188818693161011\n",
      "epoch: 0, batch: 747, loss: 0.7216490507125854\n",
      "epoch: 0, batch: 748, loss: 0.5970867276191711\n",
      "epoch: 0, batch: 749, loss: 0.6629575490951538\n",
      "epoch: 0, batch: 750, loss: 0.9620891809463501\n",
      "epoch: 0, batch: 751, loss: 0.8152241110801697\n",
      "epoch: 0, batch: 752, loss: 0.8161319494247437\n",
      "epoch: 0, batch: 753, loss: 0.8306227922439575\n",
      "epoch: 0, batch: 754, loss: 0.6771479249000549\n",
      "epoch: 0, batch: 755, loss: 0.7889484763145447\n",
      "epoch: 0, batch: 756, loss: 0.795603334903717\n",
      "epoch: 0, batch: 757, loss: 0.7325291037559509\n",
      "epoch: 0, batch: 758, loss: 0.9819434881210327\n",
      "epoch: 0, batch: 759, loss: 0.6153603792190552\n",
      "epoch: 0, batch: 760, loss: 0.6311379671096802\n",
      "epoch: 0, batch: 761, loss: 0.7727699279785156\n",
      "epoch: 0, batch: 762, loss: 0.5959538817405701\n",
      "epoch: 0, batch: 763, loss: 0.9321277141571045\n",
      "epoch: 0, batch: 764, loss: 0.9110969305038452\n",
      "epoch: 0, batch: 765, loss: 0.844383716583252\n",
      "epoch: 0, batch: 766, loss: 0.6552051305770874\n",
      "epoch: 0, batch: 767, loss: 0.7550278306007385\n",
      "epoch: 0, batch: 768, loss: 0.6791329383850098\n",
      "epoch: 0, batch: 769, loss: 0.6731429100036621\n",
      "epoch: 0, batch: 770, loss: 0.6958406567573547\n",
      "epoch: 0, batch: 771, loss: 0.6584830284118652\n",
      "epoch: 0, batch: 772, loss: 0.7302986979484558\n",
      "epoch: 0, batch: 773, loss: 0.7838608026504517\n",
      "epoch: 0, batch: 774, loss: 0.7527965307235718\n",
      "epoch: 0, batch: 775, loss: 0.7153769731521606\n",
      "epoch: 0, batch: 776, loss: 0.8197169899940491\n",
      "epoch: 0, batch: 777, loss: 0.9326828122138977\n",
      "epoch: 0, batch: 778, loss: 0.8147119283676147\n",
      "epoch: 0, batch: 779, loss: 0.6519020199775696\n",
      "epoch: 0, batch: 780, loss: 0.972159743309021\n",
      "epoch: 0, batch: 781, loss: 0.5742741823196411\n",
      "epoch: 0, batch: 782, loss: 0.8165335655212402\n",
      "epoch: 0, batch: 783, loss: 0.7642001509666443\n",
      "epoch: 0, batch: 784, loss: 0.5670412182807922\n",
      "epoch: 0, batch: 785, loss: 0.767905592918396\n",
      "epoch: 0, batch: 786, loss: 0.7380478978157043\n",
      "epoch: 0, batch: 787, loss: 0.7599181532859802\n",
      "epoch: 0, batch: 788, loss: 0.6366952061653137\n",
      "epoch: 0, batch: 789, loss: 0.7340973019599915\n",
      "epoch: 0, batch: 790, loss: 0.7327216267585754\n",
      "epoch: 0, batch: 791, loss: 0.6438593864440918\n",
      "epoch: 0, batch: 792, loss: 0.698854923248291\n",
      "epoch: 0, batch: 793, loss: 0.653757631778717\n",
      "epoch: 0, batch: 794, loss: 1.0149118900299072\n",
      "epoch: 0, batch: 795, loss: 0.796211838722229\n",
      "epoch: 0, batch: 796, loss: 0.8050325512886047\n",
      "epoch: 0, batch: 797, loss: 0.8114531636238098\n",
      "epoch: 0, batch: 798, loss: 0.7148870825767517\n",
      "epoch: 0, batch: 799, loss: 0.6486135721206665\n",
      "epoch: 0, batch: 800, loss: 0.7561521530151367\n",
      "epoch: 0, batch: 801, loss: 0.6956198811531067\n",
      "epoch: 0, batch: 802, loss: 0.8581371307373047\n",
      "epoch: 0, batch: 803, loss: 0.5979377627372742\n",
      "epoch: 0, batch: 804, loss: 0.5801814794540405\n",
      "epoch: 0, batch: 805, loss: 0.7161571979522705\n",
      "epoch: 0, batch: 806, loss: 0.5437078475952148\n",
      "epoch: 0, batch: 807, loss: 1.002549409866333\n",
      "epoch: 0, batch: 808, loss: 0.5805811285972595\n",
      "epoch: 0, batch: 809, loss: 0.9157958030700684\n",
      "epoch: 0, batch: 810, loss: 0.6784143447875977\n",
      "epoch: 0, batch: 811, loss: 0.5876684188842773\n",
      "epoch: 0, batch: 812, loss: 0.6506697535514832\n",
      "epoch: 0, batch: 813, loss: 0.6900425553321838\n",
      "epoch: 0, batch: 814, loss: 0.7277330756187439\n",
      "epoch: 0, batch: 815, loss: 0.6236554384231567\n",
      "epoch: 0, batch: 816, loss: 0.8865089416503906\n",
      "epoch: 0, batch: 817, loss: 0.6511160135269165\n",
      "epoch: 0, batch: 818, loss: 0.7761297225952148\n",
      "epoch: 0, batch: 819, loss: 0.8659673929214478\n",
      "epoch: 0, batch: 820, loss: 0.6846773028373718\n",
      "epoch: 0, batch: 821, loss: 0.8931309580802917\n",
      "epoch: 0, batch: 822, loss: 0.7711369395256042\n",
      "epoch: 0, batch: 823, loss: 0.7630757093429565\n",
      "epoch: 0, batch: 824, loss: 0.6944139003753662\n",
      "epoch: 0, batch: 825, loss: 1.0985413789749146\n",
      "epoch: 0, batch: 826, loss: 0.7603954672813416\n",
      "epoch: 0, batch: 827, loss: 0.7691981792449951\n",
      "epoch: 0, batch: 828, loss: 0.6689008474349976\n",
      "epoch: 0, batch: 829, loss: 0.657939076423645\n",
      "epoch: 0, batch: 830, loss: 0.8221916556358337\n",
      "epoch: 0, batch: 831, loss: 0.7288262248039246\n",
      "epoch: 0, batch: 832, loss: 0.610312283039093\n",
      "epoch: 0, batch: 833, loss: 0.5857396125793457\n",
      "epoch: 0, batch: 834, loss: 0.7486446499824524\n",
      "epoch: 0, batch: 835, loss: 0.7109788060188293\n",
      "epoch: 0, batch: 836, loss: 0.5727352499961853\n",
      "epoch: 0, batch: 837, loss: 0.7782803773880005\n",
      "epoch: 0, batch: 838, loss: 0.8171029090881348\n",
      "epoch: 0, batch: 839, loss: 0.7677295804023743\n",
      "epoch: 0, batch: 840, loss: 0.7914759516716003\n",
      "epoch: 0, batch: 841, loss: 0.9509565234184265\n",
      "epoch: 0, batch: 842, loss: 0.4342167377471924\n",
      "epoch: 0, batch: 843, loss: 0.629704475402832\n",
      "epoch: 0, batch: 844, loss: 0.6724401712417603\n",
      "epoch: 0, batch: 845, loss: 0.5539336204528809\n",
      "epoch: 0, batch: 846, loss: 0.472649484872818\n",
      "epoch: 0, batch: 847, loss: 0.8777389526367188\n",
      "epoch: 0, batch: 848, loss: 0.7029644250869751\n",
      "epoch: 0, batch: 849, loss: 0.5870343446731567\n",
      "epoch: 0, batch: 850, loss: 0.7661938667297363\n",
      "epoch: 0, batch: 851, loss: 0.5915254354476929\n",
      "epoch: 0, batch: 852, loss: 0.6442058682441711\n",
      "epoch: 0, batch: 853, loss: 0.8091052174568176\n",
      "epoch: 0, batch: 854, loss: 0.5704653859138489\n",
      "epoch: 0, batch: 855, loss: 0.8818761706352234\n",
      "epoch: 0, batch: 856, loss: 0.8574461340904236\n",
      "epoch: 0, batch: 857, loss: 0.9242495894432068\n",
      "epoch: 0, batch: 858, loss: 0.6550664901733398\n",
      "epoch: 0, batch: 859, loss: 0.9551442265510559\n",
      "epoch: 0, batch: 860, loss: 0.7139517664909363\n",
      "epoch: 0, batch: 861, loss: 0.6215149164199829\n",
      "epoch: 0, batch: 862, loss: 0.5800972580909729\n",
      "epoch: 0, batch: 863, loss: 0.5297791361808777\n",
      "epoch: 0, batch: 864, loss: 0.7438672184944153\n",
      "epoch: 0, batch: 865, loss: 0.7720050811767578\n",
      "epoch: 0, batch: 866, loss: 0.7224422693252563\n",
      "epoch: 0, batch: 867, loss: 0.5439376831054688\n",
      "epoch: 0, batch: 868, loss: 0.6277500987052917\n",
      "epoch: 0, batch: 869, loss: 0.9258909225463867\n",
      "epoch: 0, batch: 870, loss: 0.8071838021278381\n",
      "epoch: 0, batch: 871, loss: 0.6642545461654663\n",
      "epoch: 0, batch: 872, loss: 0.7921798229217529\n",
      "epoch: 0, batch: 873, loss: 1.0268890857696533\n",
      "epoch: 0, batch: 874, loss: 0.7316058278083801\n",
      "epoch: 0, batch: 875, loss: 0.6848043203353882\n",
      "epoch: 0, batch: 876, loss: 0.8134974837303162\n",
      "epoch: 0, batch: 877, loss: 0.7676675319671631\n",
      "epoch: 0, batch: 878, loss: 0.9706152677536011\n",
      "epoch: 0, batch: 879, loss: 0.6731021404266357\n",
      "epoch: 0, batch: 880, loss: 0.7533789277076721\n",
      "epoch: 0, batch: 881, loss: 0.7707843780517578\n",
      "epoch: 0, batch: 882, loss: 0.7362812161445618\n",
      "epoch: 0, batch: 883, loss: 0.6659755706787109\n",
      "epoch: 0, batch: 884, loss: 0.5302334427833557\n",
      "epoch: 0, batch: 885, loss: 0.787963330745697\n",
      "epoch: 0, batch: 886, loss: 0.632568359375\n",
      "epoch: 0, batch: 887, loss: 0.5649731755256653\n",
      "epoch: 0, batch: 888, loss: 0.7129518389701843\n",
      "epoch: 0, batch: 889, loss: 0.7615432739257812\n",
      "epoch: 0, batch: 890, loss: 0.798108696937561\n",
      "epoch: 0, batch: 891, loss: 0.7600536942481995\n",
      "epoch: 0, batch: 892, loss: 0.849535346031189\n",
      "epoch: 0, batch: 893, loss: 0.9381954669952393\n",
      "epoch: 0, batch: 894, loss: 0.7228377461433411\n",
      "epoch: 0, batch: 895, loss: 0.7783657908439636\n",
      "epoch: 0, batch: 896, loss: 0.7322542667388916\n",
      "epoch: 0, batch: 897, loss: 0.822607159614563\n",
      "epoch: 0, batch: 898, loss: 0.6729755997657776\n",
      "epoch: 0, batch: 899, loss: 0.6026899814605713\n",
      "epoch: 0, batch: 900, loss: 0.7992851138114929\n",
      "epoch: 0, batch: 901, loss: 0.6646857857704163\n",
      "epoch: 0, batch: 902, loss: 0.6016237139701843\n",
      "epoch: 0, batch: 903, loss: 0.7776244282722473\n",
      "epoch: 0, batch: 904, loss: 0.6439327597618103\n",
      "epoch: 0, batch: 905, loss: 0.7220354080200195\n",
      "epoch: 0, batch: 906, loss: 0.7878730893135071\n",
      "epoch: 0, batch: 907, loss: 0.9790181517601013\n",
      "epoch: 0, batch: 908, loss: 0.6820724010467529\n",
      "epoch: 0, batch: 909, loss: 0.7700801491737366\n",
      "epoch: 0, batch: 910, loss: 0.6111581325531006\n",
      "epoch: 0, batch: 911, loss: 0.8070366978645325\n",
      "epoch: 0, batch: 912, loss: 0.7261095643043518\n",
      "epoch: 0, batch: 913, loss: 0.5855169296264648\n",
      "epoch: 0, batch: 914, loss: 0.7336229085922241\n",
      "epoch: 0, batch: 915, loss: 0.6029108166694641\n",
      "epoch: 0, batch: 916, loss: 0.5827004909515381\n",
      "epoch: 0, batch: 917, loss: 0.6322264075279236\n",
      "epoch: 0, batch: 918, loss: 0.7546430826187134\n",
      "epoch: 0, batch: 919, loss: 0.914566159248352\n",
      "epoch: 0, batch: 920, loss: 0.6154603362083435\n",
      "epoch: 0, batch: 921, loss: 0.7532714605331421\n",
      "epoch: 0, batch: 922, loss: 0.7263948321342468\n",
      "epoch: 0, batch: 923, loss: 0.6955384612083435\n",
      "epoch: 0, batch: 924, loss: 0.51679927110672\n",
      "epoch: 0, batch: 925, loss: 0.47925475239753723\n",
      "epoch: 0, batch: 926, loss: 0.6115169525146484\n",
      "epoch: 0, batch: 927, loss: 0.5245960354804993\n",
      "epoch: 0, batch: 928, loss: 0.7937642335891724\n",
      "epoch: 0, batch: 929, loss: 0.5721009373664856\n",
      "epoch: 0, batch: 930, loss: 0.7654610872268677\n",
      "epoch: 0, batch: 931, loss: 0.8236327767372131\n",
      "epoch: 0, batch: 932, loss: 0.8218972682952881\n",
      "epoch: 0, batch: 933, loss: 0.7910231351852417\n",
      "epoch: 0, batch: 934, loss: 0.7482237219810486\n",
      "epoch: 0, batch: 935, loss: 0.7180531620979309\n",
      "epoch: 0, batch: 936, loss: 0.7188901901245117\n",
      "epoch: 0, batch: 937, loss: 1.1604968309402466\n",
      "epoch: 1, batch: 0, loss: 0.740506112575531\n",
      "epoch: 1, batch: 1, loss: 0.7015790343284607\n",
      "epoch: 1, batch: 2, loss: 0.5902663469314575\n",
      "epoch: 1, batch: 3, loss: 0.6472380757331848\n",
      "epoch: 1, batch: 4, loss: 0.653981626033783\n",
      "epoch: 1, batch: 5, loss: 1.0850356817245483\n",
      "epoch: 1, batch: 6, loss: 0.8618766665458679\n",
      "epoch: 1, batch: 7, loss: 0.7106226086616516\n",
      "epoch: 1, batch: 8, loss: 0.5618124008178711\n",
      "epoch: 1, batch: 9, loss: 0.6665312051773071\n",
      "epoch: 1, batch: 10, loss: 0.6176788210868835\n",
      "epoch: 1, batch: 11, loss: 0.6502041816711426\n",
      "epoch: 1, batch: 12, loss: 0.8140013813972473\n",
      "epoch: 1, batch: 13, loss: 0.7024194002151489\n",
      "epoch: 1, batch: 14, loss: 0.6231816411018372\n",
      "epoch: 1, batch: 15, loss: 0.6831691861152649\n",
      "epoch: 1, batch: 16, loss: 0.6260703206062317\n",
      "epoch: 1, batch: 17, loss: 0.6761887073516846\n",
      "epoch: 1, batch: 18, loss: 0.5417426228523254\n",
      "epoch: 1, batch: 19, loss: 0.6679885387420654\n",
      "epoch: 1, batch: 20, loss: 0.6671072244644165\n",
      "epoch: 1, batch: 21, loss: 0.8325752019882202\n",
      "epoch: 1, batch: 22, loss: 0.7241923809051514\n",
      "epoch: 1, batch: 23, loss: 0.5711545944213867\n",
      "epoch: 1, batch: 24, loss: 0.6429403424263\n",
      "epoch: 1, batch: 25, loss: 0.7047582864761353\n",
      "epoch: 1, batch: 26, loss: 0.8176189661026001\n",
      "epoch: 1, batch: 27, loss: 0.7954264879226685\n",
      "epoch: 1, batch: 28, loss: 0.590960681438446\n",
      "epoch: 1, batch: 29, loss: 0.6218693852424622\n",
      "epoch: 1, batch: 30, loss: 0.8068403601646423\n",
      "epoch: 1, batch: 31, loss: 0.8241593837738037\n",
      "epoch: 1, batch: 32, loss: 0.6995424032211304\n",
      "epoch: 1, batch: 33, loss: 0.8892107009887695\n",
      "epoch: 1, batch: 34, loss: 0.41751599311828613\n",
      "epoch: 1, batch: 35, loss: 0.8462979793548584\n",
      "epoch: 1, batch: 36, loss: 0.6323544979095459\n",
      "epoch: 1, batch: 37, loss: 0.706712007522583\n",
      "epoch: 1, batch: 38, loss: 0.5315294861793518\n",
      "epoch: 1, batch: 39, loss: 0.5996609330177307\n",
      "epoch: 1, batch: 40, loss: 0.8168326616287231\n",
      "epoch: 1, batch: 41, loss: 0.7051339745521545\n",
      "epoch: 1, batch: 42, loss: 0.6181656718254089\n",
      "epoch: 1, batch: 43, loss: 0.7563518285751343\n",
      "epoch: 1, batch: 44, loss: 0.5804468393325806\n",
      "epoch: 1, batch: 45, loss: 0.6396322846412659\n",
      "epoch: 1, batch: 46, loss: 0.7654063701629639\n",
      "epoch: 1, batch: 47, loss: 0.7536285519599915\n",
      "epoch: 1, batch: 48, loss: 0.5573934316635132\n",
      "epoch: 1, batch: 49, loss: 0.7887012362480164\n",
      "epoch: 1, batch: 50, loss: 0.701375424861908\n",
      "epoch: 1, batch: 51, loss: 0.8414835333824158\n",
      "epoch: 1, batch: 52, loss: 0.6747517585754395\n",
      "epoch: 1, batch: 53, loss: 0.7975046038627625\n",
      "epoch: 1, batch: 54, loss: 0.5986430644989014\n",
      "epoch: 1, batch: 55, loss: 0.669621467590332\n",
      "epoch: 1, batch: 56, loss: 0.6340700387954712\n",
      "epoch: 1, batch: 57, loss: 0.7194711565971375\n",
      "epoch: 1, batch: 58, loss: 0.5347901582717896\n",
      "epoch: 1, batch: 59, loss: 0.9124093651771545\n",
      "epoch: 1, batch: 60, loss: 0.9821004867553711\n",
      "epoch: 1, batch: 61, loss: 0.8515711426734924\n",
      "epoch: 1, batch: 62, loss: 0.7485508322715759\n",
      "epoch: 1, batch: 63, loss: 0.8295571804046631\n",
      "epoch: 1, batch: 64, loss: 0.5997763872146606\n",
      "epoch: 1, batch: 65, loss: 0.8614240288734436\n",
      "epoch: 1, batch: 66, loss: 0.7824632525444031\n",
      "epoch: 1, batch: 67, loss: 0.7816250324249268\n",
      "epoch: 1, batch: 68, loss: 0.594514787197113\n",
      "epoch: 1, batch: 69, loss: 0.647492527961731\n",
      "epoch: 1, batch: 70, loss: 0.8168882131576538\n",
      "epoch: 1, batch: 71, loss: 0.5701314210891724\n",
      "epoch: 1, batch: 72, loss: 0.5212563276290894\n",
      "epoch: 1, batch: 73, loss: 0.6815020442008972\n",
      "epoch: 1, batch: 74, loss: 0.7401490807533264\n",
      "epoch: 1, batch: 75, loss: 0.7042409181594849\n",
      "epoch: 1, batch: 76, loss: 0.713654637336731\n",
      "epoch: 1, batch: 77, loss: 0.5395867228507996\n",
      "epoch: 1, batch: 78, loss: 0.7288284301757812\n",
      "epoch: 1, batch: 79, loss: 0.6939618587493896\n",
      "epoch: 1, batch: 80, loss: 0.599277138710022\n",
      "epoch: 1, batch: 81, loss: 0.613514244556427\n",
      "epoch: 1, batch: 82, loss: 0.9022721648216248\n",
      "epoch: 1, batch: 83, loss: 0.6502653360366821\n",
      "epoch: 1, batch: 84, loss: 0.6337706446647644\n",
      "epoch: 1, batch: 85, loss: 0.6080200672149658\n",
      "epoch: 1, batch: 86, loss: 0.6047120690345764\n",
      "epoch: 1, batch: 87, loss: 0.7683084011077881\n",
      "epoch: 1, batch: 88, loss: 0.45865508913993835\n",
      "epoch: 1, batch: 89, loss: 0.6737924814224243\n",
      "epoch: 1, batch: 90, loss: 0.44163623452186584\n",
      "epoch: 1, batch: 91, loss: 0.600254476070404\n",
      "epoch: 1, batch: 92, loss: 0.5887824296951294\n",
      "epoch: 1, batch: 93, loss: 0.6095597147941589\n",
      "epoch: 1, batch: 94, loss: 0.5317862629890442\n",
      "epoch: 1, batch: 95, loss: 0.5901641249656677\n",
      "epoch: 1, batch: 96, loss: 0.7017679214477539\n",
      "epoch: 1, batch: 97, loss: 0.6839812994003296\n",
      "epoch: 1, batch: 98, loss: 0.5924167037010193\n",
      "epoch: 1, batch: 99, loss: 0.7371506094932556\n",
      "epoch: 1, batch: 100, loss: 0.6246510744094849\n",
      "epoch: 1, batch: 101, loss: 0.5834612250328064\n",
      "epoch: 1, batch: 102, loss: 0.6017162799835205\n",
      "epoch: 1, batch: 103, loss: 0.8407095074653625\n",
      "epoch: 1, batch: 104, loss: 0.6922211647033691\n",
      "epoch: 1, batch: 105, loss: 0.7595674991607666\n",
      "epoch: 1, batch: 106, loss: 0.5708414316177368\n",
      "epoch: 1, batch: 107, loss: 0.6732168197631836\n",
      "epoch: 1, batch: 108, loss: 0.6605618000030518\n",
      "epoch: 1, batch: 109, loss: 1.0593688488006592\n",
      "epoch: 1, batch: 110, loss: 0.5227676033973694\n",
      "epoch: 1, batch: 111, loss: 0.48663726449012756\n",
      "epoch: 1, batch: 112, loss: 0.619326114654541\n",
      "epoch: 1, batch: 113, loss: 0.5001460909843445\n",
      "epoch: 1, batch: 114, loss: 0.8579114079475403\n",
      "epoch: 1, batch: 115, loss: 0.5651218295097351\n",
      "epoch: 1, batch: 116, loss: 0.6085681915283203\n",
      "epoch: 1, batch: 117, loss: 0.6319696307182312\n",
      "epoch: 1, batch: 118, loss: 0.6276153922080994\n",
      "epoch: 1, batch: 119, loss: 0.8034191727638245\n",
      "epoch: 1, batch: 120, loss: 0.4665743410587311\n",
      "epoch: 1, batch: 121, loss: 0.7468399405479431\n",
      "epoch: 1, batch: 122, loss: 0.6563321352005005\n",
      "epoch: 1, batch: 123, loss: 0.8561921119689941\n",
      "epoch: 1, batch: 124, loss: 0.43093734979629517\n",
      "epoch: 1, batch: 125, loss: 0.9129977822303772\n",
      "epoch: 1, batch: 126, loss: 0.6425521373748779\n",
      "epoch: 1, batch: 127, loss: 0.7119592428207397\n",
      "epoch: 1, batch: 128, loss: 0.5666800737380981\n",
      "epoch: 1, batch: 129, loss: 0.7809072732925415\n",
      "epoch: 1, batch: 130, loss: 0.6239266395568848\n",
      "epoch: 1, batch: 131, loss: 0.5031956434249878\n",
      "epoch: 1, batch: 132, loss: 0.905891478061676\n",
      "epoch: 1, batch: 133, loss: 0.8988009095191956\n",
      "epoch: 1, batch: 134, loss: 0.649151086807251\n",
      "epoch: 1, batch: 135, loss: 0.7770205140113831\n",
      "epoch: 1, batch: 136, loss: 0.8430459499359131\n",
      "epoch: 1, batch: 137, loss: 0.901796281337738\n",
      "epoch: 1, batch: 138, loss: 0.8831487894058228\n",
      "epoch: 1, batch: 139, loss: 0.5715193152427673\n",
      "epoch: 1, batch: 140, loss: 0.5569022297859192\n",
      "epoch: 1, batch: 141, loss: 0.670249879360199\n",
      "epoch: 1, batch: 142, loss: 0.5871357321739197\n",
      "epoch: 1, batch: 143, loss: 0.5885408520698547\n",
      "epoch: 1, batch: 144, loss: 0.5942531228065491\n",
      "epoch: 1, batch: 145, loss: 0.7764942646026611\n",
      "epoch: 1, batch: 146, loss: 0.5199446082115173\n",
      "epoch: 1, batch: 147, loss: 0.7610796689987183\n",
      "epoch: 1, batch: 148, loss: 0.6887348890304565\n",
      "epoch: 1, batch: 149, loss: 0.5266854166984558\n",
      "epoch: 1, batch: 150, loss: 0.7105531096458435\n",
      "epoch: 1, batch: 151, loss: 0.5433283448219299\n",
      "epoch: 1, batch: 152, loss: 0.6534623503684998\n",
      "epoch: 1, batch: 153, loss: 0.7329738736152649\n",
      "epoch: 1, batch: 154, loss: 0.5309185981750488\n",
      "epoch: 1, batch: 155, loss: 0.7127376794815063\n",
      "epoch: 1, batch: 156, loss: 0.6918185949325562\n",
      "epoch: 1, batch: 157, loss: 0.6295596361160278\n",
      "epoch: 1, batch: 158, loss: 0.777587890625\n",
      "epoch: 1, batch: 159, loss: 0.5505018830299377\n",
      "epoch: 1, batch: 160, loss: 0.6598169207572937\n",
      "epoch: 1, batch: 161, loss: 0.7655023336410522\n",
      "epoch: 1, batch: 162, loss: 0.6559150815010071\n",
      "epoch: 1, batch: 163, loss: 0.5548658967018127\n",
      "epoch: 1, batch: 164, loss: 0.988586962223053\n",
      "epoch: 1, batch: 165, loss: 0.4315558671951294\n",
      "epoch: 1, batch: 166, loss: 0.6609840989112854\n",
      "epoch: 1, batch: 167, loss: 0.7468975782394409\n",
      "epoch: 1, batch: 168, loss: 0.6586302518844604\n",
      "epoch: 1, batch: 169, loss: 0.7529932260513306\n",
      "epoch: 1, batch: 170, loss: 0.5714548230171204\n",
      "epoch: 1, batch: 171, loss: 0.8430637121200562\n",
      "epoch: 1, batch: 172, loss: 0.5918173789978027\n",
      "epoch: 1, batch: 173, loss: 0.6094490885734558\n",
      "epoch: 1, batch: 174, loss: 0.6440728902816772\n",
      "epoch: 1, batch: 175, loss: 0.5195815563201904\n",
      "epoch: 1, batch: 176, loss: 0.5279369950294495\n",
      "epoch: 1, batch: 177, loss: 0.5898895859718323\n",
      "epoch: 1, batch: 178, loss: 0.721001386642456\n",
      "epoch: 1, batch: 179, loss: 0.5976427793502808\n",
      "epoch: 1, batch: 180, loss: 0.8362808227539062\n",
      "epoch: 1, batch: 181, loss: 1.0874054431915283\n",
      "epoch: 1, batch: 182, loss: 0.8118271231651306\n",
      "epoch: 1, batch: 183, loss: 0.5580382943153381\n",
      "epoch: 1, batch: 184, loss: 0.9068452715873718\n",
      "epoch: 1, batch: 185, loss: 0.8925943970680237\n",
      "epoch: 1, batch: 186, loss: 0.6657653450965881\n",
      "epoch: 1, batch: 187, loss: 0.631895899772644\n",
      "epoch: 1, batch: 188, loss: 0.7270079851150513\n",
      "epoch: 1, batch: 189, loss: 0.6725121140480042\n",
      "epoch: 1, batch: 190, loss: 0.8010933995246887\n",
      "epoch: 1, batch: 191, loss: 0.6883014440536499\n",
      "epoch: 1, batch: 192, loss: 0.6717056632041931\n",
      "epoch: 1, batch: 193, loss: 0.8304725289344788\n",
      "epoch: 1, batch: 194, loss: 0.724053144454956\n",
      "epoch: 1, batch: 195, loss: 0.748822033405304\n",
      "epoch: 1, batch: 196, loss: 0.5635024905204773\n",
      "epoch: 1, batch: 197, loss: 0.5073608160018921\n",
      "epoch: 1, batch: 198, loss: 0.5687053203582764\n",
      "epoch: 1, batch: 199, loss: 0.5106127262115479\n",
      "epoch: 1, batch: 200, loss: 0.8386935591697693\n",
      "epoch: 1, batch: 201, loss: 0.5114411115646362\n",
      "epoch: 1, batch: 202, loss: 0.6197649836540222\n",
      "epoch: 1, batch: 203, loss: 0.532609224319458\n",
      "epoch: 1, batch: 204, loss: 0.5281940698623657\n",
      "epoch: 1, batch: 205, loss: 0.6322266459465027\n",
      "epoch: 1, batch: 206, loss: 0.6319254040718079\n",
      "epoch: 1, batch: 207, loss: 0.8977973461151123\n",
      "epoch: 1, batch: 208, loss: 0.6104464530944824\n",
      "epoch: 1, batch: 209, loss: 0.890088677406311\n",
      "epoch: 1, batch: 210, loss: 0.6973568201065063\n",
      "epoch: 1, batch: 211, loss: 0.488523930311203\n",
      "epoch: 1, batch: 212, loss: 0.623903751373291\n",
      "epoch: 1, batch: 213, loss: 0.5284537076950073\n",
      "epoch: 1, batch: 214, loss: 0.7313582897186279\n",
      "epoch: 1, batch: 215, loss: 0.7114805579185486\n",
      "epoch: 1, batch: 216, loss: 0.6197264790534973\n",
      "epoch: 1, batch: 217, loss: 0.6241298913955688\n",
      "epoch: 1, batch: 218, loss: 0.6294564604759216\n",
      "epoch: 1, batch: 219, loss: 0.7454565763473511\n",
      "epoch: 1, batch: 220, loss: 0.6709036827087402\n",
      "epoch: 1, batch: 221, loss: 0.5338643193244934\n",
      "epoch: 1, batch: 222, loss: 0.6988276839256287\n",
      "epoch: 1, batch: 223, loss: 0.8957318663597107\n",
      "epoch: 1, batch: 224, loss: 0.602286159992218\n",
      "epoch: 1, batch: 225, loss: 0.36733749508857727\n",
      "epoch: 1, batch: 226, loss: 0.7624832391738892\n",
      "epoch: 1, batch: 227, loss: 0.7361294627189636\n",
      "epoch: 1, batch: 228, loss: 0.6775880455970764\n",
      "epoch: 1, batch: 229, loss: 0.5453199148178101\n",
      "epoch: 1, batch: 230, loss: 0.5908530950546265\n",
      "epoch: 1, batch: 231, loss: 0.6130958795547485\n",
      "epoch: 1, batch: 232, loss: 0.826117992401123\n",
      "epoch: 1, batch: 233, loss: 0.5976323485374451\n",
      "epoch: 1, batch: 234, loss: 0.4239133298397064\n",
      "epoch: 1, batch: 235, loss: 0.7239933013916016\n",
      "epoch: 1, batch: 236, loss: 0.542134702205658\n",
      "epoch: 1, batch: 237, loss: 0.5219020843505859\n",
      "epoch: 1, batch: 238, loss: 0.5132855176925659\n",
      "epoch: 1, batch: 239, loss: 0.5433089137077332\n",
      "epoch: 1, batch: 240, loss: 0.6464313864707947\n",
      "epoch: 1, batch: 241, loss: 0.6576828956604004\n",
      "epoch: 1, batch: 242, loss: 0.5520462989807129\n",
      "epoch: 1, batch: 243, loss: 0.8642836809158325\n",
      "epoch: 1, batch: 244, loss: 0.504578709602356\n",
      "epoch: 1, batch: 245, loss: 0.518262505531311\n",
      "epoch: 1, batch: 246, loss: 0.7153326272964478\n",
      "epoch: 1, batch: 247, loss: 0.6806699633598328\n",
      "epoch: 1, batch: 248, loss: 0.5922834873199463\n",
      "epoch: 1, batch: 249, loss: 0.6287435293197632\n",
      "epoch: 1, batch: 250, loss: 0.8919623494148254\n",
      "epoch: 1, batch: 251, loss: 0.7633413076400757\n",
      "epoch: 1, batch: 252, loss: 0.9314155578613281\n",
      "epoch: 1, batch: 253, loss: 0.6938950419425964\n",
      "epoch: 1, batch: 254, loss: 0.5334144830703735\n",
      "epoch: 1, batch: 255, loss: 0.5921983122825623\n",
      "epoch: 1, batch: 256, loss: 0.5786039233207703\n",
      "epoch: 1, batch: 257, loss: 0.8630262017250061\n",
      "epoch: 1, batch: 258, loss: 0.48664969205856323\n",
      "epoch: 1, batch: 259, loss: 0.5121893882751465\n",
      "epoch: 1, batch: 260, loss: 0.8305776119232178\n",
      "epoch: 1, batch: 261, loss: 0.6488875150680542\n",
      "epoch: 1, batch: 262, loss: 0.7842345237731934\n",
      "epoch: 1, batch: 263, loss: 0.5592083930969238\n",
      "epoch: 1, batch: 264, loss: 0.5829644799232483\n",
      "epoch: 1, batch: 265, loss: 0.7514780163764954\n",
      "epoch: 1, batch: 266, loss: 0.6843960881233215\n",
      "epoch: 1, batch: 267, loss: 0.70406174659729\n",
      "epoch: 1, batch: 268, loss: 0.5323201417922974\n",
      "epoch: 1, batch: 269, loss: 0.5412523150444031\n",
      "epoch: 1, batch: 270, loss: 0.5106776356697083\n",
      "epoch: 1, batch: 271, loss: 0.7095651626586914\n",
      "epoch: 1, batch: 272, loss: 0.5002948641777039\n",
      "epoch: 1, batch: 273, loss: 0.5672522783279419\n",
      "epoch: 1, batch: 274, loss: 0.6007798910140991\n",
      "epoch: 1, batch: 275, loss: 0.5517644286155701\n",
      "epoch: 1, batch: 276, loss: 0.7255421876907349\n",
      "epoch: 1, batch: 277, loss: 0.6878431439399719\n",
      "epoch: 1, batch: 278, loss: 0.8940702676773071\n",
      "epoch: 1, batch: 279, loss: 0.5462208986282349\n",
      "epoch: 1, batch: 280, loss: 0.4733728766441345\n",
      "epoch: 1, batch: 281, loss: 0.6626720428466797\n",
      "epoch: 1, batch: 282, loss: 0.5690591335296631\n",
      "epoch: 1, batch: 283, loss: 0.47665083408355713\n",
      "epoch: 1, batch: 284, loss: 0.454677551984787\n",
      "epoch: 1, batch: 285, loss: 0.6095452904701233\n",
      "epoch: 1, batch: 286, loss: 0.7465255260467529\n",
      "epoch: 1, batch: 287, loss: 0.5144181251525879\n",
      "epoch: 1, batch: 288, loss: 0.6591373682022095\n",
      "epoch: 1, batch: 289, loss: 0.5760012865066528\n",
      "epoch: 1, batch: 290, loss: 0.4482950270175934\n",
      "epoch: 1, batch: 291, loss: 0.44544562697410583\n",
      "epoch: 1, batch: 292, loss: 0.543272078037262\n",
      "epoch: 1, batch: 293, loss: 0.5837838649749756\n",
      "epoch: 1, batch: 294, loss: 0.5213297009468079\n",
      "epoch: 1, batch: 295, loss: 0.5313795208930969\n",
      "epoch: 1, batch: 296, loss: 0.7430846095085144\n",
      "epoch: 1, batch: 297, loss: 0.5404220819473267\n",
      "epoch: 1, batch: 298, loss: 0.5927215814590454\n",
      "epoch: 1, batch: 299, loss: 0.451555073261261\n",
      "epoch: 1, batch: 300, loss: 0.7563028931617737\n",
      "epoch: 1, batch: 301, loss: 0.7203486561775208\n",
      "epoch: 1, batch: 302, loss: 0.5078284740447998\n",
      "epoch: 1, batch: 303, loss: 0.6699392795562744\n",
      "epoch: 1, batch: 304, loss: 0.6070999503135681\n",
      "epoch: 1, batch: 305, loss: 0.7358759641647339\n",
      "epoch: 1, batch: 306, loss: 0.5903707146644592\n",
      "epoch: 1, batch: 307, loss: 0.5905700325965881\n",
      "epoch: 1, batch: 308, loss: 0.438579261302948\n",
      "epoch: 1, batch: 309, loss: 0.5198104977607727\n",
      "epoch: 1, batch: 310, loss: 0.6180276870727539\n",
      "epoch: 1, batch: 311, loss: 0.7035613656044006\n",
      "epoch: 1, batch: 312, loss: 0.5397295951843262\n",
      "epoch: 1, batch: 313, loss: 0.6455351710319519\n",
      "epoch: 1, batch: 314, loss: 0.7230941653251648\n",
      "epoch: 1, batch: 315, loss: 0.8583819270133972\n",
      "epoch: 1, batch: 316, loss: 0.6547658443450928\n",
      "epoch: 1, batch: 317, loss: 0.5737533569335938\n",
      "epoch: 1, batch: 318, loss: 0.6150932908058167\n",
      "epoch: 1, batch: 319, loss: 0.5483808517456055\n",
      "epoch: 1, batch: 320, loss: 0.6085114479064941\n",
      "epoch: 1, batch: 321, loss: 0.5702402591705322\n",
      "epoch: 1, batch: 322, loss: 0.7759974598884583\n",
      "epoch: 1, batch: 323, loss: 0.5511789917945862\n",
      "epoch: 1, batch: 324, loss: 0.5308127403259277\n",
      "epoch: 1, batch: 325, loss: 0.7059931755065918\n",
      "epoch: 1, batch: 326, loss: 0.5434892773628235\n",
      "epoch: 1, batch: 327, loss: 0.7443959712982178\n",
      "epoch: 1, batch: 328, loss: 0.6741718053817749\n",
      "epoch: 1, batch: 329, loss: 0.3858232796192169\n",
      "epoch: 1, batch: 330, loss: 0.734135091304779\n",
      "epoch: 1, batch: 331, loss: 0.816149115562439\n",
      "epoch: 1, batch: 332, loss: 0.666526198387146\n",
      "epoch: 1, batch: 333, loss: 0.5489984154701233\n",
      "epoch: 1, batch: 334, loss: 0.6706457138061523\n",
      "epoch: 1, batch: 335, loss: 0.771273672580719\n",
      "epoch: 1, batch: 336, loss: 0.4725082814693451\n",
      "epoch: 1, batch: 337, loss: 0.6714387536048889\n",
      "epoch: 1, batch: 338, loss: 0.48904332518577576\n",
      "epoch: 1, batch: 339, loss: 0.5675195455551147\n",
      "epoch: 1, batch: 340, loss: 0.7864322066307068\n",
      "epoch: 1, batch: 341, loss: 0.6496368646621704\n",
      "epoch: 1, batch: 342, loss: 0.7189154028892517\n",
      "epoch: 1, batch: 343, loss: 0.5902895331382751\n",
      "epoch: 1, batch: 344, loss: 0.8421927690505981\n",
      "epoch: 1, batch: 345, loss: 0.8702002763748169\n",
      "epoch: 1, batch: 346, loss: 0.843390941619873\n",
      "epoch: 1, batch: 347, loss: 0.6107447743415833\n",
      "epoch: 1, batch: 348, loss: 0.7035800814628601\n",
      "epoch: 1, batch: 349, loss: 0.6946781277656555\n",
      "epoch: 1, batch: 350, loss: 0.6561434268951416\n",
      "epoch: 1, batch: 351, loss: 0.5922623872756958\n",
      "epoch: 1, batch: 352, loss: 0.48670172691345215\n",
      "epoch: 1, batch: 353, loss: 0.8025325536727905\n",
      "epoch: 1, batch: 354, loss: 0.6893552541732788\n",
      "epoch: 1, batch: 355, loss: 0.7515282034873962\n",
      "epoch: 1, batch: 356, loss: 0.9991986751556396\n",
      "epoch: 1, batch: 357, loss: 0.5755349397659302\n",
      "epoch: 1, batch: 358, loss: 0.5543159246444702\n",
      "epoch: 1, batch: 359, loss: 0.5981645584106445\n",
      "epoch: 1, batch: 360, loss: 0.6039247512817383\n",
      "epoch: 1, batch: 361, loss: 0.7234150171279907\n",
      "epoch: 1, batch: 362, loss: 0.5578204393386841\n",
      "epoch: 1, batch: 363, loss: 0.5230652093887329\n",
      "epoch: 1, batch: 364, loss: 0.5547409653663635\n",
      "epoch: 1, batch: 365, loss: 0.66181480884552\n",
      "epoch: 1, batch: 366, loss: 0.6961675882339478\n",
      "epoch: 1, batch: 367, loss: 0.7739559412002563\n",
      "epoch: 1, batch: 368, loss: 0.6576882600784302\n",
      "epoch: 1, batch: 369, loss: 0.7232719659805298\n",
      "epoch: 1, batch: 370, loss: 0.8118560910224915\n",
      "epoch: 1, batch: 371, loss: 0.6072363257408142\n",
      "epoch: 1, batch: 372, loss: 0.7955735921859741\n",
      "epoch: 1, batch: 373, loss: 0.5164106488227844\n",
      "epoch: 1, batch: 374, loss: 0.8138574361801147\n",
      "epoch: 1, batch: 375, loss: 0.4521426558494568\n",
      "epoch: 1, batch: 376, loss: 0.67173832654953\n",
      "epoch: 1, batch: 377, loss: 0.6480885148048401\n",
      "epoch: 1, batch: 378, loss: 0.7092622518539429\n",
      "epoch: 1, batch: 379, loss: 0.4564083516597748\n",
      "epoch: 1, batch: 380, loss: 0.5988413095474243\n",
      "epoch: 1, batch: 381, loss: 0.6161394715309143\n",
      "epoch: 1, batch: 382, loss: 0.5027387738227844\n",
      "epoch: 1, batch: 383, loss: 0.6032072901725769\n",
      "epoch: 1, batch: 384, loss: 0.766527533531189\n",
      "epoch: 1, batch: 385, loss: 0.4862809479236603\n",
      "epoch: 1, batch: 386, loss: 0.6971996426582336\n",
      "epoch: 1, batch: 387, loss: 0.3885480463504791\n",
      "epoch: 1, batch: 388, loss: 0.5414468050003052\n",
      "epoch: 1, batch: 389, loss: 0.4710328280925751\n",
      "epoch: 1, batch: 390, loss: 0.39798155426979065\n",
      "epoch: 1, batch: 391, loss: 0.7458741068840027\n",
      "epoch: 1, batch: 392, loss: 0.44378817081451416\n",
      "epoch: 1, batch: 393, loss: 0.5279849171638489\n",
      "epoch: 1, batch: 394, loss: 0.6956981420516968\n",
      "epoch: 1, batch: 395, loss: 0.44243013858795166\n",
      "epoch: 1, batch: 396, loss: 0.6658836603164673\n",
      "epoch: 1, batch: 397, loss: 0.7404825687408447\n",
      "epoch: 1, batch: 398, loss: 0.6519128084182739\n",
      "epoch: 1, batch: 399, loss: 0.4975675940513611\n",
      "epoch: 1, batch: 400, loss: 0.6034727096557617\n",
      "epoch: 1, batch: 401, loss: 0.6010189652442932\n",
      "epoch: 1, batch: 402, loss: 0.6058845520019531\n",
      "epoch: 1, batch: 403, loss: 0.8586719632148743\n",
      "epoch: 1, batch: 404, loss: 0.8655851483345032\n",
      "epoch: 1, batch: 405, loss: 0.5334716439247131\n",
      "epoch: 1, batch: 406, loss: 0.7144455909729004\n",
      "epoch: 1, batch: 407, loss: 0.3746625781059265\n",
      "epoch: 1, batch: 408, loss: 0.519790530204773\n",
      "epoch: 1, batch: 409, loss: 0.49928343296051025\n",
      "epoch: 1, batch: 410, loss: 0.45861580967903137\n",
      "epoch: 1, batch: 411, loss: 0.7124735713005066\n",
      "epoch: 1, batch: 412, loss: 0.5793470144271851\n",
      "epoch: 1, batch: 413, loss: 0.8173219561576843\n",
      "epoch: 1, batch: 414, loss: 0.5274558663368225\n",
      "epoch: 1, batch: 415, loss: 0.3787738084793091\n",
      "epoch: 1, batch: 416, loss: 0.582941472530365\n",
      "epoch: 1, batch: 417, loss: 0.9932613968849182\n",
      "epoch: 1, batch: 418, loss: 0.8541904091835022\n",
      "epoch: 1, batch: 419, loss: 0.35613855719566345\n",
      "epoch: 1, batch: 420, loss: 0.37831974029541016\n",
      "epoch: 1, batch: 421, loss: 0.8082754611968994\n",
      "epoch: 1, batch: 422, loss: 0.6846900582313538\n",
      "epoch: 1, batch: 423, loss: 0.6034297943115234\n",
      "epoch: 1, batch: 424, loss: 0.9183875322341919\n",
      "epoch: 1, batch: 425, loss: 0.46190741658210754\n",
      "epoch: 1, batch: 426, loss: 0.4704740047454834\n",
      "epoch: 1, batch: 427, loss: 0.5865065455436707\n",
      "epoch: 1, batch: 428, loss: 0.7615078687667847\n",
      "epoch: 1, batch: 429, loss: 0.7574695348739624\n",
      "epoch: 1, batch: 430, loss: 0.6009006500244141\n",
      "epoch: 1, batch: 431, loss: 0.407031774520874\n",
      "epoch: 1, batch: 432, loss: 0.8016823530197144\n",
      "epoch: 1, batch: 433, loss: 0.6537810564041138\n",
      "epoch: 1, batch: 434, loss: 0.5961066484451294\n",
      "epoch: 1, batch: 435, loss: 0.6425735950469971\n",
      "epoch: 1, batch: 436, loss: 0.6194638013839722\n",
      "epoch: 1, batch: 437, loss: 0.6847280263900757\n",
      "epoch: 1, batch: 438, loss: 0.8732055425643921\n",
      "epoch: 1, batch: 439, loss: 0.3906833827495575\n",
      "epoch: 1, batch: 440, loss: 0.5594543218612671\n",
      "epoch: 1, batch: 441, loss: 0.8468788862228394\n",
      "epoch: 1, batch: 442, loss: 0.7607358694076538\n",
      "epoch: 1, batch: 443, loss: 0.585200846195221\n",
      "epoch: 1, batch: 444, loss: 0.45544740557670593\n",
      "epoch: 1, batch: 445, loss: 0.7394338250160217\n",
      "epoch: 1, batch: 446, loss: 0.4173315465450287\n",
      "epoch: 1, batch: 447, loss: 0.6449482440948486\n",
      "epoch: 1, batch: 448, loss: 0.768126368522644\n",
      "epoch: 1, batch: 449, loss: 0.5094408988952637\n",
      "epoch: 1, batch: 450, loss: 0.6629541516304016\n",
      "epoch: 1, batch: 451, loss: 0.5378495454788208\n",
      "epoch: 1, batch: 452, loss: 0.6656345725059509\n",
      "epoch: 1, batch: 453, loss: 0.8911321759223938\n",
      "epoch: 1, batch: 454, loss: 0.6923239231109619\n",
      "epoch: 1, batch: 455, loss: 0.4903102219104767\n",
      "epoch: 1, batch: 456, loss: 0.6887809634208679\n",
      "epoch: 1, batch: 457, loss: 0.5888676047325134\n",
      "epoch: 1, batch: 458, loss: 0.8508408665657043\n",
      "epoch: 1, batch: 459, loss: 0.5503019690513611\n",
      "epoch: 1, batch: 460, loss: 0.6357565522193909\n",
      "epoch: 1, batch: 461, loss: 0.6705224514007568\n",
      "epoch: 1, batch: 462, loss: 0.5754148960113525\n",
      "epoch: 1, batch: 463, loss: 0.8890687227249146\n",
      "epoch: 1, batch: 464, loss: 0.46032261848449707\n",
      "epoch: 1, batch: 465, loss: 0.6960873603820801\n",
      "epoch: 1, batch: 466, loss: 0.962242841720581\n",
      "epoch: 1, batch: 467, loss: 0.7125586867332458\n",
      "epoch: 1, batch: 468, loss: 0.5362246632575989\n",
      "epoch: 1, batch: 469, loss: 0.5487964153289795\n",
      "epoch: 1, batch: 470, loss: 0.6238645911216736\n",
      "epoch: 1, batch: 471, loss: 0.7475582957267761\n",
      "epoch: 1, batch: 472, loss: 0.6650385856628418\n",
      "epoch: 1, batch: 473, loss: 0.5069403648376465\n",
      "epoch: 1, batch: 474, loss: 0.48766952753067017\n",
      "epoch: 1, batch: 475, loss: 0.5254651308059692\n",
      "epoch: 1, batch: 476, loss: 0.5602627992630005\n",
      "epoch: 1, batch: 477, loss: 0.6843695044517517\n",
      "epoch: 1, batch: 478, loss: 0.7488764524459839\n",
      "epoch: 1, batch: 479, loss: 0.6274473071098328\n",
      "epoch: 1, batch: 480, loss: 0.766496479511261\n",
      "epoch: 1, batch: 481, loss: 0.919599175453186\n",
      "epoch: 1, batch: 482, loss: 0.6476253867149353\n",
      "epoch: 1, batch: 483, loss: 0.7286654114723206\n",
      "epoch: 1, batch: 484, loss: 0.5253937244415283\n",
      "epoch: 1, batch: 485, loss: 0.6809697151184082\n",
      "epoch: 1, batch: 486, loss: 0.5426749587059021\n",
      "epoch: 1, batch: 487, loss: 0.5084049105644226\n",
      "epoch: 1, batch: 488, loss: 0.6293519735336304\n",
      "epoch: 1, batch: 489, loss: 0.5560603141784668\n",
      "epoch: 1, batch: 490, loss: 0.6034458875656128\n",
      "epoch: 1, batch: 491, loss: 0.5410022735595703\n",
      "epoch: 1, batch: 492, loss: 0.4575529992580414\n",
      "epoch: 1, batch: 493, loss: 0.7154361605644226\n",
      "epoch: 1, batch: 494, loss: 0.5847707390785217\n",
      "epoch: 1, batch: 495, loss: 0.6687496900558472\n",
      "epoch: 1, batch: 496, loss: 0.7934845089912415\n",
      "epoch: 1, batch: 497, loss: 0.7837642431259155\n",
      "epoch: 1, batch: 498, loss: 0.47959452867507935\n",
      "epoch: 1, batch: 499, loss: 0.47284114360809326\n",
      "epoch: 1, batch: 500, loss: 0.6021855473518372\n",
      "epoch: 1, batch: 501, loss: 0.6053416728973389\n",
      "epoch: 1, batch: 502, loss: 0.6676449775695801\n",
      "epoch: 1, batch: 503, loss: 0.6905479431152344\n",
      "epoch: 1, batch: 504, loss: 0.4719511866569519\n",
      "epoch: 1, batch: 505, loss: 0.5914681553840637\n",
      "epoch: 1, batch: 506, loss: 0.7256853580474854\n",
      "epoch: 1, batch: 507, loss: 0.6220942139625549\n",
      "epoch: 1, batch: 508, loss: 0.6603829860687256\n",
      "epoch: 1, batch: 509, loss: 0.5423096418380737\n",
      "epoch: 1, batch: 510, loss: 0.9669046401977539\n",
      "epoch: 1, batch: 511, loss: 0.6199578046798706\n",
      "epoch: 1, batch: 512, loss: 0.565540611743927\n",
      "epoch: 1, batch: 513, loss: 0.5938907265663147\n",
      "epoch: 1, batch: 514, loss: 0.725703239440918\n",
      "epoch: 1, batch: 515, loss: 0.5840067267417908\n",
      "epoch: 1, batch: 516, loss: 0.7970854640007019\n",
      "epoch: 1, batch: 517, loss: 0.6461570262908936\n",
      "epoch: 1, batch: 518, loss: 0.5594933032989502\n",
      "epoch: 1, batch: 519, loss: 0.5493748188018799\n",
      "epoch: 1, batch: 520, loss: 0.6726066470146179\n",
      "epoch: 1, batch: 521, loss: 0.5161996483802795\n",
      "epoch: 1, batch: 522, loss: 0.4610612988471985\n",
      "epoch: 1, batch: 523, loss: 0.6935423016548157\n",
      "epoch: 1, batch: 524, loss: 0.47535017132759094\n",
      "epoch: 1, batch: 525, loss: 0.4935644268989563\n",
      "epoch: 1, batch: 526, loss: 0.7087676525115967\n",
      "epoch: 1, batch: 527, loss: 0.4611738324165344\n",
      "epoch: 1, batch: 528, loss: 0.5289269685745239\n",
      "epoch: 1, batch: 529, loss: 0.7210859060287476\n",
      "epoch: 1, batch: 530, loss: 0.5767290592193604\n",
      "epoch: 1, batch: 531, loss: 0.44253942370414734\n",
      "epoch: 1, batch: 532, loss: 0.6395377516746521\n",
      "epoch: 1, batch: 533, loss: 0.4800950288772583\n",
      "epoch: 1, batch: 534, loss: 0.5774875283241272\n",
      "epoch: 1, batch: 535, loss: 0.680134654045105\n",
      "epoch: 1, batch: 536, loss: 0.5010989904403687\n",
      "epoch: 1, batch: 537, loss: 0.6255098581314087\n",
      "epoch: 1, batch: 538, loss: 0.5959864854812622\n",
      "epoch: 1, batch: 539, loss: 0.5453316569328308\n",
      "epoch: 1, batch: 540, loss: 0.6508281230926514\n",
      "epoch: 1, batch: 541, loss: 0.6537325978279114\n",
      "epoch: 1, batch: 542, loss: 0.6654077768325806\n",
      "epoch: 1, batch: 543, loss: 0.6637736558914185\n",
      "epoch: 1, batch: 544, loss: 0.7647743225097656\n",
      "epoch: 1, batch: 545, loss: 0.5253772139549255\n",
      "epoch: 1, batch: 546, loss: 0.7395180463790894\n",
      "epoch: 1, batch: 547, loss: 0.8836695551872253\n",
      "epoch: 1, batch: 548, loss: 0.6629033088684082\n",
      "epoch: 1, batch: 549, loss: 0.8711733222007751\n",
      "epoch: 1, batch: 550, loss: 0.4804743230342865\n",
      "epoch: 1, batch: 551, loss: 0.4811873435974121\n",
      "epoch: 1, batch: 552, loss: 0.744450032711029\n",
      "epoch: 1, batch: 553, loss: 0.6541703343391418\n",
      "epoch: 1, batch: 554, loss: 0.6054034233093262\n",
      "epoch: 1, batch: 555, loss: 0.5172910094261169\n",
      "epoch: 1, batch: 556, loss: 0.5294570326805115\n",
      "epoch: 1, batch: 557, loss: 0.5748963952064514\n",
      "epoch: 1, batch: 558, loss: 0.7785479426383972\n",
      "epoch: 1, batch: 559, loss: 0.7978442907333374\n",
      "epoch: 1, batch: 560, loss: 0.48157551884651184\n",
      "epoch: 1, batch: 561, loss: 0.6125677824020386\n",
      "epoch: 1, batch: 562, loss: 0.7052233219146729\n",
      "epoch: 1, batch: 563, loss: 0.7747505903244019\n",
      "epoch: 1, batch: 564, loss: 0.4425964653491974\n",
      "epoch: 1, batch: 565, loss: 0.6073423027992249\n",
      "epoch: 1, batch: 566, loss: 0.5507229566574097\n",
      "epoch: 1, batch: 567, loss: 0.6767582893371582\n",
      "epoch: 1, batch: 568, loss: 0.8562244176864624\n",
      "epoch: 1, batch: 569, loss: 0.47520628571510315\n",
      "epoch: 1, batch: 570, loss: 0.8035248517990112\n",
      "epoch: 1, batch: 571, loss: 0.847546398639679\n",
      "epoch: 1, batch: 572, loss: 0.578225314617157\n",
      "epoch: 1, batch: 573, loss: 0.5324241518974304\n",
      "epoch: 1, batch: 574, loss: 0.6217414140701294\n",
      "epoch: 1, batch: 575, loss: 0.8795046806335449\n",
      "epoch: 1, batch: 576, loss: 0.8943816423416138\n",
      "epoch: 1, batch: 577, loss: 0.730430006980896\n",
      "epoch: 1, batch: 578, loss: 0.7674090266227722\n",
      "epoch: 1, batch: 579, loss: 0.5442994236946106\n",
      "epoch: 1, batch: 580, loss: 0.6606681942939758\n",
      "epoch: 1, batch: 581, loss: 0.6554840207099915\n",
      "epoch: 1, batch: 582, loss: 0.6694464087486267\n",
      "epoch: 1, batch: 583, loss: 0.4766140878200531\n",
      "epoch: 1, batch: 584, loss: 0.5544868111610413\n",
      "epoch: 1, batch: 585, loss: 0.6272027492523193\n",
      "epoch: 1, batch: 586, loss: 0.7576276659965515\n",
      "epoch: 1, batch: 587, loss: 0.6475967168807983\n",
      "epoch: 1, batch: 588, loss: 0.5618236660957336\n",
      "epoch: 1, batch: 589, loss: 0.8082296848297119\n",
      "epoch: 1, batch: 590, loss: 0.5839623212814331\n",
      "epoch: 1, batch: 591, loss: 0.6153600811958313\n",
      "epoch: 1, batch: 592, loss: 0.48316994309425354\n",
      "epoch: 1, batch: 593, loss: 0.46414312720298767\n",
      "epoch: 1, batch: 594, loss: 0.5396935343742371\n",
      "epoch: 1, batch: 595, loss: 0.40298038721084595\n",
      "epoch: 1, batch: 596, loss: 0.6037911176681519\n",
      "epoch: 1, batch: 597, loss: 0.618099570274353\n",
      "epoch: 1, batch: 598, loss: 0.6917627453804016\n",
      "epoch: 1, batch: 599, loss: 0.7342748045921326\n",
      "epoch: 1, batch: 600, loss: 0.8428797125816345\n",
      "epoch: 1, batch: 601, loss: 0.518966794013977\n",
      "epoch: 1, batch: 602, loss: 0.5594276785850525\n",
      "epoch: 1, batch: 603, loss: 0.6486323475837708\n",
      "epoch: 1, batch: 604, loss: 0.444195032119751\n",
      "epoch: 1, batch: 605, loss: 0.5948761701583862\n",
      "epoch: 1, batch: 606, loss: 0.8475562930107117\n",
      "epoch: 1, batch: 607, loss: 0.8988906741142273\n",
      "epoch: 1, batch: 608, loss: 0.5503732562065125\n",
      "epoch: 1, batch: 609, loss: 0.5235464572906494\n",
      "epoch: 1, batch: 610, loss: 0.5785836577415466\n",
      "epoch: 1, batch: 611, loss: 0.7810850143432617\n",
      "epoch: 1, batch: 612, loss: 0.39982008934020996\n",
      "epoch: 1, batch: 613, loss: 0.5914754271507263\n",
      "epoch: 1, batch: 614, loss: 0.6045389771461487\n",
      "epoch: 1, batch: 615, loss: 0.727379560470581\n",
      "epoch: 1, batch: 616, loss: 0.5741502046585083\n",
      "epoch: 1, batch: 617, loss: 0.6283466219902039\n",
      "epoch: 1, batch: 618, loss: 0.7231161594390869\n",
      "epoch: 1, batch: 619, loss: 0.673995852470398\n",
      "epoch: 1, batch: 620, loss: 0.6636203527450562\n",
      "epoch: 1, batch: 621, loss: 0.6591657400131226\n",
      "epoch: 1, batch: 622, loss: 0.5894296765327454\n",
      "epoch: 1, batch: 623, loss: 0.6194731593132019\n",
      "epoch: 1, batch: 624, loss: 0.8260205984115601\n",
      "epoch: 1, batch: 625, loss: 0.5937530398368835\n",
      "epoch: 1, batch: 626, loss: 0.6495260000228882\n",
      "epoch: 1, batch: 627, loss: 0.6803133487701416\n",
      "epoch: 1, batch: 628, loss: 0.7109776735305786\n",
      "epoch: 1, batch: 629, loss: 0.7129737734794617\n",
      "epoch: 1, batch: 630, loss: 0.5671635270118713\n",
      "epoch: 1, batch: 631, loss: 0.6146045327186584\n",
      "epoch: 1, batch: 632, loss: 0.524334192276001\n",
      "epoch: 1, batch: 633, loss: 0.618922233581543\n",
      "epoch: 1, batch: 634, loss: 0.5522598624229431\n",
      "epoch: 1, batch: 635, loss: 0.470454603433609\n",
      "epoch: 1, batch: 636, loss: 0.5580781698226929\n",
      "epoch: 1, batch: 637, loss: 0.6092175841331482\n",
      "epoch: 1, batch: 638, loss: 0.49494484066963196\n",
      "epoch: 1, batch: 639, loss: 0.5120063424110413\n",
      "epoch: 1, batch: 640, loss: 0.49519088864326477\n",
      "epoch: 1, batch: 641, loss: 0.6911396384239197\n",
      "epoch: 1, batch: 642, loss: 0.550432562828064\n",
      "epoch: 1, batch: 643, loss: 0.6157166957855225\n",
      "epoch: 1, batch: 644, loss: 0.5532312989234924\n",
      "epoch: 1, batch: 645, loss: 0.460619181394577\n",
      "epoch: 1, batch: 646, loss: 0.7773009538650513\n",
      "epoch: 1, batch: 647, loss: 0.6077057123184204\n",
      "epoch: 1, batch: 648, loss: 0.5463573932647705\n",
      "epoch: 1, batch: 649, loss: 0.64139723777771\n",
      "epoch: 1, batch: 650, loss: 0.48948344588279724\n",
      "epoch: 1, batch: 651, loss: 0.5383597612380981\n",
      "epoch: 1, batch: 652, loss: 0.4335358440876007\n",
      "epoch: 1, batch: 653, loss: 0.9105713367462158\n",
      "epoch: 1, batch: 654, loss: 0.381619393825531\n",
      "epoch: 1, batch: 655, loss: 0.532444953918457\n",
      "epoch: 1, batch: 656, loss: 0.640385627746582\n",
      "epoch: 1, batch: 657, loss: 0.5881662368774414\n",
      "epoch: 1, batch: 658, loss: 0.5940483808517456\n",
      "epoch: 1, batch: 659, loss: 0.48377010226249695\n",
      "epoch: 1, batch: 660, loss: 0.7034528255462646\n",
      "epoch: 1, batch: 661, loss: 0.5278280973434448\n",
      "epoch: 1, batch: 662, loss: 0.4383922815322876\n",
      "epoch: 1, batch: 663, loss: 0.5710384249687195\n",
      "epoch: 1, batch: 664, loss: 0.5881375074386597\n",
      "epoch: 1, batch: 665, loss: 0.7659199833869934\n",
      "epoch: 1, batch: 666, loss: 0.39382702112197876\n",
      "epoch: 1, batch: 667, loss: 0.751396119594574\n",
      "epoch: 1, batch: 668, loss: 0.4839877784252167\n",
      "epoch: 1, batch: 669, loss: 0.5875065326690674\n",
      "epoch: 1, batch: 670, loss: 0.6937133073806763\n",
      "epoch: 1, batch: 671, loss: 0.551133930683136\n",
      "epoch: 1, batch: 672, loss: 0.5573468208312988\n",
      "epoch: 1, batch: 673, loss: 0.4994317591190338\n",
      "epoch: 1, batch: 674, loss: 0.36870595812797546\n",
      "epoch: 1, batch: 675, loss: 1.0095086097717285\n",
      "epoch: 1, batch: 676, loss: 0.830490231513977\n",
      "epoch: 1, batch: 677, loss: 0.5046811103820801\n",
      "epoch: 1, batch: 678, loss: 0.54146808385849\n",
      "epoch: 1, batch: 679, loss: 0.43047600984573364\n",
      "epoch: 1, batch: 680, loss: 0.5301915407180786\n",
      "epoch: 1, batch: 681, loss: 0.5571938157081604\n",
      "epoch: 1, batch: 682, loss: 0.4866654574871063\n",
      "epoch: 1, batch: 683, loss: 0.5932859182357788\n",
      "epoch: 1, batch: 684, loss: 0.6387401223182678\n",
      "epoch: 1, batch: 685, loss: 0.5553175210952759\n",
      "epoch: 1, batch: 686, loss: 0.5075879096984863\n",
      "epoch: 1, batch: 687, loss: 0.42827489972114563\n",
      "epoch: 1, batch: 688, loss: 0.7972337603569031\n",
      "epoch: 1, batch: 689, loss: 0.6656821370124817\n",
      "epoch: 1, batch: 690, loss: 0.5099889039993286\n",
      "epoch: 1, batch: 691, loss: 0.5474919676780701\n",
      "epoch: 1, batch: 692, loss: 0.5186327695846558\n",
      "epoch: 1, batch: 693, loss: 0.4534628689289093\n",
      "epoch: 1, batch: 694, loss: 0.6097116470336914\n",
      "epoch: 1, batch: 695, loss: 0.4226166009902954\n",
      "epoch: 1, batch: 696, loss: 0.49086758494377136\n",
      "epoch: 1, batch: 697, loss: 0.383687287569046\n",
      "epoch: 1, batch: 698, loss: 0.5761478543281555\n",
      "epoch: 1, batch: 699, loss: 0.4686416685581207\n",
      "epoch: 1, batch: 700, loss: 0.5866557955741882\n",
      "epoch: 1, batch: 701, loss: 0.5795212388038635\n",
      "epoch: 1, batch: 702, loss: 0.4697411358356476\n",
      "epoch: 1, batch: 703, loss: 0.6222482323646545\n",
      "epoch: 1, batch: 704, loss: 0.6970778107643127\n",
      "epoch: 1, batch: 705, loss: 0.41096222400665283\n",
      "epoch: 1, batch: 706, loss: 0.5652940273284912\n",
      "epoch: 1, batch: 707, loss: 0.4266222417354584\n",
      "epoch: 1, batch: 708, loss: 0.8562315106391907\n",
      "epoch: 1, batch: 709, loss: 0.48321816325187683\n",
      "epoch: 1, batch: 710, loss: 0.4859979450702667\n",
      "epoch: 1, batch: 711, loss: 0.6096771955490112\n",
      "epoch: 1, batch: 712, loss: 0.41066721081733704\n",
      "epoch: 1, batch: 713, loss: 0.5183225870132446\n",
      "epoch: 1, batch: 714, loss: 0.5850321650505066\n",
      "epoch: 1, batch: 715, loss: 0.4828340709209442\n",
      "epoch: 1, batch: 716, loss: 0.5766037702560425\n",
      "epoch: 1, batch: 717, loss: 0.6300519704818726\n",
      "epoch: 1, batch: 718, loss: 0.6823976635932922\n",
      "epoch: 1, batch: 719, loss: 0.6565002799034119\n",
      "epoch: 1, batch: 720, loss: 0.6739010810852051\n",
      "epoch: 1, batch: 721, loss: 0.6505765914916992\n",
      "epoch: 1, batch: 722, loss: 0.3669411540031433\n",
      "epoch: 1, batch: 723, loss: 0.47392165660858154\n",
      "epoch: 1, batch: 724, loss: 0.5914419293403625\n",
      "epoch: 1, batch: 725, loss: 0.5422071814537048\n",
      "epoch: 1, batch: 726, loss: 0.571849524974823\n",
      "epoch: 1, batch: 727, loss: 0.6208145022392273\n",
      "epoch: 1, batch: 728, loss: 0.6355797648429871\n",
      "epoch: 1, batch: 729, loss: 0.44363871216773987\n",
      "epoch: 1, batch: 730, loss: 0.63266921043396\n",
      "epoch: 1, batch: 731, loss: 0.47202953696250916\n",
      "epoch: 1, batch: 732, loss: 0.4611038863658905\n",
      "epoch: 1, batch: 733, loss: 0.6953232884407043\n",
      "epoch: 1, batch: 734, loss: 0.503937840461731\n",
      "epoch: 1, batch: 735, loss: 0.45322999358177185\n",
      "epoch: 1, batch: 736, loss: 0.9131191372871399\n",
      "epoch: 1, batch: 737, loss: 0.5914970636367798\n",
      "epoch: 1, batch: 738, loss: 0.4268523156642914\n",
      "epoch: 1, batch: 739, loss: 0.6513919830322266\n",
      "epoch: 1, batch: 740, loss: 0.7535334825515747\n",
      "epoch: 1, batch: 741, loss: 0.6035484671592712\n",
      "epoch: 1, batch: 742, loss: 0.579490602016449\n",
      "epoch: 1, batch: 743, loss: 0.5577016472816467\n",
      "epoch: 1, batch: 744, loss: 0.5730753540992737\n",
      "epoch: 1, batch: 745, loss: 0.4773344099521637\n",
      "epoch: 1, batch: 746, loss: 0.6364444494247437\n",
      "epoch: 1, batch: 747, loss: 0.49071213603019714\n",
      "epoch: 1, batch: 748, loss: 0.48163625597953796\n",
      "epoch: 1, batch: 749, loss: 0.7067732214927673\n",
      "epoch: 1, batch: 750, loss: 0.6466187834739685\n",
      "epoch: 1, batch: 751, loss: 0.5331388711929321\n",
      "epoch: 1, batch: 752, loss: 0.42625877261161804\n",
      "epoch: 1, batch: 753, loss: 0.5727584958076477\n",
      "epoch: 1, batch: 754, loss: 0.5032819509506226\n",
      "epoch: 1, batch: 755, loss: 0.5522004961967468\n",
      "epoch: 1, batch: 756, loss: 0.5011606216430664\n",
      "epoch: 1, batch: 757, loss: 0.524777889251709\n",
      "epoch: 1, batch: 758, loss: 0.47341686487197876\n",
      "epoch: 1, batch: 759, loss: 0.40989959239959717\n",
      "epoch: 1, batch: 760, loss: 0.5857282876968384\n",
      "epoch: 1, batch: 761, loss: 0.6678741574287415\n",
      "epoch: 1, batch: 762, loss: 0.42527469992637634\n",
      "epoch: 1, batch: 763, loss: 0.44530239701271057\n",
      "epoch: 1, batch: 764, loss: 0.6486024260520935\n",
      "epoch: 1, batch: 765, loss: 0.8023384213447571\n",
      "epoch: 1, batch: 766, loss: 0.6498419046401978\n",
      "epoch: 1, batch: 767, loss: 0.6408321261405945\n",
      "epoch: 1, batch: 768, loss: 0.41426461935043335\n",
      "epoch: 1, batch: 769, loss: 0.5991939902305603\n",
      "epoch: 1, batch: 770, loss: 0.46065762639045715\n",
      "epoch: 1, batch: 771, loss: 0.4961353838443756\n",
      "epoch: 1, batch: 772, loss: 0.5297338366508484\n",
      "epoch: 1, batch: 773, loss: 0.4351649284362793\n",
      "epoch: 1, batch: 774, loss: 0.5120826959609985\n",
      "epoch: 1, batch: 775, loss: 0.4638440012931824\n",
      "epoch: 1, batch: 776, loss: 0.650754988193512\n",
      "epoch: 1, batch: 777, loss: 0.40872085094451904\n",
      "epoch: 1, batch: 778, loss: 0.5358374118804932\n",
      "epoch: 1, batch: 779, loss: 0.5643198490142822\n",
      "epoch: 1, batch: 780, loss: 0.5922790765762329\n",
      "epoch: 1, batch: 781, loss: 0.46910473704338074\n",
      "epoch: 1, batch: 782, loss: 0.5318436622619629\n",
      "epoch: 1, batch: 783, loss: 0.8320422172546387\n",
      "epoch: 1, batch: 784, loss: 0.5516080260276794\n",
      "epoch: 1, batch: 785, loss: 0.4885450303554535\n",
      "epoch: 1, batch: 786, loss: 0.38141366839408875\n",
      "epoch: 1, batch: 787, loss: 0.49472305178642273\n",
      "epoch: 1, batch: 788, loss: 0.30109551548957825\n",
      "epoch: 1, batch: 789, loss: 0.5188220739364624\n",
      "epoch: 1, batch: 790, loss: 0.5426562428474426\n",
      "epoch: 1, batch: 791, loss: 0.38915586471557617\n",
      "epoch: 1, batch: 792, loss: 0.7325181365013123\n",
      "epoch: 1, batch: 793, loss: 0.5674066543579102\n",
      "epoch: 1, batch: 794, loss: 0.44920268654823303\n",
      "epoch: 1, batch: 795, loss: 0.4004087448120117\n",
      "epoch: 1, batch: 796, loss: 0.5571647882461548\n",
      "epoch: 1, batch: 797, loss: 0.6459567546844482\n",
      "epoch: 1, batch: 798, loss: 0.7923907041549683\n",
      "epoch: 1, batch: 799, loss: 0.4009741544723511\n",
      "epoch: 1, batch: 800, loss: 0.6488306522369385\n",
      "epoch: 1, batch: 801, loss: 0.42855149507522583\n",
      "epoch: 1, batch: 802, loss: 0.5184006690979004\n",
      "epoch: 1, batch: 803, loss: 0.5822359323501587\n",
      "epoch: 1, batch: 804, loss: 0.5884906053543091\n",
      "epoch: 1, batch: 805, loss: 0.7601897120475769\n",
      "epoch: 1, batch: 806, loss: 0.48432642221450806\n",
      "epoch: 1, batch: 807, loss: 0.718180775642395\n",
      "epoch: 1, batch: 808, loss: 0.42210644483566284\n",
      "epoch: 1, batch: 809, loss: 0.39599621295928955\n",
      "epoch: 1, batch: 810, loss: 0.5908777117729187\n",
      "epoch: 1, batch: 811, loss: 0.43580153584480286\n",
      "epoch: 1, batch: 812, loss: 0.42505308985710144\n",
      "epoch: 1, batch: 813, loss: 0.47560620307922363\n",
      "epoch: 1, batch: 814, loss: 0.4321116805076599\n",
      "epoch: 1, batch: 815, loss: 0.4185203015804291\n",
      "epoch: 1, batch: 816, loss: 0.40184715390205383\n",
      "epoch: 1, batch: 817, loss: 0.7322763204574585\n",
      "epoch: 1, batch: 818, loss: 0.5311749577522278\n",
      "epoch: 1, batch: 819, loss: 0.4441271126270294\n",
      "epoch: 1, batch: 820, loss: 0.6385793089866638\n",
      "epoch: 1, batch: 821, loss: 0.44411855936050415\n",
      "epoch: 1, batch: 822, loss: 0.42134779691696167\n",
      "epoch: 1, batch: 823, loss: 0.39584633708000183\n",
      "epoch: 1, batch: 824, loss: 0.6985824108123779\n",
      "epoch: 1, batch: 825, loss: 0.4573797285556793\n",
      "epoch: 1, batch: 826, loss: 0.4789274036884308\n",
      "epoch: 1, batch: 827, loss: 0.3509739339351654\n",
      "epoch: 1, batch: 828, loss: 0.5489485859870911\n",
      "epoch: 1, batch: 829, loss: 0.4820520579814911\n",
      "epoch: 1, batch: 830, loss: 0.482278048992157\n",
      "epoch: 1, batch: 831, loss: 0.42708227038383484\n",
      "epoch: 1, batch: 832, loss: 0.40273430943489075\n",
      "epoch: 1, batch: 833, loss: 0.554307222366333\n",
      "epoch: 1, batch: 834, loss: 0.5274417996406555\n",
      "epoch: 1, batch: 835, loss: 0.7246612310409546\n",
      "epoch: 1, batch: 836, loss: 0.4942737817764282\n",
      "epoch: 1, batch: 837, loss: 0.47825074195861816\n",
      "epoch: 1, batch: 838, loss: 0.4993525743484497\n",
      "epoch: 1, batch: 839, loss: 0.5694155097007751\n",
      "epoch: 1, batch: 840, loss: 0.5093290209770203\n",
      "epoch: 1, batch: 841, loss: 0.4970230460166931\n",
      "epoch: 1, batch: 842, loss: 0.7542882561683655\n",
      "epoch: 1, batch: 843, loss: 0.589992105960846\n",
      "epoch: 1, batch: 844, loss: 0.5698909163475037\n",
      "epoch: 1, batch: 845, loss: 0.3661339581012726\n",
      "epoch: 1, batch: 846, loss: 0.572224497795105\n",
      "epoch: 1, batch: 847, loss: 0.4587480127811432\n",
      "epoch: 1, batch: 848, loss: 0.5086176991462708\n",
      "epoch: 1, batch: 849, loss: 0.6746925115585327\n",
      "epoch: 1, batch: 850, loss: 0.3816315829753876\n",
      "epoch: 1, batch: 851, loss: 0.5769522190093994\n",
      "epoch: 1, batch: 852, loss: 0.768185555934906\n",
      "epoch: 1, batch: 853, loss: 0.49122339487075806\n",
      "epoch: 1, batch: 854, loss: 0.5598368048667908\n",
      "epoch: 1, batch: 855, loss: 0.5596798062324524\n",
      "epoch: 1, batch: 856, loss: 0.6568105816841125\n",
      "epoch: 1, batch: 857, loss: 0.5570943355560303\n",
      "epoch: 1, batch: 858, loss: 0.6047443747520447\n",
      "epoch: 1, batch: 859, loss: 0.4381420910358429\n",
      "epoch: 1, batch: 860, loss: 0.635735034942627\n",
      "epoch: 1, batch: 861, loss: 0.579932451248169\n",
      "epoch: 1, batch: 862, loss: 0.4445425868034363\n",
      "epoch: 1, batch: 863, loss: 0.6754314303398132\n",
      "epoch: 1, batch: 864, loss: 0.34441232681274414\n",
      "epoch: 1, batch: 865, loss: 0.6845290064811707\n",
      "epoch: 1, batch: 866, loss: 0.5719128847122192\n",
      "epoch: 1, batch: 867, loss: 0.5654149055480957\n",
      "epoch: 1, batch: 868, loss: 0.3628799319267273\n",
      "epoch: 1, batch: 869, loss: 0.6013031005859375\n",
      "epoch: 1, batch: 870, loss: 0.6284276247024536\n",
      "epoch: 1, batch: 871, loss: 0.5440476536750793\n",
      "epoch: 1, batch: 872, loss: 0.45367947220802307\n",
      "epoch: 1, batch: 873, loss: 0.6448003053665161\n",
      "epoch: 1, batch: 874, loss: 0.6843700408935547\n",
      "epoch: 1, batch: 875, loss: 0.4066661596298218\n",
      "epoch: 1, batch: 876, loss: 0.6237935423851013\n",
      "epoch: 1, batch: 877, loss: 0.46474698185920715\n",
      "epoch: 1, batch: 878, loss: 0.4889700710773468\n",
      "epoch: 1, batch: 879, loss: 0.5239838361740112\n",
      "epoch: 1, batch: 880, loss: 0.5116890072822571\n",
      "epoch: 1, batch: 881, loss: 0.7350854873657227\n",
      "epoch: 1, batch: 882, loss: 0.5276544690132141\n",
      "epoch: 1, batch: 883, loss: 0.5756924152374268\n",
      "epoch: 1, batch: 884, loss: 0.545168399810791\n",
      "epoch: 1, batch: 885, loss: 0.6018937230110168\n",
      "epoch: 1, batch: 886, loss: 0.5428359508514404\n",
      "epoch: 1, batch: 887, loss: 0.4534863233566284\n",
      "epoch: 1, batch: 888, loss: 0.6289703845977783\n",
      "epoch: 1, batch: 889, loss: 0.6242455840110779\n",
      "epoch: 1, batch: 890, loss: 0.4784211814403534\n",
      "epoch: 1, batch: 891, loss: 0.4475010931491852\n",
      "epoch: 1, batch: 892, loss: 0.5414301156997681\n",
      "epoch: 1, batch: 893, loss: 0.5207993388175964\n",
      "epoch: 1, batch: 894, loss: 0.8168210983276367\n",
      "epoch: 1, batch: 895, loss: 0.8971214890480042\n",
      "epoch: 1, batch: 896, loss: 0.567200779914856\n",
      "epoch: 1, batch: 897, loss: 0.5650740265846252\n",
      "epoch: 1, batch: 898, loss: 0.6571469902992249\n",
      "epoch: 1, batch: 899, loss: 0.4753235876560211\n",
      "epoch: 1, batch: 900, loss: 0.4068971872329712\n",
      "epoch: 1, batch: 901, loss: 0.5184839367866516\n",
      "epoch: 1, batch: 902, loss: 0.38425788283348083\n",
      "epoch: 1, batch: 903, loss: 0.5379995703697205\n",
      "epoch: 1, batch: 904, loss: 0.855718195438385\n",
      "epoch: 1, batch: 905, loss: 0.4522358477115631\n",
      "epoch: 1, batch: 906, loss: 0.5050163269042969\n",
      "epoch: 1, batch: 907, loss: 0.59586101770401\n",
      "epoch: 1, batch: 908, loss: 0.5805968046188354\n",
      "epoch: 1, batch: 909, loss: 0.4625833332538605\n",
      "epoch: 1, batch: 910, loss: 0.48207128047943115\n",
      "epoch: 1, batch: 911, loss: 0.4973198473453522\n",
      "epoch: 1, batch: 912, loss: 0.652428925037384\n",
      "epoch: 1, batch: 913, loss: 0.7622002959251404\n",
      "epoch: 1, batch: 914, loss: 0.6947245597839355\n",
      "epoch: 1, batch: 915, loss: 0.5207995176315308\n",
      "epoch: 1, batch: 916, loss: 0.6479143500328064\n",
      "epoch: 1, batch: 917, loss: 0.5151867866516113\n",
      "epoch: 1, batch: 918, loss: 0.3971266746520996\n",
      "epoch: 1, batch: 919, loss: 0.7020965218544006\n",
      "epoch: 1, batch: 920, loss: 0.485537052154541\n",
      "epoch: 1, batch: 921, loss: 0.40998712182044983\n",
      "epoch: 1, batch: 922, loss: 0.6236499547958374\n",
      "epoch: 1, batch: 923, loss: 0.39304569363594055\n",
      "epoch: 1, batch: 924, loss: 0.6048045754432678\n",
      "epoch: 1, batch: 925, loss: 0.5496926307678223\n",
      "epoch: 1, batch: 926, loss: 0.5921427011489868\n",
      "epoch: 1, batch: 927, loss: 0.6171761155128479\n",
      "epoch: 1, batch: 928, loss: 0.4691629707813263\n",
      "epoch: 1, batch: 929, loss: 0.46782225370407104\n",
      "epoch: 1, batch: 930, loss: 0.4453894793987274\n",
      "epoch: 1, batch: 931, loss: 0.45439988374710083\n",
      "epoch: 1, batch: 932, loss: 0.6203761100769043\n",
      "epoch: 1, batch: 933, loss: 0.6404997110366821\n",
      "epoch: 1, batch: 934, loss: 0.333053857088089\n",
      "epoch: 1, batch: 935, loss: 0.6567904353141785\n",
      "epoch: 1, batch: 936, loss: 0.5219677090644836\n",
      "epoch: 1, batch: 937, loss: 0.7308116555213928\n",
      "epoch: 2, batch: 0, loss: 0.4974009394645691\n",
      "epoch: 2, batch: 1, loss: 0.5583238005638123\n",
      "epoch: 2, batch: 2, loss: 0.5117067694664001\n",
      "epoch: 2, batch: 3, loss: 0.5717094540596008\n",
      "epoch: 2, batch: 4, loss: 0.5701770186424255\n",
      "epoch: 2, batch: 5, loss: 0.4248035252094269\n",
      "epoch: 2, batch: 6, loss: 0.5839869379997253\n",
      "epoch: 2, batch: 7, loss: 0.44369152188301086\n",
      "epoch: 2, batch: 8, loss: 0.523549497127533\n",
      "epoch: 2, batch: 9, loss: 0.6019300818443298\n",
      "epoch: 2, batch: 10, loss: 0.48236268758773804\n",
      "epoch: 2, batch: 11, loss: 0.4583985507488251\n",
      "epoch: 2, batch: 12, loss: 0.31062185764312744\n",
      "epoch: 2, batch: 13, loss: 0.5123428106307983\n",
      "epoch: 2, batch: 14, loss: 0.3415358066558838\n",
      "epoch: 2, batch: 15, loss: 0.40879252552986145\n",
      "epoch: 2, batch: 16, loss: 0.37414637207984924\n",
      "epoch: 2, batch: 17, loss: 0.3383852243423462\n",
      "epoch: 2, batch: 18, loss: 0.4378396272659302\n",
      "epoch: 2, batch: 19, loss: 0.5491095185279846\n",
      "epoch: 2, batch: 20, loss: 0.4950195848941803\n",
      "epoch: 2, batch: 21, loss: 0.4615347683429718\n",
      "epoch: 2, batch: 22, loss: 0.4754832983016968\n",
      "epoch: 2, batch: 23, loss: 0.49199435114860535\n",
      "epoch: 2, batch: 24, loss: 0.6022433638572693\n",
      "epoch: 2, batch: 25, loss: 0.5508180856704712\n",
      "epoch: 2, batch: 26, loss: 0.4248504042625427\n",
      "epoch: 2, batch: 27, loss: 0.3711579144001007\n",
      "epoch: 2, batch: 28, loss: 0.5587482452392578\n",
      "epoch: 2, batch: 29, loss: 0.43635132908821106\n",
      "epoch: 2, batch: 30, loss: 0.4986114203929901\n",
      "epoch: 2, batch: 31, loss: 0.7030893564224243\n",
      "epoch: 2, batch: 32, loss: 0.5501004457473755\n",
      "epoch: 2, batch: 33, loss: 0.5542882084846497\n",
      "epoch: 2, batch: 34, loss: 0.7991802096366882\n",
      "epoch: 2, batch: 35, loss: 0.5806149244308472\n",
      "epoch: 2, batch: 36, loss: 0.4879321753978729\n",
      "epoch: 2, batch: 37, loss: 0.42959949374198914\n",
      "epoch: 2, batch: 38, loss: 0.6068147420883179\n",
      "epoch: 2, batch: 39, loss: 0.5776604413986206\n",
      "epoch: 2, batch: 40, loss: 0.8202240467071533\n",
      "epoch: 2, batch: 41, loss: 0.5580489635467529\n",
      "epoch: 2, batch: 42, loss: 0.33890944719314575\n",
      "epoch: 2, batch: 43, loss: 0.6160920262336731\n",
      "epoch: 2, batch: 44, loss: 0.5118041038513184\n",
      "epoch: 2, batch: 45, loss: 0.5316106081008911\n",
      "epoch: 2, batch: 46, loss: 0.5010985136032104\n",
      "epoch: 2, batch: 47, loss: 0.5847818851470947\n",
      "epoch: 2, batch: 48, loss: 0.49100178480148315\n",
      "epoch: 2, batch: 49, loss: 0.620363712310791\n",
      "epoch: 2, batch: 50, loss: 0.5580959320068359\n",
      "epoch: 2, batch: 51, loss: 0.46846577525138855\n",
      "epoch: 2, batch: 52, loss: 0.6120536923408508\n",
      "epoch: 2, batch: 53, loss: 0.5208553671836853\n",
      "epoch: 2, batch: 54, loss: 0.5941118001937866\n",
      "epoch: 2, batch: 55, loss: 0.8480179905891418\n",
      "epoch: 2, batch: 56, loss: 0.5163217782974243\n",
      "epoch: 2, batch: 57, loss: 0.36475780606269836\n",
      "epoch: 2, batch: 58, loss: 0.5238553285598755\n",
      "epoch: 2, batch: 59, loss: 0.4442191421985626\n",
      "epoch: 2, batch: 60, loss: 0.6226677894592285\n",
      "epoch: 2, batch: 61, loss: 0.48569968342781067\n",
      "epoch: 2, batch: 62, loss: 0.36613890528678894\n",
      "epoch: 2, batch: 63, loss: 0.4458141624927521\n",
      "epoch: 2, batch: 64, loss: 0.33131980895996094\n",
      "epoch: 2, batch: 65, loss: 0.6086267232894897\n",
      "epoch: 2, batch: 66, loss: 0.40292081236839294\n",
      "epoch: 2, batch: 67, loss: 0.5638168454170227\n",
      "epoch: 2, batch: 68, loss: 0.5532432198524475\n",
      "epoch: 2, batch: 69, loss: 0.4260907769203186\n",
      "epoch: 2, batch: 70, loss: 0.4313572943210602\n",
      "epoch: 2, batch: 71, loss: 0.37349581718444824\n",
      "epoch: 2, batch: 72, loss: 0.6142582893371582\n",
      "epoch: 2, batch: 73, loss: 0.35479483008384705\n",
      "epoch: 2, batch: 74, loss: 0.6373705267906189\n",
      "epoch: 2, batch: 75, loss: 0.5034632086753845\n",
      "epoch: 2, batch: 76, loss: 0.6386069059371948\n",
      "epoch: 2, batch: 77, loss: 0.42251649498939514\n",
      "epoch: 2, batch: 78, loss: 0.4266834259033203\n",
      "epoch: 2, batch: 79, loss: 0.7328269481658936\n",
      "epoch: 2, batch: 80, loss: 0.3881053924560547\n",
      "epoch: 2, batch: 81, loss: 0.3954433500766754\n",
      "epoch: 2, batch: 82, loss: 0.4319114685058594\n",
      "epoch: 2, batch: 83, loss: 0.3833126723766327\n",
      "epoch: 2, batch: 84, loss: 0.5788314342498779\n",
      "epoch: 2, batch: 85, loss: 0.5602297186851501\n",
      "epoch: 2, batch: 86, loss: 0.5226020812988281\n",
      "epoch: 2, batch: 87, loss: 0.6606476902961731\n",
      "epoch: 2, batch: 88, loss: 0.5920825600624084\n",
      "epoch: 2, batch: 89, loss: 0.5136597752571106\n",
      "epoch: 2, batch: 90, loss: 0.41061392426490784\n",
      "epoch: 2, batch: 91, loss: 0.6231195330619812\n",
      "epoch: 2, batch: 92, loss: 0.5561496615409851\n",
      "epoch: 2, batch: 93, loss: 0.5534456372261047\n",
      "epoch: 2, batch: 94, loss: 0.44685107469558716\n",
      "epoch: 2, batch: 95, loss: 0.4208945333957672\n",
      "epoch: 2, batch: 96, loss: 0.32686540484428406\n",
      "epoch: 2, batch: 97, loss: 0.3770248293876648\n",
      "epoch: 2, batch: 98, loss: 0.46344050765037537\n",
      "epoch: 2, batch: 99, loss: 0.2696634829044342\n",
      "epoch: 2, batch: 100, loss: 0.41480064392089844\n",
      "epoch: 2, batch: 101, loss: 0.6245979070663452\n",
      "epoch: 2, batch: 102, loss: 0.6019209623336792\n",
      "epoch: 2, batch: 103, loss: 0.4774083197116852\n",
      "epoch: 2, batch: 104, loss: 0.35402026772499084\n",
      "epoch: 2, batch: 105, loss: 0.3787008225917816\n",
      "epoch: 2, batch: 106, loss: 0.6909343004226685\n",
      "epoch: 2, batch: 107, loss: 0.5717592239379883\n",
      "epoch: 2, batch: 108, loss: 0.45862627029418945\n",
      "epoch: 2, batch: 109, loss: 0.4952673316001892\n",
      "epoch: 2, batch: 110, loss: 0.4247969388961792\n",
      "epoch: 2, batch: 111, loss: 0.6049157977104187\n",
      "epoch: 2, batch: 112, loss: 0.4077874422073364\n",
      "epoch: 2, batch: 113, loss: 0.6796000003814697\n",
      "epoch: 2, batch: 114, loss: 0.45399564504623413\n",
      "epoch: 2, batch: 115, loss: 0.6482797265052795\n",
      "epoch: 2, batch: 116, loss: 0.4989817142486572\n",
      "epoch: 2, batch: 117, loss: 0.46516430377960205\n",
      "epoch: 2, batch: 118, loss: 0.3662301301956177\n",
      "epoch: 2, batch: 119, loss: 0.44517064094543457\n",
      "epoch: 2, batch: 120, loss: 0.5715100765228271\n",
      "epoch: 2, batch: 121, loss: 0.5966776609420776\n",
      "epoch: 2, batch: 122, loss: 0.5105258226394653\n",
      "epoch: 2, batch: 123, loss: 0.6111836433410645\n",
      "epoch: 2, batch: 124, loss: 0.4459799528121948\n",
      "epoch: 2, batch: 125, loss: 0.8191361427307129\n",
      "epoch: 2, batch: 126, loss: 0.5927284359931946\n",
      "epoch: 2, batch: 127, loss: 0.30115270614624023\n",
      "epoch: 2, batch: 128, loss: 0.42641934752464294\n",
      "epoch: 2, batch: 129, loss: 0.499269038438797\n",
      "epoch: 2, batch: 130, loss: 0.5213696956634521\n",
      "epoch: 2, batch: 131, loss: 0.600233256816864\n",
      "epoch: 2, batch: 132, loss: 0.39472657442092896\n",
      "epoch: 2, batch: 133, loss: 0.702251672744751\n",
      "epoch: 2, batch: 134, loss: 0.4978063106536865\n",
      "epoch: 2, batch: 135, loss: 0.5269795060157776\n",
      "epoch: 2, batch: 136, loss: 0.20518338680267334\n",
      "epoch: 2, batch: 137, loss: 0.4786711633205414\n",
      "epoch: 2, batch: 138, loss: 0.5368707180023193\n",
      "epoch: 2, batch: 139, loss: 0.5617406964302063\n",
      "epoch: 2, batch: 140, loss: 0.4657091498374939\n",
      "epoch: 2, batch: 141, loss: 0.5292800068855286\n",
      "epoch: 2, batch: 142, loss: 0.4055428206920624\n",
      "epoch: 2, batch: 143, loss: 0.3974936604499817\n",
      "epoch: 2, batch: 144, loss: 0.5396376252174377\n",
      "epoch: 2, batch: 145, loss: 0.5154470205307007\n",
      "epoch: 2, batch: 146, loss: 0.5523977875709534\n",
      "epoch: 2, batch: 147, loss: 0.48446038365364075\n",
      "epoch: 2, batch: 148, loss: 0.4923277795314789\n",
      "epoch: 2, batch: 149, loss: 0.5674794912338257\n",
      "epoch: 2, batch: 150, loss: 0.6179835796356201\n",
      "epoch: 2, batch: 151, loss: 0.5834837555885315\n",
      "epoch: 2, batch: 152, loss: 0.3713693618774414\n",
      "epoch: 2, batch: 153, loss: 0.4494457542896271\n",
      "epoch: 2, batch: 154, loss: 0.40071576833724976\n",
      "epoch: 2, batch: 155, loss: 0.5873302817344666\n",
      "epoch: 2, batch: 156, loss: 0.8939865827560425\n",
      "epoch: 2, batch: 157, loss: 0.42304742336273193\n",
      "epoch: 2, batch: 158, loss: 0.5845217108726501\n",
      "epoch: 2, batch: 159, loss: 0.5220085978507996\n",
      "epoch: 2, batch: 160, loss: 0.41443827748298645\n",
      "epoch: 2, batch: 161, loss: 0.3262505829334259\n",
      "epoch: 2, batch: 162, loss: 0.7375026941299438\n",
      "epoch: 2, batch: 163, loss: 0.43372055888175964\n",
      "epoch: 2, batch: 164, loss: 0.49433058500289917\n",
      "epoch: 2, batch: 165, loss: 0.4320027232170105\n",
      "epoch: 2, batch: 166, loss: 0.49977928400039673\n",
      "epoch: 2, batch: 167, loss: 0.48325395584106445\n",
      "epoch: 2, batch: 168, loss: 0.5484039187431335\n",
      "epoch: 2, batch: 169, loss: 0.576130211353302\n",
      "epoch: 2, batch: 170, loss: 0.2884296774864197\n",
      "epoch: 2, batch: 171, loss: 0.6500476598739624\n",
      "epoch: 2, batch: 172, loss: 0.511883020401001\n",
      "epoch: 2, batch: 173, loss: 0.6673123836517334\n",
      "epoch: 2, batch: 174, loss: 0.44990265369415283\n",
      "epoch: 2, batch: 175, loss: 0.5057224631309509\n",
      "epoch: 2, batch: 176, loss: 0.5775339007377625\n",
      "epoch: 2, batch: 177, loss: 0.5013179183006287\n",
      "epoch: 2, batch: 178, loss: 0.5175125598907471\n",
      "epoch: 2, batch: 179, loss: 0.37227723002433777\n",
      "epoch: 2, batch: 180, loss: 0.4455015957355499\n",
      "epoch: 2, batch: 181, loss: 0.5155101418495178\n",
      "epoch: 2, batch: 182, loss: 0.5546693801879883\n",
      "epoch: 2, batch: 183, loss: 0.4760502576828003\n",
      "epoch: 2, batch: 184, loss: 0.4455516040325165\n",
      "epoch: 2, batch: 185, loss: 0.34534549713134766\n",
      "epoch: 2, batch: 186, loss: 0.4526748061180115\n",
      "epoch: 2, batch: 187, loss: 0.41494086384773254\n",
      "epoch: 2, batch: 188, loss: 0.7199380993843079\n",
      "epoch: 2, batch: 189, loss: 0.6146475076675415\n",
      "epoch: 2, batch: 190, loss: 0.40751317143440247\n",
      "epoch: 2, batch: 191, loss: 0.4169055223464966\n",
      "epoch: 2, batch: 192, loss: 0.3713061511516571\n",
      "epoch: 2, batch: 193, loss: 0.5533553957939148\n",
      "epoch: 2, batch: 194, loss: 0.3162044286727905\n",
      "epoch: 2, batch: 195, loss: 0.42725715041160583\n",
      "epoch: 2, batch: 196, loss: 0.6210042238235474\n",
      "epoch: 2, batch: 197, loss: 0.6327008605003357\n",
      "epoch: 2, batch: 198, loss: 0.6870838403701782\n",
      "epoch: 2, batch: 199, loss: 0.5803285837173462\n",
      "epoch: 2, batch: 200, loss: 0.40434765815734863\n",
      "epoch: 2, batch: 201, loss: 0.6450583338737488\n",
      "epoch: 2, batch: 202, loss: 0.7918163537979126\n",
      "epoch: 2, batch: 203, loss: 0.6310465335845947\n",
      "epoch: 2, batch: 204, loss: 0.6119393110275269\n",
      "epoch: 2, batch: 205, loss: 0.3522335886955261\n",
      "epoch: 2, batch: 206, loss: 0.6445342898368835\n",
      "epoch: 2, batch: 207, loss: 0.5400424003601074\n",
      "epoch: 2, batch: 208, loss: 0.36435991525650024\n",
      "epoch: 2, batch: 209, loss: 0.4796602427959442\n",
      "epoch: 2, batch: 210, loss: 0.4082626700401306\n",
      "epoch: 2, batch: 211, loss: 0.47824332118034363\n",
      "epoch: 2, batch: 212, loss: 0.5263425707817078\n",
      "epoch: 2, batch: 213, loss: 0.7861928343772888\n",
      "epoch: 2, batch: 214, loss: 0.4122552275657654\n",
      "epoch: 2, batch: 215, loss: 0.37881651520729065\n",
      "epoch: 2, batch: 216, loss: 0.4151500463485718\n",
      "epoch: 2, batch: 217, loss: 0.648253321647644\n",
      "epoch: 2, batch: 218, loss: 0.3839573264122009\n",
      "epoch: 2, batch: 219, loss: 0.44411417841911316\n",
      "epoch: 2, batch: 220, loss: 0.4327486455440521\n",
      "epoch: 2, batch: 221, loss: 0.5723402500152588\n",
      "epoch: 2, batch: 222, loss: 0.40105870366096497\n",
      "epoch: 2, batch: 223, loss: 0.5135042667388916\n",
      "epoch: 2, batch: 224, loss: 0.642796516418457\n",
      "epoch: 2, batch: 225, loss: 0.62859046459198\n",
      "epoch: 2, batch: 226, loss: 0.7577328681945801\n",
      "epoch: 2, batch: 227, loss: 0.5638061165809631\n",
      "epoch: 2, batch: 228, loss: 0.6717352867126465\n",
      "epoch: 2, batch: 229, loss: 0.5521715879440308\n",
      "epoch: 2, batch: 230, loss: 0.5299018025398254\n",
      "epoch: 2, batch: 231, loss: 0.42545846104621887\n",
      "epoch: 2, batch: 232, loss: 0.5902158617973328\n",
      "epoch: 2, batch: 233, loss: 0.3269573152065277\n",
      "epoch: 2, batch: 234, loss: 0.6952486634254456\n",
      "epoch: 2, batch: 235, loss: 0.5291740298271179\n",
      "epoch: 2, batch: 236, loss: 0.6029182076454163\n",
      "epoch: 2, batch: 237, loss: 0.5445398092269897\n",
      "epoch: 2, batch: 238, loss: 0.363513708114624\n",
      "epoch: 2, batch: 239, loss: 0.44289058446884155\n",
      "epoch: 2, batch: 240, loss: 0.48736730217933655\n",
      "epoch: 2, batch: 241, loss: 0.4630739688873291\n",
      "epoch: 2, batch: 242, loss: 0.5568850040435791\n",
      "epoch: 2, batch: 243, loss: 0.33946773409843445\n",
      "epoch: 2, batch: 244, loss: 0.5784909129142761\n",
      "epoch: 2, batch: 245, loss: 0.5625300407409668\n",
      "epoch: 2, batch: 246, loss: 0.4373047649860382\n",
      "epoch: 2, batch: 247, loss: 0.5813998579978943\n",
      "epoch: 2, batch: 248, loss: 0.4948703646659851\n",
      "epoch: 2, batch: 249, loss: 0.3905546963214874\n",
      "epoch: 2, batch: 250, loss: 0.44259965419769287\n",
      "epoch: 2, batch: 251, loss: 0.42066481709480286\n",
      "epoch: 2, batch: 252, loss: 0.6459475159645081\n",
      "epoch: 2, batch: 253, loss: 0.6321086287498474\n",
      "epoch: 2, batch: 254, loss: 0.4847891628742218\n",
      "epoch: 2, batch: 255, loss: 0.5658289790153503\n",
      "epoch: 2, batch: 256, loss: 0.6566826105117798\n",
      "epoch: 2, batch: 257, loss: 0.35522663593292236\n",
      "epoch: 2, batch: 258, loss: 0.500476062297821\n",
      "epoch: 2, batch: 259, loss: 0.3698370158672333\n",
      "epoch: 2, batch: 260, loss: 0.6975359916687012\n",
      "epoch: 2, batch: 261, loss: 0.4251399338245392\n",
      "epoch: 2, batch: 262, loss: 0.35043442249298096\n",
      "epoch: 2, batch: 263, loss: 0.3789912462234497\n",
      "epoch: 2, batch: 264, loss: 0.4004065990447998\n",
      "epoch: 2, batch: 265, loss: 0.5090935230255127\n",
      "epoch: 2, batch: 266, loss: 0.3663525879383087\n",
      "epoch: 2, batch: 267, loss: 0.4855598509311676\n",
      "epoch: 2, batch: 268, loss: 0.4555329978466034\n",
      "epoch: 2, batch: 269, loss: 0.49233266711235046\n",
      "epoch: 2, batch: 270, loss: 0.5038048624992371\n",
      "epoch: 2, batch: 271, loss: 0.41911402344703674\n",
      "epoch: 2, batch: 272, loss: 0.47343575954437256\n",
      "epoch: 2, batch: 273, loss: 0.49540823698043823\n",
      "epoch: 2, batch: 274, loss: 0.3987255394458771\n",
      "epoch: 2, batch: 275, loss: 0.4138111472129822\n",
      "epoch: 2, batch: 276, loss: 0.4279683828353882\n",
      "epoch: 2, batch: 277, loss: 0.3816637098789215\n",
      "epoch: 2, batch: 278, loss: 0.44315826892852783\n",
      "epoch: 2, batch: 279, loss: 0.31416332721710205\n",
      "epoch: 2, batch: 280, loss: 0.5684868097305298\n",
      "epoch: 2, batch: 281, loss: 0.5731911063194275\n",
      "epoch: 2, batch: 282, loss: 0.3754401206970215\n",
      "epoch: 2, batch: 283, loss: 0.4585087299346924\n",
      "epoch: 2, batch: 284, loss: 0.5959469676017761\n",
      "epoch: 2, batch: 285, loss: 0.3023497760295868\n",
      "epoch: 2, batch: 286, loss: 0.41525447368621826\n",
      "epoch: 2, batch: 287, loss: 0.563332200050354\n",
      "epoch: 2, batch: 288, loss: 0.7973220944404602\n",
      "epoch: 2, batch: 289, loss: 0.3621268570423126\n",
      "epoch: 2, batch: 290, loss: 0.4392085671424866\n",
      "epoch: 2, batch: 291, loss: 0.7637102007865906\n",
      "epoch: 2, batch: 292, loss: 0.5069003701210022\n",
      "epoch: 2, batch: 293, loss: 0.2996805012226105\n",
      "epoch: 2, batch: 294, loss: 0.5937529802322388\n",
      "epoch: 2, batch: 295, loss: 0.4518774449825287\n",
      "epoch: 2, batch: 296, loss: 0.4090002477169037\n",
      "epoch: 2, batch: 297, loss: 0.3926357924938202\n",
      "epoch: 2, batch: 298, loss: 0.6480333805084229\n",
      "epoch: 2, batch: 299, loss: 0.46948888897895813\n",
      "epoch: 2, batch: 300, loss: 0.531262993812561\n",
      "epoch: 2, batch: 301, loss: 0.4949171543121338\n",
      "epoch: 2, batch: 302, loss: 0.6087329983711243\n",
      "epoch: 2, batch: 303, loss: 0.6186652779579163\n",
      "epoch: 2, batch: 304, loss: 0.5731757283210754\n",
      "epoch: 2, batch: 305, loss: 0.5017593502998352\n",
      "epoch: 2, batch: 306, loss: 0.41485631465911865\n",
      "epoch: 2, batch: 307, loss: 0.4293106198310852\n",
      "epoch: 2, batch: 308, loss: 0.517188310623169\n",
      "epoch: 2, batch: 309, loss: 0.3313036561012268\n",
      "epoch: 2, batch: 310, loss: 0.4168304204940796\n",
      "epoch: 2, batch: 311, loss: 0.6460897922515869\n",
      "epoch: 2, batch: 312, loss: 0.4544396698474884\n",
      "epoch: 2, batch: 313, loss: 0.3441437780857086\n",
      "epoch: 2, batch: 314, loss: 0.5719502568244934\n",
      "epoch: 2, batch: 315, loss: 0.5261913537979126\n",
      "epoch: 2, batch: 316, loss: 0.4857714772224426\n",
      "epoch: 2, batch: 317, loss: 0.4314239025115967\n",
      "epoch: 2, batch: 318, loss: 0.6153639554977417\n",
      "epoch: 2, batch: 319, loss: 0.46886107325553894\n",
      "epoch: 2, batch: 320, loss: 0.5760715007781982\n",
      "epoch: 2, batch: 321, loss: 0.452126681804657\n",
      "epoch: 2, batch: 322, loss: 0.35868778824806213\n",
      "epoch: 2, batch: 323, loss: 0.4158778190612793\n",
      "epoch: 2, batch: 324, loss: 0.3464602530002594\n",
      "epoch: 2, batch: 325, loss: 0.31049028038978577\n",
      "epoch: 2, batch: 326, loss: 0.3848516047000885\n",
      "epoch: 2, batch: 327, loss: 0.4041375517845154\n",
      "epoch: 2, batch: 328, loss: 0.5138487815856934\n",
      "epoch: 2, batch: 329, loss: 0.3832775950431824\n",
      "epoch: 2, batch: 330, loss: 0.4350931942462921\n",
      "epoch: 2, batch: 331, loss: 0.36935263872146606\n",
      "epoch: 2, batch: 332, loss: 0.6423050165176392\n",
      "epoch: 2, batch: 333, loss: 0.5571306943893433\n",
      "epoch: 2, batch: 334, loss: 0.232568621635437\n",
      "epoch: 2, batch: 335, loss: 0.4633563160896301\n",
      "epoch: 2, batch: 336, loss: 0.2639283537864685\n",
      "epoch: 2, batch: 337, loss: 0.5527758002281189\n",
      "epoch: 2, batch: 338, loss: 0.5388262271881104\n",
      "epoch: 2, batch: 339, loss: 0.4549638330936432\n",
      "epoch: 2, batch: 340, loss: 0.5639693737030029\n",
      "epoch: 2, batch: 341, loss: 0.52360600233078\n",
      "epoch: 2, batch: 342, loss: 0.24049779772758484\n",
      "epoch: 2, batch: 343, loss: 0.31372499465942383\n",
      "epoch: 2, batch: 344, loss: 0.3233402669429779\n",
      "epoch: 2, batch: 345, loss: 0.47573569416999817\n",
      "epoch: 2, batch: 346, loss: 0.4292495846748352\n",
      "epoch: 2, batch: 347, loss: 0.6459931135177612\n",
      "epoch: 2, batch: 348, loss: 0.649146318435669\n",
      "epoch: 2, batch: 349, loss: 0.4504951536655426\n",
      "epoch: 2, batch: 350, loss: 0.5791847109794617\n",
      "epoch: 2, batch: 351, loss: 0.6976593136787415\n",
      "epoch: 2, batch: 352, loss: 0.3742823898792267\n",
      "epoch: 2, batch: 353, loss: 0.66942298412323\n",
      "epoch: 2, batch: 354, loss: 0.36105409264564514\n",
      "epoch: 2, batch: 355, loss: 0.739373505115509\n",
      "epoch: 2, batch: 356, loss: 0.2923305928707123\n",
      "epoch: 2, batch: 357, loss: 0.33409982919692993\n",
      "epoch: 2, batch: 358, loss: 0.390077143907547\n",
      "epoch: 2, batch: 359, loss: 0.4498615860939026\n",
      "epoch: 2, batch: 360, loss: 0.41123369336128235\n",
      "epoch: 2, batch: 361, loss: 0.6697965264320374\n",
      "epoch: 2, batch: 362, loss: 0.4827911853790283\n",
      "epoch: 2, batch: 363, loss: 0.3967784345149994\n",
      "epoch: 2, batch: 364, loss: 0.4487999677658081\n",
      "epoch: 2, batch: 365, loss: 0.6792147159576416\n",
      "epoch: 2, batch: 366, loss: 0.4689774215221405\n",
      "epoch: 2, batch: 367, loss: 0.5520139932632446\n",
      "epoch: 2, batch: 368, loss: 0.3628808259963989\n",
      "epoch: 2, batch: 369, loss: 0.5040318369865417\n",
      "epoch: 2, batch: 370, loss: 0.4258238673210144\n",
      "epoch: 2, batch: 371, loss: 0.6144314408302307\n",
      "epoch: 2, batch: 372, loss: 0.34045282006263733\n",
      "epoch: 2, batch: 373, loss: 0.5917448997497559\n",
      "epoch: 2, batch: 374, loss: 0.8100945949554443\n",
      "epoch: 2, batch: 375, loss: 0.5177599787712097\n",
      "epoch: 2, batch: 376, loss: 0.3498663604259491\n",
      "epoch: 2, batch: 377, loss: 0.4860912263393402\n",
      "epoch: 2, batch: 378, loss: 0.38516902923583984\n",
      "epoch: 2, batch: 379, loss: 0.36824700236320496\n",
      "epoch: 2, batch: 380, loss: 0.3162807524204254\n",
      "epoch: 2, batch: 381, loss: 0.44989722967147827\n",
      "epoch: 2, batch: 382, loss: 0.4237291216850281\n",
      "epoch: 2, batch: 383, loss: 0.4885764420032501\n",
      "epoch: 2, batch: 384, loss: 0.48251768946647644\n",
      "epoch: 2, batch: 385, loss: 0.590358316898346\n",
      "epoch: 2, batch: 386, loss: 0.3407670557498932\n",
      "epoch: 2, batch: 387, loss: 0.5923705101013184\n",
      "epoch: 2, batch: 388, loss: 0.4311138987541199\n",
      "epoch: 2, batch: 389, loss: 0.2644185423851013\n",
      "epoch: 2, batch: 390, loss: 0.7436864376068115\n",
      "epoch: 2, batch: 391, loss: 0.4376364052295685\n",
      "epoch: 2, batch: 392, loss: 0.4847630560398102\n",
      "epoch: 2, batch: 393, loss: 0.473829448223114\n",
      "epoch: 2, batch: 394, loss: 0.5393492579460144\n",
      "epoch: 2, batch: 395, loss: 0.39898526668548584\n",
      "epoch: 2, batch: 396, loss: 0.49614495038986206\n",
      "epoch: 2, batch: 397, loss: 0.407486230134964\n",
      "epoch: 2, batch: 398, loss: 0.6351458430290222\n",
      "epoch: 2, batch: 399, loss: 0.5407161116600037\n",
      "epoch: 2, batch: 400, loss: 0.4470718204975128\n",
      "epoch: 2, batch: 401, loss: 0.4260385036468506\n",
      "epoch: 2, batch: 402, loss: 0.5568916201591492\n",
      "epoch: 2, batch: 403, loss: 0.29413720965385437\n",
      "epoch: 2, batch: 404, loss: 0.45243698358535767\n",
      "epoch: 2, batch: 405, loss: 0.5620386004447937\n",
      "epoch: 2, batch: 406, loss: 0.44182032346725464\n",
      "epoch: 2, batch: 407, loss: 0.4013760983943939\n",
      "epoch: 2, batch: 408, loss: 0.4677371084690094\n",
      "epoch: 2, batch: 409, loss: 0.4953277111053467\n",
      "epoch: 2, batch: 410, loss: 0.38841500878334045\n",
      "epoch: 2, batch: 411, loss: 0.49232086539268494\n",
      "epoch: 2, batch: 412, loss: 0.35782119631767273\n",
      "epoch: 2, batch: 413, loss: 0.40381503105163574\n",
      "epoch: 2, batch: 414, loss: 0.4353766441345215\n",
      "epoch: 2, batch: 415, loss: 0.4554285407066345\n",
      "epoch: 2, batch: 416, loss: 0.36098554730415344\n",
      "epoch: 2, batch: 417, loss: 0.30922621488571167\n",
      "epoch: 2, batch: 418, loss: 0.28866657614707947\n",
      "epoch: 2, batch: 419, loss: 0.5389092564582825\n",
      "epoch: 2, batch: 420, loss: 0.5003883838653564\n",
      "epoch: 2, batch: 421, loss: 0.5277706384658813\n",
      "epoch: 2, batch: 422, loss: 0.5566208958625793\n",
      "epoch: 2, batch: 423, loss: 0.503229558467865\n",
      "epoch: 2, batch: 424, loss: 0.5115251541137695\n",
      "epoch: 2, batch: 425, loss: 0.689819872379303\n",
      "epoch: 2, batch: 426, loss: 0.6666070222854614\n",
      "epoch: 2, batch: 427, loss: 0.37825971841812134\n",
      "epoch: 2, batch: 428, loss: 0.3474486470222473\n",
      "epoch: 2, batch: 429, loss: 0.4297242760658264\n",
      "epoch: 2, batch: 430, loss: 0.4599741995334625\n",
      "epoch: 2, batch: 431, loss: 0.5235175490379333\n",
      "epoch: 2, batch: 432, loss: 0.4866226315498352\n",
      "epoch: 2, batch: 433, loss: 0.3421177268028259\n",
      "epoch: 2, batch: 434, loss: 0.3848631978034973\n",
      "epoch: 2, batch: 435, loss: 0.5129269957542419\n",
      "epoch: 2, batch: 436, loss: 0.5473196506500244\n",
      "epoch: 2, batch: 437, loss: 0.455342173576355\n",
      "epoch: 2, batch: 438, loss: 0.45796510577201843\n",
      "epoch: 2, batch: 439, loss: 0.4660419523715973\n",
      "epoch: 2, batch: 440, loss: 0.5142229795455933\n",
      "epoch: 2, batch: 441, loss: 0.4102076292037964\n",
      "epoch: 2, batch: 442, loss: 0.3402641713619232\n",
      "epoch: 2, batch: 443, loss: 0.5913501381874084\n",
      "epoch: 2, batch: 444, loss: 0.5558770298957825\n",
      "epoch: 2, batch: 445, loss: 0.5255035758018494\n",
      "epoch: 2, batch: 446, loss: 0.4338962733745575\n",
      "epoch: 2, batch: 447, loss: 0.4504842758178711\n",
      "epoch: 2, batch: 448, loss: 0.35782358050346375\n",
      "epoch: 2, batch: 449, loss: 0.6737719774246216\n",
      "epoch: 2, batch: 450, loss: 0.4327368438243866\n",
      "epoch: 2, batch: 451, loss: 0.6094180345535278\n",
      "epoch: 2, batch: 452, loss: 0.44216471910476685\n",
      "epoch: 2, batch: 453, loss: 0.3302006721496582\n",
      "epoch: 2, batch: 454, loss: 0.3426930010318756\n",
      "epoch: 2, batch: 455, loss: 0.6088899970054626\n",
      "epoch: 2, batch: 456, loss: 0.7032833099365234\n",
      "epoch: 2, batch: 457, loss: 0.5201270580291748\n",
      "epoch: 2, batch: 458, loss: 0.38528335094451904\n",
      "epoch: 2, batch: 459, loss: 0.3702467679977417\n",
      "epoch: 2, batch: 460, loss: 0.42609700560569763\n",
      "epoch: 2, batch: 461, loss: 0.4799039363861084\n",
      "epoch: 2, batch: 462, loss: 0.4940658211708069\n",
      "epoch: 2, batch: 463, loss: 0.4236881136894226\n",
      "epoch: 2, batch: 464, loss: 0.42981380224227905\n",
      "epoch: 2, batch: 465, loss: 0.5212585926055908\n",
      "epoch: 2, batch: 466, loss: 0.5112255215644836\n",
      "epoch: 2, batch: 467, loss: 0.4727747142314911\n",
      "epoch: 2, batch: 468, loss: 0.5416128039360046\n",
      "epoch: 2, batch: 469, loss: 0.31761857867240906\n",
      "epoch: 2, batch: 470, loss: 0.4462157189846039\n",
      "epoch: 2, batch: 471, loss: 0.42870497703552246\n",
      "epoch: 2, batch: 472, loss: 0.5962963700294495\n",
      "epoch: 2, batch: 473, loss: 0.29452750086784363\n",
      "epoch: 2, batch: 474, loss: 0.5469644069671631\n",
      "epoch: 2, batch: 475, loss: 0.34647640585899353\n",
      "epoch: 2, batch: 476, loss: 0.40141817927360535\n",
      "epoch: 2, batch: 477, loss: 0.3635290563106537\n",
      "epoch: 2, batch: 478, loss: 0.38023409247398376\n",
      "epoch: 2, batch: 479, loss: 0.43080297112464905\n",
      "epoch: 2, batch: 480, loss: 0.5109825134277344\n",
      "epoch: 2, batch: 481, loss: 0.5293112993240356\n",
      "epoch: 2, batch: 482, loss: 0.3961997330188751\n",
      "epoch: 2, batch: 483, loss: 0.3207172751426697\n",
      "epoch: 2, batch: 484, loss: 0.5379771590232849\n",
      "epoch: 2, batch: 485, loss: 0.2788134217262268\n",
      "epoch: 2, batch: 486, loss: 0.5868940949440002\n",
      "epoch: 2, batch: 487, loss: 0.4259714186191559\n",
      "epoch: 2, batch: 488, loss: 0.559428870677948\n",
      "epoch: 2, batch: 489, loss: 0.4684171974658966\n",
      "epoch: 2, batch: 490, loss: 0.4030695855617523\n",
      "epoch: 2, batch: 491, loss: 0.6242319345474243\n",
      "epoch: 2, batch: 492, loss: 0.28817006945610046\n",
      "epoch: 2, batch: 493, loss: 0.4534108638763428\n",
      "epoch: 2, batch: 494, loss: 0.39565759897232056\n",
      "epoch: 2, batch: 495, loss: 0.464682400226593\n",
      "epoch: 2, batch: 496, loss: 0.6044809818267822\n",
      "epoch: 2, batch: 497, loss: 0.35384073853492737\n",
      "epoch: 2, batch: 498, loss: 0.2799712121486664\n",
      "epoch: 2, batch: 499, loss: 0.43569958209991455\n",
      "epoch: 2, batch: 500, loss: 0.33595502376556396\n",
      "epoch: 2, batch: 501, loss: 0.4947797358036041\n",
      "epoch: 2, batch: 502, loss: 0.6425273418426514\n",
      "epoch: 2, batch: 503, loss: 0.6111144423484802\n",
      "epoch: 2, batch: 504, loss: 0.2240976095199585\n",
      "epoch: 2, batch: 505, loss: 0.5601988434791565\n",
      "epoch: 2, batch: 506, loss: 0.3072636127471924\n",
      "epoch: 2, batch: 507, loss: 0.4044088125228882\n",
      "epoch: 2, batch: 508, loss: 0.341419517993927\n",
      "epoch: 2, batch: 509, loss: 0.5010143518447876\n",
      "epoch: 2, batch: 510, loss: 0.3963138163089752\n",
      "epoch: 2, batch: 511, loss: 0.604323148727417\n",
      "epoch: 2, batch: 512, loss: 0.3246036469936371\n",
      "epoch: 2, batch: 513, loss: 0.44874733686447144\n",
      "epoch: 2, batch: 514, loss: 0.34348762035369873\n",
      "epoch: 2, batch: 515, loss: 0.4471750855445862\n",
      "epoch: 2, batch: 516, loss: 0.49528923630714417\n",
      "epoch: 2, batch: 517, loss: 0.403994083404541\n",
      "epoch: 2, batch: 518, loss: 0.5483537912368774\n",
      "epoch: 2, batch: 519, loss: 0.4604637324810028\n",
      "epoch: 2, batch: 520, loss: 0.32931190729141235\n",
      "epoch: 2, batch: 521, loss: 0.39291369915008545\n",
      "epoch: 2, batch: 522, loss: 0.4170423448085785\n",
      "epoch: 2, batch: 523, loss: 0.5456193089485168\n",
      "epoch: 2, batch: 524, loss: 0.37507277727127075\n",
      "epoch: 2, batch: 525, loss: 0.46928706765174866\n",
      "epoch: 2, batch: 526, loss: 0.5148378014564514\n",
      "epoch: 2, batch: 527, loss: 0.288497656583786\n",
      "epoch: 2, batch: 528, loss: 0.6125563383102417\n",
      "epoch: 2, batch: 529, loss: 0.47033411264419556\n",
      "epoch: 2, batch: 530, loss: 0.48677411675453186\n",
      "epoch: 2, batch: 531, loss: 0.46288561820983887\n",
      "epoch: 2, batch: 532, loss: 0.5316524505615234\n",
      "epoch: 2, batch: 533, loss: 0.541826605796814\n",
      "epoch: 2, batch: 534, loss: 0.4337795078754425\n",
      "epoch: 2, batch: 535, loss: 0.409988671541214\n",
      "epoch: 2, batch: 536, loss: 0.37982994318008423\n",
      "epoch: 2, batch: 537, loss: 0.4497006833553314\n",
      "epoch: 2, batch: 538, loss: 0.3054135739803314\n",
      "epoch: 2, batch: 539, loss: 0.555709958076477\n",
      "epoch: 2, batch: 540, loss: 0.6349033713340759\n",
      "epoch: 2, batch: 541, loss: 0.4275466501712799\n",
      "epoch: 2, batch: 542, loss: 0.4433811902999878\n",
      "epoch: 2, batch: 543, loss: 0.6248385310173035\n",
      "epoch: 2, batch: 544, loss: 0.5381338596343994\n",
      "epoch: 2, batch: 545, loss: 0.27912887930870056\n",
      "epoch: 2, batch: 546, loss: 0.44335392117500305\n",
      "epoch: 2, batch: 547, loss: 0.4180605411529541\n",
      "epoch: 2, batch: 548, loss: 0.5647185444831848\n",
      "epoch: 2, batch: 549, loss: 0.5917098522186279\n",
      "epoch: 2, batch: 550, loss: 0.4025194048881531\n",
      "epoch: 2, batch: 551, loss: 0.3065314292907715\n",
      "epoch: 2, batch: 552, loss: 0.5211362838745117\n",
      "epoch: 2, batch: 553, loss: 0.5042617917060852\n",
      "epoch: 2, batch: 554, loss: 0.3617488741874695\n",
      "epoch: 2, batch: 555, loss: 0.5012179017066956\n",
      "epoch: 2, batch: 556, loss: 0.41345417499542236\n",
      "epoch: 2, batch: 557, loss: 0.35535186529159546\n",
      "epoch: 2, batch: 558, loss: 0.5558730959892273\n",
      "epoch: 2, batch: 559, loss: 0.27189430594444275\n",
      "epoch: 2, batch: 560, loss: 0.47907713055610657\n",
      "epoch: 2, batch: 561, loss: 0.3075430691242218\n",
      "epoch: 2, batch: 562, loss: 0.4304305911064148\n",
      "epoch: 2, batch: 563, loss: 0.5268312692642212\n",
      "epoch: 2, batch: 564, loss: 0.539776086807251\n",
      "epoch: 2, batch: 565, loss: 0.4203612208366394\n",
      "epoch: 2, batch: 566, loss: 0.36295467615127563\n",
      "epoch: 2, batch: 567, loss: 0.3492887020111084\n",
      "epoch: 2, batch: 568, loss: 0.34626495838165283\n",
      "epoch: 2, batch: 569, loss: 0.292231947183609\n",
      "epoch: 2, batch: 570, loss: 0.43383169174194336\n",
      "epoch: 2, batch: 571, loss: 0.33551448583602905\n",
      "epoch: 2, batch: 572, loss: 0.5823379755020142\n",
      "epoch: 2, batch: 573, loss: 0.46602874994277954\n",
      "epoch: 2, batch: 574, loss: 0.4654153883457184\n",
      "epoch: 2, batch: 575, loss: 0.5537570118904114\n",
      "epoch: 2, batch: 576, loss: 0.5554780960083008\n",
      "epoch: 2, batch: 577, loss: 0.348470002412796\n",
      "epoch: 2, batch: 578, loss: 0.19657118618488312\n",
      "epoch: 2, batch: 579, loss: 0.52173912525177\n",
      "epoch: 2, batch: 580, loss: 0.5149286985397339\n",
      "epoch: 2, batch: 581, loss: 0.630297839641571\n",
      "epoch: 2, batch: 582, loss: 0.467068076133728\n",
      "epoch: 2, batch: 583, loss: 0.5464944839477539\n",
      "epoch: 2, batch: 584, loss: 0.6817545294761658\n",
      "epoch: 2, batch: 585, loss: 0.5437830686569214\n",
      "epoch: 2, batch: 586, loss: 0.3441122770309448\n",
      "epoch: 2, batch: 587, loss: 0.2942790687084198\n",
      "epoch: 2, batch: 588, loss: 0.564791738986969\n",
      "epoch: 2, batch: 589, loss: 0.5405666828155518\n",
      "epoch: 2, batch: 590, loss: 0.5042344927787781\n",
      "epoch: 2, batch: 591, loss: 0.5067522525787354\n",
      "epoch: 2, batch: 592, loss: 0.6103488802909851\n",
      "epoch: 2, batch: 593, loss: 0.5426198244094849\n",
      "epoch: 2, batch: 594, loss: 0.45254573225975037\n",
      "epoch: 2, batch: 595, loss: 0.49308526515960693\n",
      "epoch: 2, batch: 596, loss: 0.4241660535335541\n",
      "epoch: 2, batch: 597, loss: 0.40089747309684753\n",
      "epoch: 2, batch: 598, loss: 0.5155225992202759\n",
      "epoch: 2, batch: 599, loss: 0.32667404413223267\n",
      "epoch: 2, batch: 600, loss: 0.4546489417552948\n",
      "epoch: 2, batch: 601, loss: 0.4534544348716736\n",
      "epoch: 2, batch: 602, loss: 0.4549919664859772\n",
      "epoch: 2, batch: 603, loss: 0.33132052421569824\n",
      "epoch: 2, batch: 604, loss: 0.5919433832168579\n",
      "epoch: 2, batch: 605, loss: 0.5158668756484985\n",
      "epoch: 2, batch: 606, loss: 0.3675297796726227\n",
      "epoch: 2, batch: 607, loss: 0.3883746266365051\n",
      "epoch: 2, batch: 608, loss: 0.3159644305706024\n",
      "epoch: 2, batch: 609, loss: 0.5237341523170471\n",
      "epoch: 2, batch: 610, loss: 0.386292427778244\n",
      "epoch: 2, batch: 611, loss: 0.2233859747648239\n",
      "epoch: 2, batch: 612, loss: 0.31168800592422485\n",
      "epoch: 2, batch: 613, loss: 0.3487932085990906\n",
      "epoch: 2, batch: 614, loss: 0.3593771159648895\n",
      "epoch: 2, batch: 615, loss: 0.30519312620162964\n",
      "epoch: 2, batch: 616, loss: 0.4223610758781433\n",
      "epoch: 2, batch: 617, loss: 0.4266011118888855\n",
      "epoch: 2, batch: 618, loss: 0.5177931785583496\n",
      "epoch: 2, batch: 619, loss: 0.4807491898536682\n",
      "epoch: 2, batch: 620, loss: 0.1860901266336441\n",
      "epoch: 2, batch: 621, loss: 0.370734304189682\n",
      "epoch: 2, batch: 622, loss: 0.37770596146583557\n",
      "epoch: 2, batch: 623, loss: 0.6720845699310303\n",
      "epoch: 2, batch: 624, loss: 0.2628735601902008\n",
      "epoch: 2, batch: 625, loss: 0.29328569769859314\n",
      "epoch: 2, batch: 626, loss: 0.20342758297920227\n",
      "epoch: 2, batch: 627, loss: 0.3934326767921448\n",
      "epoch: 2, batch: 628, loss: 0.5640363693237305\n",
      "epoch: 2, batch: 629, loss: 0.4917580485343933\n",
      "epoch: 2, batch: 630, loss: 0.261994868516922\n",
      "epoch: 2, batch: 631, loss: 0.4619865417480469\n",
      "epoch: 2, batch: 632, loss: 0.30216509103775024\n",
      "epoch: 2, batch: 633, loss: 0.5658265352249146\n",
      "epoch: 2, batch: 634, loss: 0.5889288187026978\n",
      "epoch: 2, batch: 635, loss: 0.4311259686946869\n",
      "epoch: 2, batch: 636, loss: 0.318448930978775\n",
      "epoch: 2, batch: 637, loss: 0.3990601897239685\n",
      "epoch: 2, batch: 638, loss: 0.38795262575149536\n",
      "epoch: 2, batch: 639, loss: 0.23502188920974731\n",
      "epoch: 2, batch: 640, loss: 0.4502412676811218\n",
      "epoch: 2, batch: 641, loss: 0.3274557590484619\n",
      "epoch: 2, batch: 642, loss: 0.40238404273986816\n",
      "epoch: 2, batch: 643, loss: 0.43674346804618835\n",
      "epoch: 2, batch: 644, loss: 0.32745227217674255\n",
      "epoch: 2, batch: 645, loss: 0.32274457812309265\n",
      "epoch: 2, batch: 646, loss: 0.4473722577095032\n",
      "epoch: 2, batch: 647, loss: 0.44504866003990173\n",
      "epoch: 2, batch: 648, loss: 0.46571850776672363\n",
      "epoch: 2, batch: 649, loss: 0.47807952761650085\n",
      "epoch: 2, batch: 650, loss: 0.29007357358932495\n",
      "epoch: 2, batch: 651, loss: 0.3567616641521454\n",
      "epoch: 2, batch: 652, loss: 0.6902691125869751\n",
      "epoch: 2, batch: 653, loss: 0.31793326139450073\n",
      "epoch: 2, batch: 654, loss: 0.45510515570640564\n",
      "epoch: 2, batch: 655, loss: 0.48602843284606934\n",
      "epoch: 2, batch: 656, loss: 0.5773400664329529\n",
      "epoch: 2, batch: 657, loss: 0.48195183277130127\n",
      "epoch: 2, batch: 658, loss: 0.5321582555770874\n",
      "epoch: 2, batch: 659, loss: 0.40391868352890015\n",
      "epoch: 2, batch: 660, loss: 0.33751922845840454\n",
      "epoch: 2, batch: 661, loss: 0.37896040081977844\n",
      "epoch: 2, batch: 662, loss: 0.4755290448665619\n",
      "epoch: 2, batch: 663, loss: 0.4462999105453491\n",
      "epoch: 2, batch: 664, loss: 0.8541427850723267\n",
      "epoch: 2, batch: 665, loss: 0.4100479483604431\n",
      "epoch: 2, batch: 666, loss: 0.374399870634079\n",
      "epoch: 2, batch: 667, loss: 0.4841119647026062\n",
      "epoch: 2, batch: 668, loss: 0.22517243027687073\n",
      "epoch: 2, batch: 669, loss: 0.5204256772994995\n",
      "epoch: 2, batch: 670, loss: 0.30182838439941406\n",
      "epoch: 2, batch: 671, loss: 0.3841928541660309\n",
      "epoch: 2, batch: 672, loss: 0.41711947321891785\n",
      "epoch: 2, batch: 673, loss: 0.4137982130050659\n",
      "epoch: 2, batch: 674, loss: 0.27026763558387756\n",
      "epoch: 2, batch: 675, loss: 0.5748892426490784\n",
      "epoch: 2, batch: 676, loss: 0.5994085669517517\n",
      "epoch: 2, batch: 677, loss: 0.48316580057144165\n",
      "epoch: 2, batch: 678, loss: 0.4438685178756714\n",
      "epoch: 2, batch: 679, loss: 0.37931373715400696\n",
      "epoch: 2, batch: 680, loss: 0.5861894488334656\n",
      "epoch: 2, batch: 681, loss: 0.5419679284095764\n",
      "epoch: 2, batch: 682, loss: 0.2967891991138458\n",
      "epoch: 2, batch: 683, loss: 0.36241427063941956\n",
      "epoch: 2, batch: 684, loss: 0.5923022031784058\n",
      "epoch: 2, batch: 685, loss: 0.5038070678710938\n",
      "epoch: 2, batch: 686, loss: 0.3499726951122284\n",
      "epoch: 2, batch: 687, loss: 0.3777701258659363\n",
      "epoch: 2, batch: 688, loss: 0.37139156460762024\n",
      "epoch: 2, batch: 689, loss: 0.2800712585449219\n",
      "epoch: 2, batch: 690, loss: 0.350898802280426\n",
      "epoch: 2, batch: 691, loss: 0.44312795996665955\n",
      "epoch: 2, batch: 692, loss: 0.4712640047073364\n",
      "epoch: 2, batch: 693, loss: 0.24309290945529938\n",
      "epoch: 2, batch: 694, loss: 0.3232569396495819\n",
      "epoch: 2, batch: 695, loss: 0.4920109212398529\n",
      "epoch: 2, batch: 696, loss: 0.36341264843940735\n",
      "epoch: 2, batch: 697, loss: 0.23792660236358643\n",
      "epoch: 2, batch: 698, loss: 0.5353942513465881\n",
      "epoch: 2, batch: 699, loss: 0.3624192178249359\n",
      "epoch: 2, batch: 700, loss: 0.2858310639858246\n",
      "epoch: 2, batch: 701, loss: 0.3642973005771637\n",
      "epoch: 2, batch: 702, loss: 0.3251281976699829\n",
      "epoch: 2, batch: 703, loss: 0.41099947690963745\n",
      "epoch: 2, batch: 704, loss: 0.3971344530582428\n",
      "epoch: 2, batch: 705, loss: 0.37667953968048096\n",
      "epoch: 2, batch: 706, loss: 0.5195073485374451\n",
      "epoch: 2, batch: 707, loss: 0.295686811208725\n",
      "epoch: 2, batch: 708, loss: 0.2809216380119324\n",
      "epoch: 2, batch: 709, loss: 0.27763766050338745\n",
      "epoch: 2, batch: 710, loss: 0.4122001528739929\n",
      "epoch: 2, batch: 711, loss: 0.42986351251602173\n",
      "epoch: 2, batch: 712, loss: 0.3533288836479187\n",
      "epoch: 2, batch: 713, loss: 0.2297622114419937\n",
      "epoch: 2, batch: 714, loss: 0.48099812865257263\n",
      "epoch: 2, batch: 715, loss: 0.21875685453414917\n",
      "epoch: 2, batch: 716, loss: 0.7054283618927002\n",
      "epoch: 2, batch: 717, loss: 0.6642957925796509\n",
      "epoch: 2, batch: 718, loss: 0.5523443222045898\n",
      "epoch: 2, batch: 719, loss: 0.3261162340641022\n",
      "epoch: 2, batch: 720, loss: 0.17551374435424805\n",
      "epoch: 2, batch: 721, loss: 0.5684168338775635\n",
      "epoch: 2, batch: 722, loss: 0.3515491783618927\n",
      "epoch: 2, batch: 723, loss: 0.31040793657302856\n",
      "epoch: 2, batch: 724, loss: 0.319589763879776\n",
      "epoch: 2, batch: 725, loss: 0.2486269772052765\n",
      "epoch: 2, batch: 726, loss: 0.6440203189849854\n",
      "epoch: 2, batch: 727, loss: 0.33907923102378845\n",
      "epoch: 2, batch: 728, loss: 0.6803005933761597\n",
      "epoch: 2, batch: 729, loss: 0.4858342111110687\n",
      "epoch: 2, batch: 730, loss: 0.36654549837112427\n",
      "epoch: 2, batch: 731, loss: 0.31014207005500793\n",
      "epoch: 2, batch: 732, loss: 0.5435820817947388\n",
      "epoch: 2, batch: 733, loss: 0.5179658532142639\n",
      "epoch: 2, batch: 734, loss: 0.42529064416885376\n",
      "epoch: 2, batch: 735, loss: 0.3161746859550476\n",
      "epoch: 2, batch: 736, loss: 0.578653872013092\n",
      "epoch: 2, batch: 737, loss: 0.2832459807395935\n",
      "epoch: 2, batch: 738, loss: 0.4003406763076782\n",
      "epoch: 2, batch: 739, loss: 0.35559388995170593\n",
      "epoch: 2, batch: 740, loss: 0.5876026749610901\n",
      "epoch: 2, batch: 741, loss: 0.2588738799095154\n",
      "epoch: 2, batch: 742, loss: 0.4119647145271301\n",
      "epoch: 2, batch: 743, loss: 0.4814360737800598\n",
      "epoch: 2, batch: 744, loss: 0.5024824738502502\n",
      "epoch: 2, batch: 745, loss: 0.4044094681739807\n",
      "epoch: 2, batch: 746, loss: 0.3372000753879547\n",
      "epoch: 2, batch: 747, loss: 0.327875018119812\n",
      "epoch: 2, batch: 748, loss: 0.5120123028755188\n",
      "epoch: 2, batch: 749, loss: 0.4898053705692291\n",
      "epoch: 2, batch: 750, loss: 0.6921597123146057\n",
      "epoch: 2, batch: 751, loss: 0.43477657437324524\n",
      "epoch: 2, batch: 752, loss: 0.4961399435997009\n",
      "epoch: 2, batch: 753, loss: 0.5837632417678833\n",
      "epoch: 2, batch: 754, loss: 0.6211419105529785\n",
      "epoch: 2, batch: 755, loss: 0.32155972719192505\n",
      "epoch: 2, batch: 756, loss: 0.36539575457572937\n",
      "epoch: 2, batch: 757, loss: 0.335676908493042\n",
      "epoch: 2, batch: 758, loss: 0.34333908557891846\n",
      "epoch: 2, batch: 759, loss: 0.4925684928894043\n",
      "epoch: 2, batch: 760, loss: 0.16666032373905182\n",
      "epoch: 2, batch: 761, loss: 0.31290340423583984\n",
      "epoch: 2, batch: 762, loss: 0.4216533303260803\n",
      "epoch: 2, batch: 763, loss: 0.4795916676521301\n",
      "epoch: 2, batch: 764, loss: 0.49710142612457275\n",
      "epoch: 2, batch: 765, loss: 0.44680818915367126\n",
      "epoch: 2, batch: 766, loss: 0.4810200333595276\n",
      "epoch: 2, batch: 767, loss: 0.402251660823822\n",
      "epoch: 2, batch: 768, loss: 0.30604231357574463\n",
      "epoch: 2, batch: 769, loss: 0.26919040083885193\n",
      "epoch: 2, batch: 770, loss: 0.38996294140815735\n",
      "epoch: 2, batch: 771, loss: 0.4731394052505493\n",
      "epoch: 2, batch: 772, loss: 0.2745448648929596\n",
      "epoch: 2, batch: 773, loss: 0.7509693503379822\n",
      "epoch: 2, batch: 774, loss: 0.4357496201992035\n",
      "epoch: 2, batch: 775, loss: 0.29384130239486694\n",
      "epoch: 2, batch: 776, loss: 0.3206847608089447\n",
      "epoch: 2, batch: 777, loss: 0.48609408736228943\n",
      "epoch: 2, batch: 778, loss: 0.3568514585494995\n",
      "epoch: 2, batch: 779, loss: 0.3900005519390106\n",
      "epoch: 2, batch: 780, loss: 0.4494500756263733\n",
      "epoch: 2, batch: 781, loss: 0.46885809302330017\n",
      "epoch: 2, batch: 782, loss: 0.6085700392723083\n",
      "epoch: 2, batch: 783, loss: 0.44503164291381836\n",
      "epoch: 2, batch: 784, loss: 0.550107479095459\n",
      "epoch: 2, batch: 785, loss: 0.5736551880836487\n",
      "epoch: 2, batch: 786, loss: 0.5394781231880188\n",
      "epoch: 2, batch: 787, loss: 0.29146257042884827\n",
      "epoch: 2, batch: 788, loss: 0.47982531785964966\n",
      "epoch: 2, batch: 789, loss: 0.5037646293640137\n",
      "epoch: 2, batch: 790, loss: 0.3991166949272156\n",
      "epoch: 2, batch: 791, loss: 0.3142493665218353\n",
      "epoch: 2, batch: 792, loss: 0.35894885659217834\n",
      "epoch: 2, batch: 793, loss: 0.5048084855079651\n",
      "epoch: 2, batch: 794, loss: 0.5422303676605225\n",
      "epoch: 2, batch: 795, loss: 0.32010477781295776\n",
      "epoch: 2, batch: 796, loss: 0.5127924084663391\n",
      "epoch: 2, batch: 797, loss: 0.39975565671920776\n",
      "epoch: 2, batch: 798, loss: 0.5034546256065369\n",
      "epoch: 2, batch: 799, loss: 0.4446345269680023\n",
      "epoch: 2, batch: 800, loss: 0.5268005728721619\n",
      "epoch: 2, batch: 801, loss: 0.24700403213500977\n",
      "epoch: 2, batch: 802, loss: 0.32732346653938293\n",
      "epoch: 2, batch: 803, loss: 0.47115135192871094\n",
      "epoch: 2, batch: 804, loss: 0.40813881158828735\n",
      "epoch: 2, batch: 805, loss: 0.38460853695869446\n",
      "epoch: 2, batch: 806, loss: 0.48680955171585083\n",
      "epoch: 2, batch: 807, loss: 0.3206239342689514\n",
      "epoch: 2, batch: 808, loss: 0.6685141921043396\n",
      "epoch: 2, batch: 809, loss: 0.30782222747802734\n",
      "epoch: 2, batch: 810, loss: 0.49171820282936096\n",
      "epoch: 2, batch: 811, loss: 0.45963016152381897\n",
      "epoch: 2, batch: 812, loss: 0.6391814351081848\n",
      "epoch: 2, batch: 813, loss: 0.41161414980888367\n",
      "epoch: 2, batch: 814, loss: 0.31418144702911377\n",
      "epoch: 2, batch: 815, loss: 0.48181450366973877\n",
      "epoch: 2, batch: 816, loss: 0.2895010709762573\n",
      "epoch: 2, batch: 817, loss: 0.3810417056083679\n",
      "epoch: 2, batch: 818, loss: 0.2432226836681366\n",
      "epoch: 2, batch: 819, loss: 0.4588431715965271\n",
      "epoch: 2, batch: 820, loss: 0.3179977834224701\n",
      "epoch: 2, batch: 821, loss: 0.3251500129699707\n",
      "epoch: 2, batch: 822, loss: 0.30923986434936523\n",
      "epoch: 2, batch: 823, loss: 0.4081750512123108\n",
      "epoch: 2, batch: 824, loss: 0.48280733823776245\n",
      "epoch: 2, batch: 825, loss: 0.2948232591152191\n",
      "epoch: 2, batch: 826, loss: 0.3004368841648102\n",
      "epoch: 2, batch: 827, loss: 0.5654338598251343\n",
      "epoch: 2, batch: 828, loss: 0.3341040313243866\n",
      "epoch: 2, batch: 829, loss: 0.24260050058364868\n",
      "epoch: 2, batch: 830, loss: 0.4614792466163635\n",
      "epoch: 2, batch: 831, loss: 0.36385083198547363\n",
      "epoch: 2, batch: 832, loss: 0.3475237190723419\n",
      "epoch: 2, batch: 833, loss: 0.40956011414527893\n",
      "epoch: 2, batch: 834, loss: 0.31802791357040405\n",
      "epoch: 2, batch: 835, loss: 0.4410787522792816\n",
      "epoch: 2, batch: 836, loss: 0.5168248414993286\n",
      "epoch: 2, batch: 837, loss: 0.38826972246170044\n",
      "epoch: 2, batch: 838, loss: 0.2936302721500397\n",
      "epoch: 2, batch: 839, loss: 0.3559578061103821\n",
      "epoch: 2, batch: 840, loss: 0.4811707139015198\n",
      "epoch: 2, batch: 841, loss: 0.31961873173713684\n",
      "epoch: 2, batch: 842, loss: 0.4672338664531708\n",
      "epoch: 2, batch: 843, loss: 0.5339016914367676\n",
      "epoch: 2, batch: 844, loss: 0.40669572353363037\n",
      "epoch: 2, batch: 845, loss: 0.3428287208080292\n",
      "epoch: 2, batch: 846, loss: 0.3812408149242401\n",
      "epoch: 2, batch: 847, loss: 0.2939456105232239\n",
      "epoch: 2, batch: 848, loss: 0.30590537190437317\n",
      "epoch: 2, batch: 849, loss: 0.4242425560951233\n",
      "epoch: 2, batch: 850, loss: 0.4156719744205475\n",
      "epoch: 2, batch: 851, loss: 0.21350528299808502\n",
      "epoch: 2, batch: 852, loss: 0.4274200201034546\n",
      "epoch: 2, batch: 853, loss: 0.37782159447669983\n",
      "epoch: 2, batch: 854, loss: 0.22360482811927795\n",
      "epoch: 2, batch: 855, loss: 0.3137829601764679\n",
      "epoch: 2, batch: 856, loss: 0.3039928078651428\n",
      "epoch: 2, batch: 857, loss: 0.31068655848503113\n",
      "epoch: 2, batch: 858, loss: 0.6987155079841614\n",
      "epoch: 2, batch: 859, loss: 0.24655529856681824\n",
      "epoch: 2, batch: 860, loss: 0.41115084290504456\n",
      "epoch: 2, batch: 861, loss: 0.4501330852508545\n",
      "epoch: 2, batch: 862, loss: 0.38038626313209534\n",
      "epoch: 2, batch: 863, loss: 0.6004028916358948\n",
      "epoch: 2, batch: 864, loss: 0.4679449498653412\n",
      "epoch: 2, batch: 865, loss: 0.48154425621032715\n",
      "epoch: 2, batch: 866, loss: 0.2638492286205292\n",
      "epoch: 2, batch: 867, loss: 0.36778420209884644\n",
      "epoch: 2, batch: 868, loss: 0.3251851499080658\n",
      "epoch: 2, batch: 869, loss: 0.30216899514198303\n",
      "epoch: 2, batch: 870, loss: 0.25386783480644226\n",
      "epoch: 2, batch: 871, loss: 0.5179129242897034\n",
      "epoch: 2, batch: 872, loss: 0.5782117247581482\n",
      "epoch: 2, batch: 873, loss: 0.3539658784866333\n",
      "epoch: 2, batch: 874, loss: 0.43780043721199036\n",
      "epoch: 2, batch: 875, loss: 0.3259786069393158\n",
      "epoch: 2, batch: 876, loss: 0.6083784699440002\n",
      "epoch: 2, batch: 877, loss: 0.36633676290512085\n",
      "epoch: 2, batch: 878, loss: 0.4164804518222809\n",
      "epoch: 2, batch: 879, loss: 0.40785807371139526\n",
      "epoch: 2, batch: 880, loss: 0.4470613896846771\n",
      "epoch: 2, batch: 881, loss: 0.3017658591270447\n",
      "epoch: 2, batch: 882, loss: 0.28619086742401123\n",
      "epoch: 2, batch: 883, loss: 0.2512437403202057\n",
      "epoch: 2, batch: 884, loss: 0.47100451588630676\n",
      "epoch: 2, batch: 885, loss: 0.5197171568870544\n",
      "epoch: 2, batch: 886, loss: 0.18627509474754333\n",
      "epoch: 2, batch: 887, loss: 0.3946671783924103\n",
      "epoch: 2, batch: 888, loss: 0.26954880356788635\n",
      "epoch: 2, batch: 889, loss: 0.47540009021759033\n",
      "epoch: 2, batch: 890, loss: 0.3427399694919586\n",
      "epoch: 2, batch: 891, loss: 0.3664633333683014\n",
      "epoch: 2, batch: 892, loss: 0.4501410722732544\n",
      "epoch: 2, batch: 893, loss: 0.3851522207260132\n",
      "epoch: 2, batch: 894, loss: 0.4469744563102722\n",
      "epoch: 2, batch: 895, loss: 0.5168149471282959\n",
      "epoch: 2, batch: 896, loss: 0.4645926058292389\n",
      "epoch: 2, batch: 897, loss: 0.301057904958725\n",
      "epoch: 2, batch: 898, loss: 0.3267432749271393\n",
      "epoch: 2, batch: 899, loss: 0.3672485649585724\n",
      "epoch: 2, batch: 900, loss: 0.4010383188724518\n",
      "epoch: 2, batch: 901, loss: 0.2596190869808197\n",
      "epoch: 2, batch: 902, loss: 0.4489396810531616\n",
      "epoch: 2, batch: 903, loss: 0.3887069523334503\n",
      "epoch: 2, batch: 904, loss: 0.3788934051990509\n",
      "epoch: 2, batch: 905, loss: 0.393173485994339\n",
      "epoch: 2, batch: 906, loss: 0.15587197244167328\n",
      "epoch: 2, batch: 907, loss: 0.4513101279735565\n",
      "epoch: 2, batch: 908, loss: 0.5211719870567322\n",
      "epoch: 2, batch: 909, loss: 0.3056831955909729\n",
      "epoch: 2, batch: 910, loss: 0.4317634701728821\n",
      "epoch: 2, batch: 911, loss: 0.3207843005657196\n",
      "epoch: 2, batch: 912, loss: 0.47218573093414307\n",
      "epoch: 2, batch: 913, loss: 0.240875706076622\n",
      "epoch: 2, batch: 914, loss: 0.3545520305633545\n",
      "epoch: 2, batch: 915, loss: 0.3548957109451294\n",
      "epoch: 2, batch: 916, loss: 0.4874613285064697\n",
      "epoch: 2, batch: 917, loss: 0.6394175887107849\n",
      "epoch: 2, batch: 918, loss: 0.25691887736320496\n",
      "epoch: 2, batch: 919, loss: 0.5001188516616821\n",
      "epoch: 2, batch: 920, loss: 0.5928041338920593\n",
      "epoch: 2, batch: 921, loss: 0.3312085270881653\n",
      "epoch: 2, batch: 922, loss: 0.3147195875644684\n",
      "epoch: 2, batch: 923, loss: 0.5420582890510559\n",
      "epoch: 2, batch: 924, loss: 0.48866409063339233\n",
      "epoch: 2, batch: 925, loss: 0.5044528841972351\n",
      "epoch: 2, batch: 926, loss: 0.38457661867141724\n",
      "epoch: 2, batch: 927, loss: 0.35668548941612244\n",
      "epoch: 2, batch: 928, loss: 0.4765594005584717\n",
      "epoch: 2, batch: 929, loss: 0.4221017062664032\n",
      "epoch: 2, batch: 930, loss: 0.5047829747200012\n",
      "epoch: 2, batch: 931, loss: 0.533407986164093\n",
      "epoch: 2, batch: 932, loss: 0.4989778995513916\n",
      "epoch: 2, batch: 933, loss: 0.3830520808696747\n",
      "epoch: 2, batch: 934, loss: 0.26905956864356995\n",
      "epoch: 2, batch: 935, loss: 0.30615004897117615\n",
      "epoch: 2, batch: 936, loss: 0.3779618740081787\n",
      "epoch: 2, batch: 937, loss: 0.42908433079719543\n",
      "epoch: 3, batch: 0, loss: 0.39034971594810486\n",
      "epoch: 3, batch: 1, loss: 0.3031330406665802\n",
      "epoch: 3, batch: 2, loss: 0.4033832550048828\n",
      "epoch: 3, batch: 3, loss: 0.5966919660568237\n",
      "epoch: 3, batch: 4, loss: 0.3618769645690918\n",
      "epoch: 3, batch: 5, loss: 0.5273445248603821\n",
      "epoch: 3, batch: 6, loss: 0.2569596469402313\n",
      "epoch: 3, batch: 7, loss: 0.35701802372932434\n",
      "epoch: 3, batch: 8, loss: 0.5592275261878967\n",
      "epoch: 3, batch: 9, loss: 0.36882227659225464\n",
      "epoch: 3, batch: 10, loss: 0.3785207271575928\n",
      "epoch: 3, batch: 11, loss: 0.338870108127594\n",
      "epoch: 3, batch: 12, loss: 0.49791190028190613\n",
      "epoch: 3, batch: 13, loss: 0.38016852736473083\n",
      "epoch: 3, batch: 14, loss: 0.48687276244163513\n",
      "epoch: 3, batch: 15, loss: 0.5069555640220642\n",
      "epoch: 3, batch: 16, loss: 0.3134097158908844\n",
      "epoch: 3, batch: 17, loss: 0.4296441972255707\n",
      "epoch: 3, batch: 18, loss: 0.35324591398239136\n",
      "epoch: 3, batch: 19, loss: 0.39121341705322266\n",
      "epoch: 3, batch: 20, loss: 0.46493396162986755\n",
      "epoch: 3, batch: 21, loss: 0.6377602815628052\n",
      "epoch: 3, batch: 22, loss: 0.29329273104667664\n",
      "epoch: 3, batch: 23, loss: 0.608123242855072\n",
      "epoch: 3, batch: 24, loss: 0.3633560240268707\n",
      "epoch: 3, batch: 25, loss: 0.26541101932525635\n",
      "epoch: 3, batch: 26, loss: 0.305999755859375\n",
      "epoch: 3, batch: 27, loss: 0.23540997505187988\n",
      "epoch: 3, batch: 28, loss: 0.5205498337745667\n",
      "epoch: 3, batch: 29, loss: 0.5438066124916077\n",
      "epoch: 3, batch: 30, loss: 0.4034659266471863\n",
      "epoch: 3, batch: 31, loss: 0.24182820320129395\n",
      "epoch: 3, batch: 32, loss: 0.18468452990055084\n",
      "epoch: 3, batch: 33, loss: 0.41268956661224365\n",
      "epoch: 3, batch: 34, loss: 0.3425001800060272\n",
      "epoch: 3, batch: 35, loss: 0.31476372480392456\n",
      "epoch: 3, batch: 36, loss: 0.42848265171051025\n",
      "epoch: 3, batch: 37, loss: 0.47772812843322754\n",
      "epoch: 3, batch: 38, loss: 0.1758147031068802\n",
      "epoch: 3, batch: 39, loss: 0.27861735224723816\n",
      "epoch: 3, batch: 40, loss: 0.49011316895484924\n",
      "epoch: 3, batch: 41, loss: 0.30247417092323303\n",
      "epoch: 3, batch: 42, loss: 0.3628050982952118\n",
      "epoch: 3, batch: 43, loss: 0.45506757497787476\n",
      "epoch: 3, batch: 44, loss: 0.5539820194244385\n",
      "epoch: 3, batch: 45, loss: 0.30248022079467773\n",
      "epoch: 3, batch: 46, loss: 0.33014029264450073\n",
      "epoch: 3, batch: 47, loss: 0.3844618499279022\n",
      "epoch: 3, batch: 48, loss: 0.15743988752365112\n",
      "epoch: 3, batch: 49, loss: 0.45627889037132263\n",
      "epoch: 3, batch: 50, loss: 0.22139784693717957\n",
      "epoch: 3, batch: 51, loss: 0.30083969235420227\n",
      "epoch: 3, batch: 52, loss: 0.39184609055519104\n",
      "epoch: 3, batch: 53, loss: 0.2701016962528229\n",
      "epoch: 3, batch: 54, loss: 0.641318678855896\n",
      "epoch: 3, batch: 55, loss: 0.29770269989967346\n",
      "epoch: 3, batch: 56, loss: 0.3992512822151184\n",
      "epoch: 3, batch: 57, loss: 0.4048592150211334\n",
      "epoch: 3, batch: 58, loss: 0.32459500432014465\n",
      "epoch: 3, batch: 59, loss: 0.3676602244377136\n",
      "epoch: 3, batch: 60, loss: 0.5291883945465088\n",
      "epoch: 3, batch: 61, loss: 0.5306589603424072\n",
      "epoch: 3, batch: 62, loss: 0.3226677477359772\n",
      "epoch: 3, batch: 63, loss: 0.33891454339027405\n",
      "epoch: 3, batch: 64, loss: 0.31705164909362793\n",
      "epoch: 3, batch: 65, loss: 0.5279668569564819\n",
      "epoch: 3, batch: 66, loss: 0.44801002740859985\n",
      "epoch: 3, batch: 67, loss: 0.22284835577011108\n",
      "epoch: 3, batch: 68, loss: 0.33831021189689636\n",
      "epoch: 3, batch: 69, loss: 0.4062246084213257\n",
      "epoch: 3, batch: 70, loss: 0.6026015281677246\n",
      "epoch: 3, batch: 71, loss: 0.47520047426223755\n",
      "epoch: 3, batch: 72, loss: 0.5446410179138184\n",
      "epoch: 3, batch: 73, loss: 0.46425706148147583\n",
      "epoch: 3, batch: 74, loss: 0.31019341945648193\n",
      "epoch: 3, batch: 75, loss: 0.2298584133386612\n",
      "epoch: 3, batch: 76, loss: 0.35305529832839966\n",
      "epoch: 3, batch: 77, loss: 0.3103707432746887\n",
      "epoch: 3, batch: 78, loss: 0.42792096734046936\n",
      "epoch: 3, batch: 79, loss: 0.35928139090538025\n",
      "epoch: 3, batch: 80, loss: 0.36916032433509827\n",
      "epoch: 3, batch: 81, loss: 0.2209877222776413\n",
      "epoch: 3, batch: 82, loss: 0.5390173196792603\n",
      "epoch: 3, batch: 83, loss: 0.36822745203971863\n",
      "epoch: 3, batch: 84, loss: 0.47365182638168335\n",
      "epoch: 3, batch: 85, loss: 0.43007776141166687\n",
      "epoch: 3, batch: 86, loss: 0.33574482798576355\n",
      "epoch: 3, batch: 87, loss: 0.22847257554531097\n",
      "epoch: 3, batch: 88, loss: 0.30139288306236267\n",
      "epoch: 3, batch: 89, loss: 0.2639274299144745\n",
      "epoch: 3, batch: 90, loss: 0.45043492317199707\n",
      "epoch: 3, batch: 91, loss: 0.4147273898124695\n",
      "epoch: 3, batch: 92, loss: 0.44278284907341003\n",
      "epoch: 3, batch: 93, loss: 0.3471800982952118\n",
      "epoch: 3, batch: 94, loss: 0.25067952275276184\n",
      "epoch: 3, batch: 95, loss: 0.37062492966651917\n",
      "epoch: 3, batch: 96, loss: 0.44475987553596497\n",
      "epoch: 3, batch: 97, loss: 0.48802006244659424\n",
      "epoch: 3, batch: 98, loss: 0.3480348289012909\n",
      "epoch: 3, batch: 99, loss: 0.34005221724510193\n",
      "epoch: 3, batch: 100, loss: 0.30870020389556885\n",
      "epoch: 3, batch: 101, loss: 0.510071337223053\n",
      "epoch: 3, batch: 102, loss: 0.6043912172317505\n",
      "epoch: 3, batch: 103, loss: 0.5270854234695435\n",
      "epoch: 3, batch: 104, loss: 0.2462272047996521\n",
      "epoch: 3, batch: 105, loss: 0.48701009154319763\n",
      "epoch: 3, batch: 106, loss: 0.29184260964393616\n",
      "epoch: 3, batch: 107, loss: 0.24842019379138947\n",
      "epoch: 3, batch: 108, loss: 0.6005651354789734\n",
      "epoch: 3, batch: 109, loss: 0.43239983916282654\n",
      "epoch: 3, batch: 110, loss: 0.45835569500923157\n",
      "epoch: 3, batch: 111, loss: 0.2741696536540985\n",
      "epoch: 3, batch: 112, loss: 0.3572262227535248\n",
      "epoch: 3, batch: 113, loss: 0.5736731886863708\n",
      "epoch: 3, batch: 114, loss: 0.6300312280654907\n",
      "epoch: 3, batch: 115, loss: 0.2830311357975006\n",
      "epoch: 3, batch: 116, loss: 0.5560641288757324\n",
      "epoch: 3, batch: 117, loss: 0.28258952498435974\n",
      "epoch: 3, batch: 118, loss: 0.3537841737270355\n",
      "epoch: 3, batch: 119, loss: 0.3227599561214447\n",
      "epoch: 3, batch: 120, loss: 0.38124993443489075\n",
      "epoch: 3, batch: 121, loss: 0.2804369330406189\n",
      "epoch: 3, batch: 122, loss: 0.46331509947776794\n",
      "epoch: 3, batch: 123, loss: 0.4085676968097687\n",
      "epoch: 3, batch: 124, loss: 0.26794689893722534\n",
      "epoch: 3, batch: 125, loss: 0.3472657799720764\n",
      "epoch: 3, batch: 126, loss: 0.34884220361709595\n",
      "epoch: 3, batch: 127, loss: 0.24993418157100677\n",
      "epoch: 3, batch: 128, loss: 0.2582791745662689\n",
      "epoch: 3, batch: 129, loss: 0.4715442359447479\n",
      "epoch: 3, batch: 130, loss: 0.36643579602241516\n",
      "epoch: 3, batch: 131, loss: 0.4421859085559845\n",
      "epoch: 3, batch: 132, loss: 0.3513104319572449\n",
      "epoch: 3, batch: 133, loss: 0.3208860456943512\n",
      "epoch: 3, batch: 134, loss: 0.22756585478782654\n",
      "epoch: 3, batch: 135, loss: 0.3583862781524658\n",
      "epoch: 3, batch: 136, loss: 0.4091367721557617\n",
      "epoch: 3, batch: 137, loss: 0.6337522268295288\n",
      "epoch: 3, batch: 138, loss: 0.26089394092559814\n",
      "epoch: 3, batch: 139, loss: 0.27144744992256165\n",
      "epoch: 3, batch: 140, loss: 0.32551461458206177\n",
      "epoch: 3, batch: 141, loss: 0.4707048237323761\n",
      "epoch: 3, batch: 142, loss: 0.5789195895195007\n",
      "epoch: 3, batch: 143, loss: 0.5555068850517273\n",
      "epoch: 3, batch: 144, loss: 0.31987276673316956\n",
      "epoch: 3, batch: 145, loss: 0.2797965109348297\n",
      "epoch: 3, batch: 146, loss: 0.28513777256011963\n",
      "epoch: 3, batch: 147, loss: 0.49735063314437866\n",
      "epoch: 3, batch: 148, loss: 0.42936718463897705\n",
      "epoch: 3, batch: 149, loss: 0.357234388589859\n",
      "epoch: 3, batch: 150, loss: 0.4486646056175232\n",
      "epoch: 3, batch: 151, loss: 0.462877482175827\n",
      "epoch: 3, batch: 152, loss: 0.252507746219635\n",
      "epoch: 3, batch: 153, loss: 0.329751193523407\n",
      "epoch: 3, batch: 154, loss: 0.41646358370780945\n",
      "epoch: 3, batch: 155, loss: 0.38500362634658813\n",
      "epoch: 3, batch: 156, loss: 0.2773200273513794\n",
      "epoch: 3, batch: 157, loss: 0.3794693052768707\n",
      "epoch: 3, batch: 158, loss: 0.4306234121322632\n",
      "epoch: 3, batch: 159, loss: 0.25074249505996704\n",
      "epoch: 3, batch: 160, loss: 0.36102670431137085\n",
      "epoch: 3, batch: 161, loss: 0.39785391092300415\n",
      "epoch: 3, batch: 162, loss: 0.44557899236679077\n",
      "epoch: 3, batch: 163, loss: 0.25988221168518066\n",
      "epoch: 3, batch: 164, loss: 0.4622824192047119\n",
      "epoch: 3, batch: 165, loss: 0.46021416783332825\n",
      "epoch: 3, batch: 166, loss: 0.2595261335372925\n",
      "epoch: 3, batch: 167, loss: 0.25435152649879456\n",
      "epoch: 3, batch: 168, loss: 0.32989802956581116\n",
      "epoch: 3, batch: 169, loss: 0.386197954416275\n",
      "epoch: 3, batch: 170, loss: 0.5884818434715271\n",
      "epoch: 3, batch: 171, loss: 0.43815162777900696\n",
      "epoch: 3, batch: 172, loss: 0.21428757905960083\n",
      "epoch: 3, batch: 173, loss: 0.3419220745563507\n",
      "epoch: 3, batch: 174, loss: 0.27717751264572144\n",
      "epoch: 3, batch: 175, loss: 0.5127203464508057\n",
      "epoch: 3, batch: 176, loss: 0.2363399714231491\n",
      "epoch: 3, batch: 177, loss: 0.46894338726997375\n",
      "epoch: 3, batch: 178, loss: 0.5016270875930786\n",
      "epoch: 3, batch: 179, loss: 0.4884689152240753\n",
      "epoch: 3, batch: 180, loss: 0.243832528591156\n",
      "epoch: 3, batch: 181, loss: 0.28043854236602783\n",
      "epoch: 3, batch: 182, loss: 0.26079151034355164\n",
      "epoch: 3, batch: 183, loss: 0.2748889923095703\n",
      "epoch: 3, batch: 184, loss: 0.39240217208862305\n",
      "epoch: 3, batch: 185, loss: 0.44258156418800354\n",
      "epoch: 3, batch: 186, loss: 0.31141749024391174\n",
      "epoch: 3, batch: 187, loss: 0.3900783061981201\n",
      "epoch: 3, batch: 188, loss: 0.2354397177696228\n",
      "epoch: 3, batch: 189, loss: 0.4089622497558594\n",
      "epoch: 3, batch: 190, loss: 0.2674233913421631\n",
      "epoch: 3, batch: 191, loss: 0.3740541636943817\n",
      "epoch: 3, batch: 192, loss: 0.37327203154563904\n",
      "epoch: 3, batch: 193, loss: 0.24896836280822754\n",
      "epoch: 3, batch: 194, loss: 0.4196792542934418\n",
      "epoch: 3, batch: 195, loss: 0.2968035340309143\n",
      "epoch: 3, batch: 196, loss: 0.4953215420246124\n",
      "epoch: 3, batch: 197, loss: 0.20530429482460022\n",
      "epoch: 3, batch: 198, loss: 0.29427382349967957\n",
      "epoch: 3, batch: 199, loss: 0.3313428461551666\n",
      "epoch: 3, batch: 200, loss: 0.37784695625305176\n",
      "epoch: 3, batch: 201, loss: 0.3807171881198883\n",
      "epoch: 3, batch: 202, loss: 0.31217077374458313\n",
      "epoch: 3, batch: 203, loss: 0.34336453676223755\n",
      "epoch: 3, batch: 204, loss: 0.27857711911201477\n",
      "epoch: 3, batch: 205, loss: 0.46542248129844666\n",
      "epoch: 3, batch: 206, loss: 0.37498462200164795\n",
      "epoch: 3, batch: 207, loss: 0.3662179112434387\n",
      "epoch: 3, batch: 208, loss: 0.5058493614196777\n",
      "epoch: 3, batch: 209, loss: 0.3221951127052307\n",
      "epoch: 3, batch: 210, loss: 0.34718048572540283\n",
      "epoch: 3, batch: 211, loss: 0.19549182057380676\n",
      "epoch: 3, batch: 212, loss: 0.31410402059555054\n",
      "epoch: 3, batch: 213, loss: 0.22868295013904572\n",
      "epoch: 3, batch: 214, loss: 0.4697391390800476\n",
      "epoch: 3, batch: 215, loss: 0.2486715316772461\n",
      "epoch: 3, batch: 216, loss: 0.3972643315792084\n",
      "epoch: 3, batch: 217, loss: 0.4230143129825592\n",
      "epoch: 3, batch: 218, loss: 0.43486058712005615\n",
      "epoch: 3, batch: 219, loss: 0.26753151416778564\n",
      "epoch: 3, batch: 220, loss: 0.2956509292125702\n",
      "epoch: 3, batch: 221, loss: 0.6306505799293518\n",
      "epoch: 3, batch: 222, loss: 0.3069561719894409\n",
      "epoch: 3, batch: 223, loss: 0.43367624282836914\n",
      "epoch: 3, batch: 224, loss: 0.3351385295391083\n",
      "epoch: 3, batch: 225, loss: 0.37842854857444763\n",
      "epoch: 3, batch: 226, loss: 0.22845648229122162\n",
      "epoch: 3, batch: 227, loss: 0.47312137484550476\n",
      "epoch: 3, batch: 228, loss: 0.2886940836906433\n",
      "epoch: 3, batch: 229, loss: 0.45711851119995117\n",
      "epoch: 3, batch: 230, loss: 0.2876790165901184\n",
      "epoch: 3, batch: 231, loss: 0.4335109293460846\n",
      "epoch: 3, batch: 232, loss: 0.2872829735279083\n",
      "epoch: 3, batch: 233, loss: 0.3597564995288849\n",
      "epoch: 3, batch: 234, loss: 0.255971759557724\n",
      "epoch: 3, batch: 235, loss: 0.34569546580314636\n",
      "epoch: 3, batch: 236, loss: 0.48252660036087036\n",
      "epoch: 3, batch: 237, loss: 0.36105719208717346\n",
      "epoch: 3, batch: 238, loss: 0.6494301557540894\n",
      "epoch: 3, batch: 239, loss: 0.31508615612983704\n",
      "epoch: 3, batch: 240, loss: 0.47205621004104614\n",
      "epoch: 3, batch: 241, loss: 0.2829586863517761\n",
      "epoch: 3, batch: 242, loss: 0.3398923873901367\n",
      "epoch: 3, batch: 243, loss: 0.5808520913124084\n",
      "epoch: 3, batch: 244, loss: 0.5001184344291687\n",
      "epoch: 3, batch: 245, loss: 0.5171892046928406\n",
      "epoch: 3, batch: 246, loss: 0.3250669240951538\n",
      "epoch: 3, batch: 247, loss: 0.32099035382270813\n",
      "epoch: 3, batch: 248, loss: 0.2824009656906128\n",
      "epoch: 3, batch: 249, loss: 0.4219660460948944\n",
      "epoch: 3, batch: 250, loss: 0.35989809036254883\n",
      "epoch: 3, batch: 251, loss: 0.46406090259552\n",
      "epoch: 3, batch: 252, loss: 0.4368453025817871\n",
      "epoch: 3, batch: 253, loss: 0.34972867369651794\n",
      "epoch: 3, batch: 254, loss: 0.4836033284664154\n",
      "epoch: 3, batch: 255, loss: 0.3540165424346924\n",
      "epoch: 3, batch: 256, loss: 0.5023567080497742\n",
      "epoch: 3, batch: 257, loss: 0.44202664494514465\n",
      "epoch: 3, batch: 258, loss: 0.31543388962745667\n",
      "epoch: 3, batch: 259, loss: 0.4862728416919708\n",
      "epoch: 3, batch: 260, loss: 0.42671599984169006\n",
      "epoch: 3, batch: 261, loss: 0.34028422832489014\n",
      "epoch: 3, batch: 262, loss: 0.46149513125419617\n",
      "epoch: 3, batch: 263, loss: 0.39542633295059204\n",
      "epoch: 3, batch: 264, loss: 0.532214879989624\n",
      "epoch: 3, batch: 265, loss: 0.4826222062110901\n",
      "epoch: 3, batch: 266, loss: 0.4645140469074249\n",
      "epoch: 3, batch: 267, loss: 0.3807115852832794\n",
      "epoch: 3, batch: 268, loss: 0.3795575797557831\n",
      "epoch: 3, batch: 269, loss: 0.32151955366134644\n",
      "epoch: 3, batch: 270, loss: 0.3794027864933014\n",
      "epoch: 3, batch: 271, loss: 0.47992879152297974\n",
      "epoch: 3, batch: 272, loss: 0.3818490505218506\n",
      "epoch: 3, batch: 273, loss: 0.4384481608867645\n",
      "epoch: 3, batch: 274, loss: 0.2671060860157013\n",
      "epoch: 3, batch: 275, loss: 0.349701464176178\n",
      "epoch: 3, batch: 276, loss: 0.2535497844219208\n",
      "epoch: 3, batch: 277, loss: 0.3248056173324585\n",
      "epoch: 3, batch: 278, loss: 0.21475383639335632\n",
      "epoch: 3, batch: 279, loss: 0.35292401909828186\n",
      "epoch: 3, batch: 280, loss: 0.27767932415008545\n",
      "epoch: 3, batch: 281, loss: 0.4293745756149292\n",
      "epoch: 3, batch: 282, loss: 0.32763591408729553\n",
      "epoch: 3, batch: 283, loss: 0.33912670612335205\n",
      "epoch: 3, batch: 284, loss: 0.44722458720207214\n",
      "epoch: 3, batch: 285, loss: 0.46581435203552246\n",
      "epoch: 3, batch: 286, loss: 0.40390095114707947\n",
      "epoch: 3, batch: 287, loss: 0.4138859212398529\n",
      "epoch: 3, batch: 288, loss: 0.33001670241355896\n",
      "epoch: 3, batch: 289, loss: 0.5380008220672607\n",
      "epoch: 3, batch: 290, loss: 0.43741607666015625\n",
      "epoch: 3, batch: 291, loss: 0.3897389769554138\n",
      "epoch: 3, batch: 292, loss: 0.44857287406921387\n",
      "epoch: 3, batch: 293, loss: 0.4084513187408447\n",
      "epoch: 3, batch: 294, loss: 0.23406429588794708\n",
      "epoch: 3, batch: 295, loss: 0.20561754703521729\n",
      "epoch: 3, batch: 296, loss: 0.3026086688041687\n",
      "epoch: 3, batch: 297, loss: 0.4393664598464966\n",
      "epoch: 3, batch: 298, loss: 0.5917623043060303\n",
      "epoch: 3, batch: 299, loss: 0.5327897667884827\n",
      "epoch: 3, batch: 300, loss: 0.4630827009677887\n",
      "epoch: 3, batch: 301, loss: 0.26115885376930237\n",
      "epoch: 3, batch: 302, loss: 0.39995744824409485\n",
      "epoch: 3, batch: 303, loss: 0.2776946425437927\n",
      "epoch: 3, batch: 304, loss: 0.24345651268959045\n",
      "epoch: 3, batch: 305, loss: 0.43561694025993347\n",
      "epoch: 3, batch: 306, loss: 0.42342036962509155\n",
      "epoch: 3, batch: 307, loss: 0.44047796726226807\n",
      "epoch: 3, batch: 308, loss: 0.47094106674194336\n",
      "epoch: 3, batch: 309, loss: 0.41055047512054443\n",
      "epoch: 3, batch: 310, loss: 0.2767444849014282\n",
      "epoch: 3, batch: 311, loss: 0.5138071179389954\n",
      "epoch: 3, batch: 312, loss: 0.4681757092475891\n",
      "epoch: 3, batch: 313, loss: 0.23737579584121704\n",
      "epoch: 3, batch: 314, loss: 0.32534682750701904\n",
      "epoch: 3, batch: 315, loss: 0.4341088533401489\n",
      "epoch: 3, batch: 316, loss: 0.2373952567577362\n",
      "epoch: 3, batch: 317, loss: 0.2933390140533447\n",
      "epoch: 3, batch: 318, loss: 0.49805399775505066\n",
      "epoch: 3, batch: 319, loss: 0.2940073013305664\n",
      "epoch: 3, batch: 320, loss: 0.3108077943325043\n",
      "epoch: 3, batch: 321, loss: 0.5028637647628784\n",
      "epoch: 3, batch: 322, loss: 0.4357197880744934\n",
      "epoch: 3, batch: 323, loss: 0.33192145824432373\n",
      "epoch: 3, batch: 324, loss: 0.2675236165523529\n",
      "epoch: 3, batch: 325, loss: 0.25323331356048584\n",
      "epoch: 3, batch: 326, loss: 0.41342654824256897\n",
      "epoch: 3, batch: 327, loss: 0.4865538775920868\n",
      "epoch: 3, batch: 328, loss: 0.351972758769989\n",
      "epoch: 3, batch: 329, loss: 0.3624260127544403\n",
      "epoch: 3, batch: 330, loss: 0.4245075583457947\n",
      "epoch: 3, batch: 331, loss: 0.3424537777900696\n",
      "epoch: 3, batch: 332, loss: 0.2566753923892975\n",
      "epoch: 3, batch: 333, loss: 0.2731212079524994\n",
      "epoch: 3, batch: 334, loss: 0.3278954029083252\n",
      "epoch: 3, batch: 335, loss: 0.2593996822834015\n",
      "epoch: 3, batch: 336, loss: 0.22011932730674744\n",
      "epoch: 3, batch: 337, loss: 0.27986568212509155\n",
      "epoch: 3, batch: 338, loss: 0.40239694714546204\n",
      "epoch: 3, batch: 339, loss: 0.2720225751399994\n",
      "epoch: 3, batch: 340, loss: 0.3409665822982788\n",
      "epoch: 3, batch: 341, loss: 0.39614084362983704\n",
      "epoch: 3, batch: 342, loss: 0.3283049762248993\n",
      "epoch: 3, batch: 343, loss: 0.3692649006843567\n",
      "epoch: 3, batch: 344, loss: 0.4298947751522064\n",
      "epoch: 3, batch: 345, loss: 0.33695271611213684\n",
      "epoch: 3, batch: 346, loss: 0.4523228704929352\n",
      "epoch: 3, batch: 347, loss: 0.5779436230659485\n",
      "epoch: 3, batch: 348, loss: 0.4633413255214691\n",
      "epoch: 3, batch: 349, loss: 0.42277106642723083\n",
      "epoch: 3, batch: 350, loss: 0.26372185349464417\n",
      "epoch: 3, batch: 351, loss: 0.31828948855400085\n",
      "epoch: 3, batch: 352, loss: 0.28577518463134766\n",
      "epoch: 3, batch: 353, loss: 0.40592673420906067\n",
      "epoch: 3, batch: 354, loss: 0.6560996770858765\n",
      "epoch: 3, batch: 355, loss: 0.39891842007637024\n",
      "epoch: 3, batch: 356, loss: 0.3257547616958618\n",
      "epoch: 3, batch: 357, loss: 0.4541740417480469\n",
      "epoch: 3, batch: 358, loss: 0.3825335204601288\n",
      "epoch: 3, batch: 359, loss: 0.3371327817440033\n",
      "epoch: 3, batch: 360, loss: 0.2833852767944336\n",
      "epoch: 3, batch: 361, loss: 0.29399165511131287\n",
      "epoch: 3, batch: 362, loss: 0.2855631709098816\n",
      "epoch: 3, batch: 363, loss: 0.323846697807312\n",
      "epoch: 3, batch: 364, loss: 0.3931207060813904\n",
      "epoch: 3, batch: 365, loss: 0.41511648893356323\n",
      "epoch: 3, batch: 366, loss: 0.4536994397640228\n",
      "epoch: 3, batch: 367, loss: 0.6340160369873047\n",
      "epoch: 3, batch: 368, loss: 0.291252464056015\n",
      "epoch: 3, batch: 369, loss: 0.4131397604942322\n",
      "epoch: 3, batch: 370, loss: 0.269119530916214\n",
      "epoch: 3, batch: 371, loss: 0.3604895770549774\n",
      "epoch: 3, batch: 372, loss: 0.38082584738731384\n",
      "epoch: 3, batch: 373, loss: 0.2888930141925812\n",
      "epoch: 3, batch: 374, loss: 0.3523053526878357\n",
      "epoch: 3, batch: 375, loss: 0.23933473229408264\n",
      "epoch: 3, batch: 376, loss: 0.35751017928123474\n",
      "epoch: 3, batch: 377, loss: 0.30095240473747253\n",
      "epoch: 3, batch: 378, loss: 0.25288522243499756\n",
      "epoch: 3, batch: 379, loss: 0.49135372042655945\n",
      "epoch: 3, batch: 380, loss: 0.3071373701095581\n",
      "epoch: 3, batch: 381, loss: 0.2995914816856384\n",
      "epoch: 3, batch: 382, loss: 0.2908173203468323\n",
      "epoch: 3, batch: 383, loss: 0.3126188814640045\n",
      "epoch: 3, batch: 384, loss: 0.49372848868370056\n",
      "epoch: 3, batch: 385, loss: 0.5376097559928894\n",
      "epoch: 3, batch: 386, loss: 0.24731074273586273\n",
      "epoch: 3, batch: 387, loss: 0.22379496693611145\n",
      "epoch: 3, batch: 388, loss: 0.2855343520641327\n",
      "epoch: 3, batch: 389, loss: 0.48552826046943665\n",
      "epoch: 3, batch: 390, loss: 0.37817081809043884\n",
      "epoch: 3, batch: 391, loss: 0.41315630078315735\n",
      "epoch: 3, batch: 392, loss: 0.3628740608692169\n",
      "epoch: 3, batch: 393, loss: 0.2515677809715271\n",
      "epoch: 3, batch: 394, loss: 0.3292258679866791\n",
      "epoch: 3, batch: 395, loss: 0.33123916387557983\n",
      "epoch: 3, batch: 396, loss: 0.2331494688987732\n",
      "epoch: 3, batch: 397, loss: 0.3422607481479645\n",
      "epoch: 3, batch: 398, loss: 0.2994377017021179\n",
      "epoch: 3, batch: 399, loss: 0.40486350655555725\n",
      "epoch: 3, batch: 400, loss: 0.31599459052085876\n",
      "epoch: 3, batch: 401, loss: 0.6136540174484253\n",
      "epoch: 3, batch: 402, loss: 0.44970670342445374\n",
      "epoch: 3, batch: 403, loss: 0.3114559054374695\n",
      "epoch: 3, batch: 404, loss: 0.3086443841457367\n",
      "epoch: 3, batch: 405, loss: 0.3482377827167511\n",
      "epoch: 3, batch: 406, loss: 0.3338083326816559\n",
      "epoch: 3, batch: 407, loss: 0.2975313365459442\n",
      "epoch: 3, batch: 408, loss: 0.5341278910636902\n",
      "epoch: 3, batch: 409, loss: 0.4137020409107208\n",
      "epoch: 3, batch: 410, loss: 0.2733638286590576\n",
      "epoch: 3, batch: 411, loss: 0.42694348096847534\n",
      "epoch: 3, batch: 412, loss: 0.4602603614330292\n",
      "epoch: 3, batch: 413, loss: 0.327222615480423\n",
      "epoch: 3, batch: 414, loss: 0.45477262139320374\n",
      "epoch: 3, batch: 415, loss: 0.4084618091583252\n",
      "epoch: 3, batch: 416, loss: 0.3226078450679779\n",
      "epoch: 3, batch: 417, loss: 0.2531835734844208\n",
      "epoch: 3, batch: 418, loss: 0.3118552565574646\n",
      "epoch: 3, batch: 419, loss: 0.3502022922039032\n",
      "epoch: 3, batch: 420, loss: 0.422381192445755\n",
      "epoch: 3, batch: 421, loss: 0.45234328508377075\n",
      "epoch: 3, batch: 422, loss: 0.32343727350234985\n",
      "epoch: 3, batch: 423, loss: 0.4232875406742096\n",
      "epoch: 3, batch: 424, loss: 0.281209796667099\n",
      "epoch: 3, batch: 425, loss: 0.24403253197669983\n",
      "epoch: 3, batch: 426, loss: 0.30213358998298645\n",
      "epoch: 3, batch: 427, loss: 0.34629127383232117\n",
      "epoch: 3, batch: 428, loss: 0.3868575096130371\n",
      "epoch: 3, batch: 429, loss: 0.33879828453063965\n",
      "epoch: 3, batch: 430, loss: 0.6035816669464111\n",
      "epoch: 3, batch: 431, loss: 0.30082640051841736\n",
      "epoch: 3, batch: 432, loss: 0.6016094088554382\n",
      "epoch: 3, batch: 433, loss: 0.4344245493412018\n",
      "epoch: 3, batch: 434, loss: 0.2387070506811142\n",
      "epoch: 3, batch: 435, loss: 0.4368728995323181\n",
      "epoch: 3, batch: 436, loss: 0.3605540692806244\n",
      "epoch: 3, batch: 437, loss: 0.22000567615032196\n",
      "epoch: 3, batch: 438, loss: 0.32012850046157837\n",
      "epoch: 3, batch: 439, loss: 0.3299218714237213\n",
      "epoch: 3, batch: 440, loss: 0.4421328902244568\n",
      "epoch: 3, batch: 441, loss: 0.4244675934314728\n",
      "epoch: 3, batch: 442, loss: 0.396240234375\n",
      "epoch: 3, batch: 443, loss: 0.4931075870990753\n",
      "epoch: 3, batch: 444, loss: 0.6640309691429138\n",
      "epoch: 3, batch: 445, loss: 0.5032057166099548\n",
      "epoch: 3, batch: 446, loss: 0.5022278428077698\n",
      "epoch: 3, batch: 447, loss: 0.5122125744819641\n",
      "epoch: 3, batch: 448, loss: 0.5021136999130249\n",
      "epoch: 3, batch: 449, loss: 0.43981730937957764\n",
      "epoch: 3, batch: 450, loss: 0.6039626002311707\n",
      "epoch: 3, batch: 451, loss: 0.3227754831314087\n",
      "epoch: 3, batch: 452, loss: 0.2713654935359955\n",
      "epoch: 3, batch: 453, loss: 0.3260478973388672\n",
      "epoch: 3, batch: 454, loss: 0.252206027507782\n",
      "epoch: 3, batch: 455, loss: 0.3015086054801941\n",
      "epoch: 3, batch: 456, loss: 0.325837105512619\n",
      "epoch: 3, batch: 457, loss: 0.3895372152328491\n",
      "epoch: 3, batch: 458, loss: 0.43554428219795227\n",
      "epoch: 3, batch: 459, loss: 0.3669079542160034\n",
      "epoch: 3, batch: 460, loss: 0.6321870684623718\n",
      "epoch: 3, batch: 461, loss: 0.25665953755378723\n",
      "epoch: 3, batch: 462, loss: 0.2029767483472824\n",
      "epoch: 3, batch: 463, loss: 0.3240814208984375\n",
      "epoch: 3, batch: 464, loss: 0.34474894404411316\n",
      "epoch: 3, batch: 465, loss: 0.22880545258522034\n",
      "epoch: 3, batch: 466, loss: 0.1450488567352295\n",
      "epoch: 3, batch: 467, loss: 0.3248995244503021\n",
      "epoch: 3, batch: 468, loss: 0.3365289866924286\n",
      "epoch: 3, batch: 469, loss: 0.36149248480796814\n",
      "epoch: 3, batch: 470, loss: 0.3006436824798584\n",
      "epoch: 3, batch: 471, loss: 0.30976295471191406\n",
      "epoch: 3, batch: 472, loss: 0.4083784520626068\n",
      "epoch: 3, batch: 473, loss: 0.39246535301208496\n",
      "epoch: 3, batch: 474, loss: 0.22811099886894226\n",
      "epoch: 3, batch: 475, loss: 0.3375379145145416\n",
      "epoch: 3, batch: 476, loss: 0.422465056180954\n",
      "epoch: 3, batch: 477, loss: 0.3830357491970062\n",
      "epoch: 3, batch: 478, loss: 0.15691380202770233\n",
      "epoch: 3, batch: 479, loss: 0.5356713533401489\n",
      "epoch: 3, batch: 480, loss: 0.41046595573425293\n",
      "epoch: 3, batch: 481, loss: 0.3102464973926544\n",
      "epoch: 3, batch: 482, loss: 0.4082052707672119\n",
      "epoch: 3, batch: 483, loss: 0.3049945533275604\n",
      "epoch: 3, batch: 484, loss: 0.25631487369537354\n",
      "epoch: 3, batch: 485, loss: 0.3182311952114105\n",
      "epoch: 3, batch: 486, loss: 0.3157535791397095\n",
      "epoch: 3, batch: 487, loss: 0.3223879039287567\n",
      "epoch: 3, batch: 488, loss: 0.4178219437599182\n",
      "epoch: 3, batch: 489, loss: 0.3011053800582886\n",
      "epoch: 3, batch: 490, loss: 0.2982826828956604\n",
      "epoch: 3, batch: 491, loss: 0.20893381536006927\n",
      "epoch: 3, batch: 492, loss: 0.3415817320346832\n",
      "epoch: 3, batch: 493, loss: 0.2975325584411621\n",
      "epoch: 3, batch: 494, loss: 0.470416784286499\n",
      "epoch: 3, batch: 495, loss: 0.28739064931869507\n",
      "epoch: 3, batch: 496, loss: 0.40931788086891174\n",
      "epoch: 3, batch: 497, loss: 0.363472580909729\n",
      "epoch: 3, batch: 498, loss: 0.3889234662055969\n",
      "epoch: 3, batch: 499, loss: 0.4459971487522125\n",
      "epoch: 3, batch: 500, loss: 0.2920795977115631\n",
      "epoch: 3, batch: 501, loss: 0.5094069242477417\n",
      "epoch: 3, batch: 502, loss: 0.6208915710449219\n",
      "epoch: 3, batch: 503, loss: 0.5059060454368591\n",
      "epoch: 3, batch: 504, loss: 0.2978771924972534\n",
      "epoch: 3, batch: 505, loss: 0.4120784103870392\n",
      "epoch: 3, batch: 506, loss: 0.3630124628543854\n",
      "epoch: 3, batch: 507, loss: 0.32835420966148376\n",
      "epoch: 3, batch: 508, loss: 0.27885881066322327\n",
      "epoch: 3, batch: 509, loss: 0.2346000373363495\n",
      "epoch: 3, batch: 510, loss: 0.30255046486854553\n",
      "epoch: 3, batch: 511, loss: 0.5177379250526428\n",
      "epoch: 3, batch: 512, loss: 0.37980541586875916\n",
      "epoch: 3, batch: 513, loss: 0.3237055242061615\n",
      "epoch: 3, batch: 514, loss: 0.4194818437099457\n",
      "epoch: 3, batch: 515, loss: 0.4315594732761383\n",
      "epoch: 3, batch: 516, loss: 0.23721694946289062\n",
      "epoch: 3, batch: 517, loss: 0.20272669196128845\n",
      "epoch: 3, batch: 518, loss: 0.3977515399456024\n",
      "epoch: 3, batch: 519, loss: 0.3161797523498535\n",
      "epoch: 3, batch: 520, loss: 0.4622364640235901\n",
      "epoch: 3, batch: 521, loss: 0.19472770392894745\n",
      "epoch: 3, batch: 522, loss: 0.37423187494277954\n",
      "epoch: 3, batch: 523, loss: 0.42328590154647827\n",
      "epoch: 3, batch: 524, loss: 0.6181442737579346\n",
      "epoch: 3, batch: 525, loss: 0.2752709984779358\n",
      "epoch: 3, batch: 526, loss: 0.3804173171520233\n",
      "epoch: 3, batch: 527, loss: 0.2265027016401291\n",
      "epoch: 3, batch: 528, loss: 0.21224772930145264\n",
      "epoch: 3, batch: 529, loss: 0.33096185326576233\n",
      "epoch: 3, batch: 530, loss: 0.23093536496162415\n",
      "epoch: 3, batch: 531, loss: 0.4061410427093506\n",
      "epoch: 3, batch: 532, loss: 0.5521331429481506\n",
      "epoch: 3, batch: 533, loss: 0.3845418095588684\n",
      "epoch: 3, batch: 534, loss: 0.31962624192237854\n",
      "epoch: 3, batch: 535, loss: 0.23800688982009888\n",
      "epoch: 3, batch: 536, loss: 0.31678834557533264\n",
      "epoch: 3, batch: 537, loss: 0.4227895140647888\n",
      "epoch: 3, batch: 538, loss: 0.5725116729736328\n",
      "epoch: 3, batch: 539, loss: 0.29289698600769043\n",
      "epoch: 3, batch: 540, loss: 0.480336993932724\n",
      "epoch: 3, batch: 541, loss: 0.27959874272346497\n",
      "epoch: 3, batch: 542, loss: 0.4023408889770508\n",
      "epoch: 3, batch: 543, loss: 0.3890823721885681\n",
      "epoch: 3, batch: 544, loss: 0.41363325715065\n",
      "epoch: 3, batch: 545, loss: 0.43682974576950073\n",
      "epoch: 3, batch: 546, loss: 0.5587372183799744\n",
      "epoch: 3, batch: 547, loss: 0.2577427923679352\n",
      "epoch: 3, batch: 548, loss: 0.337829053401947\n",
      "epoch: 3, batch: 549, loss: 0.2967721223831177\n",
      "epoch: 3, batch: 550, loss: 0.44904714822769165\n",
      "epoch: 3, batch: 551, loss: 0.41645264625549316\n",
      "epoch: 3, batch: 552, loss: 0.5104301571846008\n",
      "epoch: 3, batch: 553, loss: 0.34516680240631104\n",
      "epoch: 3, batch: 554, loss: 0.49634304642677307\n",
      "epoch: 3, batch: 555, loss: 0.404239296913147\n",
      "epoch: 3, batch: 556, loss: 0.20815595984458923\n",
      "epoch: 3, batch: 557, loss: 0.1642877757549286\n",
      "epoch: 3, batch: 558, loss: 0.31693848967552185\n",
      "epoch: 3, batch: 559, loss: 0.47059303522109985\n",
      "epoch: 3, batch: 560, loss: 0.26150697469711304\n",
      "epoch: 3, batch: 561, loss: 0.30471712350845337\n",
      "epoch: 3, batch: 562, loss: 0.30796459317207336\n",
      "epoch: 3, batch: 563, loss: 0.4840542674064636\n",
      "epoch: 3, batch: 564, loss: 0.2776634395122528\n",
      "epoch: 3, batch: 565, loss: 0.6500571370124817\n",
      "epoch: 3, batch: 566, loss: 0.38871318101882935\n",
      "epoch: 3, batch: 567, loss: 0.21975389122962952\n",
      "epoch: 3, batch: 568, loss: 0.32508528232574463\n",
      "epoch: 3, batch: 569, loss: 0.17910143733024597\n",
      "epoch: 3, batch: 570, loss: 0.3998890817165375\n",
      "epoch: 3, batch: 571, loss: 0.4385434687137604\n",
      "epoch: 3, batch: 572, loss: 0.2245171070098877\n",
      "epoch: 3, batch: 573, loss: 0.532624363899231\n",
      "epoch: 3, batch: 574, loss: 0.40178370475769043\n",
      "epoch: 3, batch: 575, loss: 0.37385061383247375\n",
      "epoch: 3, batch: 576, loss: 0.3454393148422241\n",
      "epoch: 3, batch: 577, loss: 0.23792695999145508\n",
      "epoch: 3, batch: 578, loss: 0.25374117493629456\n",
      "epoch: 3, batch: 579, loss: 0.3100696802139282\n",
      "epoch: 3, batch: 580, loss: 0.3683539927005768\n",
      "epoch: 3, batch: 581, loss: 0.239393949508667\n",
      "epoch: 3, batch: 582, loss: 0.3104568421840668\n",
      "epoch: 3, batch: 583, loss: 0.29503434896469116\n",
      "epoch: 3, batch: 584, loss: 0.24004638195037842\n",
      "epoch: 3, batch: 585, loss: 0.32659831643104553\n",
      "epoch: 3, batch: 586, loss: 0.36307206749916077\n",
      "epoch: 3, batch: 587, loss: 0.30336061120033264\n",
      "epoch: 3, batch: 588, loss: 0.19965600967407227\n",
      "epoch: 3, batch: 589, loss: 0.3009115755558014\n",
      "epoch: 3, batch: 590, loss: 0.5327348113059998\n",
      "epoch: 3, batch: 591, loss: 0.33194565773010254\n",
      "epoch: 3, batch: 592, loss: 0.3404409885406494\n",
      "epoch: 3, batch: 593, loss: 0.39521196484565735\n",
      "epoch: 3, batch: 594, loss: 0.2932603657245636\n",
      "epoch: 3, batch: 595, loss: 0.33309638500213623\n",
      "epoch: 3, batch: 596, loss: 0.48814159631729126\n",
      "epoch: 3, batch: 597, loss: 0.3428797125816345\n",
      "epoch: 3, batch: 598, loss: 0.6159416437149048\n",
      "epoch: 3, batch: 599, loss: 0.3750385642051697\n",
      "epoch: 3, batch: 600, loss: 0.3885532021522522\n",
      "epoch: 3, batch: 601, loss: 0.2601851522922516\n",
      "epoch: 3, batch: 602, loss: 0.2057902067899704\n",
      "epoch: 3, batch: 603, loss: 0.48674076795578003\n",
      "epoch: 3, batch: 604, loss: 0.2836165130138397\n",
      "epoch: 3, batch: 605, loss: 0.4752960503101349\n",
      "epoch: 3, batch: 606, loss: 0.280463844537735\n",
      "epoch: 3, batch: 607, loss: 0.2246173471212387\n",
      "epoch: 3, batch: 608, loss: 0.33421018719673157\n",
      "epoch: 3, batch: 609, loss: 0.5965510606765747\n",
      "epoch: 3, batch: 610, loss: 0.5300272703170776\n",
      "epoch: 3, batch: 611, loss: 0.3056933581829071\n",
      "epoch: 3, batch: 612, loss: 0.39764177799224854\n",
      "epoch: 3, batch: 613, loss: 0.17923519015312195\n",
      "epoch: 3, batch: 614, loss: 0.3110881745815277\n",
      "epoch: 3, batch: 615, loss: 0.43653661012649536\n",
      "epoch: 3, batch: 616, loss: 0.4156641960144043\n",
      "epoch: 3, batch: 617, loss: 0.4024065434932709\n",
      "epoch: 3, batch: 618, loss: 0.38468804955482483\n",
      "epoch: 3, batch: 619, loss: 0.2891843020915985\n",
      "epoch: 3, batch: 620, loss: 0.454026997089386\n",
      "epoch: 3, batch: 621, loss: 0.46947288513183594\n",
      "epoch: 3, batch: 622, loss: 0.5496765375137329\n",
      "epoch: 3, batch: 623, loss: 0.3636980354785919\n",
      "epoch: 3, batch: 624, loss: 0.5429053902626038\n",
      "epoch: 3, batch: 625, loss: 0.36403632164001465\n",
      "epoch: 3, batch: 626, loss: 0.39268290996551514\n",
      "epoch: 3, batch: 627, loss: 0.39544740319252014\n",
      "epoch: 3, batch: 628, loss: 0.5655069947242737\n",
      "epoch: 3, batch: 629, loss: 0.23087628185749054\n",
      "epoch: 3, batch: 630, loss: 0.27461570501327515\n",
      "epoch: 3, batch: 631, loss: 0.5449386239051819\n",
      "epoch: 3, batch: 632, loss: 0.26204022765159607\n",
      "epoch: 3, batch: 633, loss: 0.4934024512767792\n",
      "epoch: 3, batch: 634, loss: 0.26240968704223633\n",
      "epoch: 3, batch: 635, loss: 0.20264388620853424\n",
      "epoch: 3, batch: 636, loss: 0.3782004117965698\n",
      "epoch: 3, batch: 637, loss: 0.35137656331062317\n",
      "epoch: 3, batch: 638, loss: 0.41751906275749207\n",
      "epoch: 3, batch: 639, loss: 0.4130062162876129\n",
      "epoch: 3, batch: 640, loss: 0.3248950242996216\n",
      "epoch: 3, batch: 641, loss: 0.36835041642189026\n",
      "epoch: 3, batch: 642, loss: 0.17525328695774078\n",
      "epoch: 3, batch: 643, loss: 0.41813504695892334\n",
      "epoch: 3, batch: 644, loss: 0.1764148622751236\n",
      "epoch: 3, batch: 645, loss: 0.3303159475326538\n",
      "epoch: 3, batch: 646, loss: 0.4460943937301636\n",
      "epoch: 3, batch: 647, loss: 0.3872852325439453\n",
      "epoch: 3, batch: 648, loss: 0.3825629651546478\n",
      "epoch: 3, batch: 649, loss: 0.3475259244441986\n",
      "epoch: 3, batch: 650, loss: 0.28305238485336304\n",
      "epoch: 3, batch: 651, loss: 0.3021966814994812\n",
      "epoch: 3, batch: 652, loss: 0.2432078719139099\n",
      "epoch: 3, batch: 653, loss: 0.319236695766449\n",
      "epoch: 3, batch: 654, loss: 0.34281468391418457\n",
      "epoch: 3, batch: 655, loss: 0.2837441563606262\n",
      "epoch: 3, batch: 656, loss: 0.4771088659763336\n",
      "epoch: 3, batch: 657, loss: 0.32495784759521484\n",
      "epoch: 3, batch: 658, loss: 0.4290507435798645\n",
      "epoch: 3, batch: 659, loss: 0.4479341506958008\n",
      "epoch: 3, batch: 660, loss: 0.4153687059879303\n",
      "epoch: 3, batch: 661, loss: 0.3148531913757324\n",
      "epoch: 3, batch: 662, loss: 0.4100651741027832\n",
      "epoch: 3, batch: 663, loss: 0.2778967320919037\n",
      "epoch: 3, batch: 664, loss: 0.31328651309013367\n",
      "epoch: 3, batch: 665, loss: 0.5779672861099243\n",
      "epoch: 3, batch: 666, loss: 0.21329393982887268\n",
      "epoch: 3, batch: 667, loss: 0.3421870768070221\n",
      "epoch: 3, batch: 668, loss: 0.2644491493701935\n",
      "epoch: 3, batch: 669, loss: 0.28263989090919495\n",
      "epoch: 3, batch: 670, loss: 0.44322681427001953\n",
      "epoch: 3, batch: 671, loss: 0.4264812469482422\n",
      "epoch: 3, batch: 672, loss: 0.3916280269622803\n",
      "epoch: 3, batch: 673, loss: 0.25477078557014465\n",
      "epoch: 3, batch: 674, loss: 0.2534050941467285\n",
      "epoch: 3, batch: 675, loss: 0.3088774085044861\n",
      "epoch: 3, batch: 676, loss: 0.2981395721435547\n",
      "epoch: 3, batch: 677, loss: 0.3975486755371094\n",
      "epoch: 3, batch: 678, loss: 0.21613021194934845\n",
      "epoch: 3, batch: 679, loss: 0.3100621998310089\n",
      "epoch: 3, batch: 680, loss: 0.3845840096473694\n",
      "epoch: 3, batch: 681, loss: 0.35571375489234924\n",
      "epoch: 3, batch: 682, loss: 0.45980238914489746\n",
      "epoch: 3, batch: 683, loss: 0.279472291469574\n",
      "epoch: 3, batch: 684, loss: 0.2952955663204193\n",
      "epoch: 3, batch: 685, loss: 0.247876837849617\n",
      "epoch: 3, batch: 686, loss: 0.27502816915512085\n",
      "epoch: 3, batch: 687, loss: 0.4074803590774536\n",
      "epoch: 3, batch: 688, loss: 0.25512316823005676\n",
      "epoch: 3, batch: 689, loss: 0.3836900293827057\n",
      "epoch: 3, batch: 690, loss: 0.3315719664096832\n",
      "epoch: 3, batch: 691, loss: 0.46760693192481995\n",
      "epoch: 3, batch: 692, loss: 0.37126269936561584\n",
      "epoch: 3, batch: 693, loss: 0.35846132040023804\n",
      "epoch: 3, batch: 694, loss: 0.2867428958415985\n",
      "epoch: 3, batch: 695, loss: 0.5091052055358887\n",
      "epoch: 3, batch: 696, loss: 0.3974883258342743\n",
      "epoch: 3, batch: 697, loss: 0.5128874778747559\n",
      "epoch: 3, batch: 698, loss: 0.3659156262874603\n",
      "epoch: 3, batch: 699, loss: 0.39779454469680786\n",
      "epoch: 3, batch: 700, loss: 0.5566219091415405\n",
      "epoch: 3, batch: 701, loss: 0.19745969772338867\n",
      "epoch: 3, batch: 702, loss: 0.27226632833480835\n",
      "epoch: 3, batch: 703, loss: 0.34460288286209106\n",
      "epoch: 3, batch: 704, loss: 0.2779924273490906\n",
      "epoch: 3, batch: 705, loss: 0.2467188835144043\n",
      "epoch: 3, batch: 706, loss: 0.3369581401348114\n",
      "epoch: 3, batch: 707, loss: 0.4159378409385681\n",
      "epoch: 3, batch: 708, loss: 0.34340900182724\n",
      "epoch: 3, batch: 709, loss: 0.3151428699493408\n",
      "epoch: 3, batch: 710, loss: 0.3283351957798004\n",
      "epoch: 3, batch: 711, loss: 0.20291101932525635\n",
      "epoch: 3, batch: 712, loss: 0.24589835107326508\n",
      "epoch: 3, batch: 713, loss: 0.2300807237625122\n",
      "epoch: 3, batch: 714, loss: 0.28717896342277527\n",
      "epoch: 3, batch: 715, loss: 0.3897053897380829\n",
      "epoch: 3, batch: 716, loss: 0.3886103332042694\n",
      "epoch: 3, batch: 717, loss: 0.25554439425468445\n",
      "epoch: 3, batch: 718, loss: 0.23421938717365265\n",
      "epoch: 3, batch: 719, loss: 0.4460372030735016\n",
      "epoch: 3, batch: 720, loss: 0.21759860217571259\n",
      "epoch: 3, batch: 721, loss: 0.27853646874427795\n",
      "epoch: 3, batch: 722, loss: 0.37899211049079895\n",
      "epoch: 3, batch: 723, loss: 0.26447343826293945\n",
      "epoch: 3, batch: 724, loss: 0.30928006768226624\n",
      "epoch: 3, batch: 725, loss: 0.5100586414337158\n",
      "epoch: 3, batch: 726, loss: 0.4087768495082855\n",
      "epoch: 3, batch: 727, loss: 0.38599279522895813\n",
      "epoch: 3, batch: 728, loss: 0.25827381014823914\n",
      "epoch: 3, batch: 729, loss: 0.38319388031959534\n",
      "epoch: 3, batch: 730, loss: 0.5038735270500183\n",
      "epoch: 3, batch: 731, loss: 0.4236367642879486\n",
      "epoch: 3, batch: 732, loss: 0.3848353326320648\n",
      "epoch: 3, batch: 733, loss: 0.6652751564979553\n",
      "epoch: 3, batch: 734, loss: 0.2268712818622589\n",
      "epoch: 3, batch: 735, loss: 0.2007538080215454\n",
      "epoch: 3, batch: 736, loss: 0.3823055922985077\n",
      "epoch: 3, batch: 737, loss: 0.43439310789108276\n",
      "epoch: 3, batch: 738, loss: 0.18350020051002502\n",
      "epoch: 3, batch: 739, loss: 0.2104373723268509\n",
      "epoch: 3, batch: 740, loss: 0.2820652723312378\n",
      "epoch: 3, batch: 741, loss: 0.2466944009065628\n",
      "epoch: 3, batch: 742, loss: 0.29791921377182007\n",
      "epoch: 3, batch: 743, loss: 0.30176645517349243\n",
      "epoch: 3, batch: 744, loss: 0.4769846796989441\n",
      "epoch: 3, batch: 745, loss: 0.354883074760437\n",
      "epoch: 3, batch: 746, loss: 0.102261483669281\n",
      "epoch: 3, batch: 747, loss: 0.35142114758491516\n",
      "epoch: 3, batch: 748, loss: 0.25740283727645874\n",
      "epoch: 3, batch: 749, loss: 0.13343720138072968\n",
      "epoch: 3, batch: 750, loss: 0.23309291899204254\n",
      "epoch: 3, batch: 751, loss: 0.1453327089548111\n",
      "epoch: 3, batch: 752, loss: 0.22829103469848633\n",
      "epoch: 3, batch: 753, loss: 0.17921875417232513\n",
      "epoch: 3, batch: 754, loss: 0.45098379254341125\n",
      "epoch: 3, batch: 755, loss: 0.28898200392723083\n",
      "epoch: 3, batch: 756, loss: 0.5254520773887634\n",
      "epoch: 3, batch: 757, loss: 0.2984011471271515\n",
      "epoch: 3, batch: 758, loss: 0.38045021891593933\n",
      "epoch: 3, batch: 759, loss: 0.35848140716552734\n",
      "epoch: 3, batch: 760, loss: 0.416352778673172\n",
      "epoch: 3, batch: 761, loss: 0.22506387531757355\n",
      "epoch: 3, batch: 762, loss: 0.33491745591163635\n",
      "epoch: 3, batch: 763, loss: 0.46634015440940857\n",
      "epoch: 3, batch: 764, loss: 0.2898433208465576\n",
      "epoch: 3, batch: 765, loss: 0.49598458409309387\n",
      "epoch: 3, batch: 766, loss: 0.2109009027481079\n",
      "epoch: 3, batch: 767, loss: 0.46891123056411743\n",
      "epoch: 3, batch: 768, loss: 0.4970175623893738\n",
      "epoch: 3, batch: 769, loss: 0.26176074147224426\n",
      "epoch: 3, batch: 770, loss: 0.29160571098327637\n",
      "epoch: 3, batch: 771, loss: 0.48599326610565186\n",
      "epoch: 3, batch: 772, loss: 0.63653963804245\n",
      "epoch: 3, batch: 773, loss: 0.33149582147598267\n",
      "epoch: 3, batch: 774, loss: 0.31551674008369446\n",
      "epoch: 3, batch: 775, loss: 0.30124497413635254\n",
      "epoch: 3, batch: 776, loss: 0.3555200397968292\n",
      "epoch: 3, batch: 777, loss: 0.21754750609397888\n",
      "epoch: 3, batch: 778, loss: 0.47879132628440857\n",
      "epoch: 3, batch: 779, loss: 0.35876500606536865\n",
      "epoch: 3, batch: 780, loss: 0.4905577301979065\n",
      "epoch: 3, batch: 781, loss: 0.36005064845085144\n",
      "epoch: 3, batch: 782, loss: 0.43153202533721924\n",
      "epoch: 3, batch: 783, loss: 0.2541503310203552\n",
      "epoch: 3, batch: 784, loss: 0.2568170726299286\n",
      "epoch: 3, batch: 785, loss: 0.48223525285720825\n",
      "epoch: 3, batch: 786, loss: 0.262320339679718\n",
      "epoch: 3, batch: 787, loss: 0.37284013628959656\n",
      "epoch: 3, batch: 788, loss: 0.5353714227676392\n",
      "epoch: 3, batch: 789, loss: 0.42129361629486084\n",
      "epoch: 3, batch: 790, loss: 0.3205450773239136\n",
      "epoch: 3, batch: 791, loss: 0.41739675402641296\n",
      "epoch: 3, batch: 792, loss: 0.35571157932281494\n",
      "epoch: 3, batch: 793, loss: 0.3536418080329895\n",
      "epoch: 3, batch: 794, loss: 0.2663130462169647\n",
      "epoch: 3, batch: 795, loss: 0.30842357873916626\n",
      "epoch: 3, batch: 796, loss: 0.3320128321647644\n",
      "epoch: 3, batch: 797, loss: 0.41669321060180664\n",
      "epoch: 3, batch: 798, loss: 0.4048522412776947\n",
      "epoch: 3, batch: 799, loss: 0.27281293272972107\n",
      "epoch: 3, batch: 800, loss: 0.3529623746871948\n",
      "epoch: 3, batch: 801, loss: 0.3111041486263275\n",
      "epoch: 3, batch: 802, loss: 0.30361321568489075\n",
      "epoch: 3, batch: 803, loss: 0.34937793016433716\n",
      "epoch: 3, batch: 804, loss: 0.3825807571411133\n",
      "epoch: 3, batch: 805, loss: 0.24560236930847168\n",
      "epoch: 3, batch: 806, loss: 0.26881057024002075\n",
      "epoch: 3, batch: 807, loss: 0.26600953936576843\n",
      "epoch: 3, batch: 808, loss: 0.538669228553772\n",
      "epoch: 3, batch: 809, loss: 0.3696568012237549\n",
      "epoch: 3, batch: 810, loss: 0.517840564250946\n",
      "epoch: 3, batch: 811, loss: 0.22311285138130188\n",
      "epoch: 3, batch: 812, loss: 0.3642541766166687\n",
      "epoch: 3, batch: 813, loss: 0.32645687460899353\n",
      "epoch: 3, batch: 814, loss: 0.35225701332092285\n",
      "epoch: 3, batch: 815, loss: 0.32769131660461426\n",
      "epoch: 3, batch: 816, loss: 0.2931467294692993\n",
      "epoch: 3, batch: 817, loss: 0.34299784898757935\n",
      "epoch: 3, batch: 818, loss: 0.4005584120750427\n",
      "epoch: 3, batch: 819, loss: 0.4427257776260376\n",
      "epoch: 3, batch: 820, loss: 0.39435625076293945\n",
      "epoch: 3, batch: 821, loss: 0.4397556185722351\n",
      "epoch: 3, batch: 822, loss: 0.23338763415813446\n",
      "epoch: 3, batch: 823, loss: 0.2817766070365906\n",
      "epoch: 3, batch: 824, loss: 0.27583637833595276\n",
      "epoch: 3, batch: 825, loss: 0.3119403123855591\n",
      "epoch: 3, batch: 826, loss: 0.38812255859375\n",
      "epoch: 3, batch: 827, loss: 0.2862081527709961\n",
      "epoch: 3, batch: 828, loss: 0.5597718358039856\n",
      "epoch: 3, batch: 829, loss: 0.39779573678970337\n",
      "epoch: 3, batch: 830, loss: 0.3183017373085022\n",
      "epoch: 3, batch: 831, loss: 0.3902280330657959\n",
      "epoch: 3, batch: 832, loss: 0.32391655445098877\n",
      "epoch: 3, batch: 833, loss: 0.23089638352394104\n",
      "epoch: 3, batch: 834, loss: 0.2511431872844696\n",
      "epoch: 3, batch: 835, loss: 0.38235411047935486\n",
      "epoch: 3, batch: 836, loss: 0.1549588441848755\n",
      "epoch: 3, batch: 837, loss: 0.46534761786460876\n",
      "epoch: 3, batch: 838, loss: 0.19511109590530396\n",
      "epoch: 3, batch: 839, loss: 0.46423962712287903\n",
      "epoch: 3, batch: 840, loss: 0.26936599612236023\n",
      "epoch: 3, batch: 841, loss: 0.310641348361969\n",
      "epoch: 3, batch: 842, loss: 0.35392436385154724\n",
      "epoch: 3, batch: 843, loss: 0.3597673177719116\n",
      "epoch: 3, batch: 844, loss: 0.34995993971824646\n",
      "epoch: 3, batch: 845, loss: 0.393288791179657\n",
      "epoch: 3, batch: 846, loss: 0.2772471010684967\n",
      "epoch: 3, batch: 847, loss: 0.2657183110713959\n",
      "epoch: 3, batch: 848, loss: 0.5735238194465637\n",
      "epoch: 3, batch: 849, loss: 0.4425700604915619\n",
      "epoch: 3, batch: 850, loss: 0.39165639877319336\n",
      "epoch: 3, batch: 851, loss: 0.265238881111145\n",
      "epoch: 3, batch: 852, loss: 0.21991676092147827\n",
      "epoch: 3, batch: 853, loss: 0.17656753957271576\n",
      "epoch: 3, batch: 854, loss: 0.4317872226238251\n",
      "epoch: 3, batch: 855, loss: 0.3234880864620209\n",
      "epoch: 3, batch: 856, loss: 0.28735896944999695\n",
      "epoch: 3, batch: 857, loss: 0.3710418939590454\n",
      "epoch: 3, batch: 858, loss: 0.29616421461105347\n",
      "epoch: 3, batch: 859, loss: 0.24822035431861877\n",
      "epoch: 3, batch: 860, loss: 0.6091400384902954\n",
      "epoch: 3, batch: 861, loss: 0.2505813241004944\n",
      "epoch: 3, batch: 862, loss: 0.35915470123291016\n",
      "epoch: 3, batch: 863, loss: 0.6059480905532837\n",
      "epoch: 3, batch: 864, loss: 0.4624296724796295\n",
      "epoch: 3, batch: 865, loss: 0.45979559421539307\n",
      "epoch: 3, batch: 866, loss: 0.25326281785964966\n",
      "epoch: 3, batch: 867, loss: 0.2729809582233429\n",
      "epoch: 3, batch: 868, loss: 0.2857661247253418\n",
      "epoch: 3, batch: 869, loss: 0.2829533815383911\n",
      "epoch: 3, batch: 870, loss: 0.31537142395973206\n",
      "epoch: 3, batch: 871, loss: 0.41432762145996094\n",
      "epoch: 3, batch: 872, loss: 0.44339805841445923\n",
      "epoch: 3, batch: 873, loss: 0.16696970164775848\n",
      "epoch: 3, batch: 874, loss: 0.40911540389060974\n",
      "epoch: 3, batch: 875, loss: 0.26199913024902344\n",
      "epoch: 3, batch: 876, loss: 0.5980890989303589\n",
      "epoch: 3, batch: 877, loss: 0.5122107267379761\n",
      "epoch: 3, batch: 878, loss: 0.41970255970954895\n",
      "epoch: 3, batch: 879, loss: 0.248532235622406\n",
      "epoch: 3, batch: 880, loss: 0.20541518926620483\n",
      "epoch: 3, batch: 881, loss: 0.20693181455135345\n",
      "epoch: 3, batch: 882, loss: 0.3492025136947632\n",
      "epoch: 3, batch: 883, loss: 0.4292013347148895\n",
      "epoch: 3, batch: 884, loss: 0.27360472083091736\n",
      "epoch: 3, batch: 885, loss: 0.15809683501720428\n",
      "epoch: 3, batch: 886, loss: 0.3857257068157196\n",
      "epoch: 3, batch: 887, loss: 0.26375141739845276\n",
      "epoch: 3, batch: 888, loss: 0.18860530853271484\n",
      "epoch: 3, batch: 889, loss: 0.31246769428253174\n",
      "epoch: 3, batch: 890, loss: 0.36704644560813904\n",
      "epoch: 3, batch: 891, loss: 0.2805492579936981\n",
      "epoch: 3, batch: 892, loss: 0.3142527639865875\n",
      "epoch: 3, batch: 893, loss: 0.32572564482688904\n",
      "epoch: 3, batch: 894, loss: 0.29151976108551025\n",
      "epoch: 3, batch: 895, loss: 0.3822110593318939\n",
      "epoch: 3, batch: 896, loss: 0.38582268357276917\n",
      "epoch: 3, batch: 897, loss: 0.377901166677475\n",
      "epoch: 3, batch: 898, loss: 0.3016071021556854\n",
      "epoch: 3, batch: 899, loss: 0.38969773054122925\n",
      "epoch: 3, batch: 900, loss: 0.3388490676879883\n",
      "epoch: 3, batch: 901, loss: 0.29571378231048584\n",
      "epoch: 3, batch: 902, loss: 0.41066479682922363\n",
      "epoch: 3, batch: 903, loss: 0.4830924868583679\n",
      "epoch: 3, batch: 904, loss: 0.34608030319213867\n",
      "epoch: 3, batch: 905, loss: 0.3802952468395233\n",
      "epoch: 3, batch: 906, loss: 0.31292811036109924\n",
      "epoch: 3, batch: 907, loss: 0.18317115306854248\n",
      "epoch: 3, batch: 908, loss: 0.20286430418491364\n",
      "epoch: 3, batch: 909, loss: 0.2505726218223572\n",
      "epoch: 3, batch: 910, loss: 0.30087053775787354\n",
      "epoch: 3, batch: 911, loss: 0.21329137682914734\n",
      "epoch: 3, batch: 912, loss: 0.5180206298828125\n",
      "epoch: 3, batch: 913, loss: 0.3833821713924408\n",
      "epoch: 3, batch: 914, loss: 0.2908298075199127\n",
      "epoch: 3, batch: 915, loss: 0.4353224039077759\n",
      "epoch: 3, batch: 916, loss: 0.24378006160259247\n",
      "epoch: 3, batch: 917, loss: 0.2467452585697174\n",
      "epoch: 3, batch: 918, loss: 0.446108341217041\n",
      "epoch: 3, batch: 919, loss: 0.36603865027427673\n",
      "epoch: 3, batch: 920, loss: 0.3865017592906952\n",
      "epoch: 3, batch: 921, loss: 0.2394821047782898\n",
      "epoch: 3, batch: 922, loss: 0.3879982531070709\n",
      "epoch: 3, batch: 923, loss: 0.3776715397834778\n",
      "epoch: 3, batch: 924, loss: 0.4307020604610443\n",
      "epoch: 3, batch: 925, loss: 0.33032453060150146\n",
      "epoch: 3, batch: 926, loss: 0.32023677229881287\n",
      "epoch: 3, batch: 927, loss: 0.28270044922828674\n",
      "epoch: 3, batch: 928, loss: 0.17580729722976685\n",
      "epoch: 3, batch: 929, loss: 0.3521382808685303\n",
      "epoch: 3, batch: 930, loss: 0.3797595500946045\n",
      "epoch: 3, batch: 931, loss: 0.3086543083190918\n",
      "epoch: 3, batch: 932, loss: 0.3081375062465668\n",
      "epoch: 3, batch: 933, loss: 0.4202973544597626\n",
      "epoch: 3, batch: 934, loss: 0.33022022247314453\n",
      "epoch: 3, batch: 935, loss: 0.21752817928791046\n",
      "epoch: 3, batch: 936, loss: 0.3479733169078827\n",
      "epoch: 3, batch: 937, loss: 0.20148655772209167\n",
      "epoch: 4, batch: 0, loss: 0.39922159910202026\n",
      "epoch: 4, batch: 1, loss: 0.5655309557914734\n",
      "epoch: 4, batch: 2, loss: 0.29602208733558655\n",
      "epoch: 4, batch: 3, loss: 0.3413309156894684\n",
      "epoch: 4, batch: 4, loss: 0.3707958459854126\n",
      "epoch: 4, batch: 5, loss: 0.48175403475761414\n",
      "epoch: 4, batch: 6, loss: 0.36113807559013367\n",
      "epoch: 4, batch: 7, loss: 0.46751105785369873\n",
      "epoch: 4, batch: 8, loss: 0.35881149768829346\n",
      "epoch: 4, batch: 9, loss: 0.26900285482406616\n",
      "epoch: 4, batch: 10, loss: 0.3689325749874115\n",
      "epoch: 4, batch: 11, loss: 0.25499969720840454\n",
      "epoch: 4, batch: 12, loss: 0.23681294918060303\n",
      "epoch: 4, batch: 13, loss: 0.3559839129447937\n",
      "epoch: 4, batch: 14, loss: 0.292999267578125\n",
      "epoch: 4, batch: 15, loss: 0.3388209342956543\n",
      "epoch: 4, batch: 16, loss: 0.23307698965072632\n",
      "epoch: 4, batch: 17, loss: 0.3588377833366394\n",
      "epoch: 4, batch: 18, loss: 0.22684554755687714\n",
      "epoch: 4, batch: 19, loss: 0.4476052522659302\n",
      "epoch: 4, batch: 20, loss: 0.31377658247947693\n",
      "epoch: 4, batch: 21, loss: 0.3298894762992859\n",
      "epoch: 4, batch: 22, loss: 0.3852495551109314\n",
      "epoch: 4, batch: 23, loss: 0.2481732815504074\n",
      "epoch: 4, batch: 24, loss: 0.2708774209022522\n",
      "epoch: 4, batch: 25, loss: 0.31909024715423584\n",
      "epoch: 4, batch: 26, loss: 0.5842841267585754\n",
      "epoch: 4, batch: 27, loss: 0.3247537910938263\n",
      "epoch: 4, batch: 28, loss: 0.3160487711429596\n",
      "epoch: 4, batch: 29, loss: 0.34702229499816895\n",
      "epoch: 4, batch: 30, loss: 0.6162821054458618\n",
      "epoch: 4, batch: 31, loss: 0.3341788053512573\n",
      "epoch: 4, batch: 32, loss: 0.4765525460243225\n",
      "epoch: 4, batch: 33, loss: 0.4211157262325287\n",
      "epoch: 4, batch: 34, loss: 0.3321080207824707\n",
      "epoch: 4, batch: 35, loss: 0.4159037172794342\n",
      "epoch: 4, batch: 36, loss: 0.29010650515556335\n",
      "epoch: 4, batch: 37, loss: 0.3157384395599365\n",
      "epoch: 4, batch: 38, loss: 0.5185773372650146\n",
      "epoch: 4, batch: 39, loss: 0.3814133405685425\n",
      "epoch: 4, batch: 40, loss: 0.1956283301115036\n",
      "epoch: 4, batch: 41, loss: 0.26564279198646545\n",
      "epoch: 4, batch: 42, loss: 0.32412850856781006\n",
      "epoch: 4, batch: 43, loss: 0.22251765429973602\n",
      "epoch: 4, batch: 44, loss: 0.20837275683879852\n",
      "epoch: 4, batch: 45, loss: 0.20588363707065582\n",
      "epoch: 4, batch: 46, loss: 0.31508636474609375\n",
      "epoch: 4, batch: 47, loss: 0.2662729322910309\n",
      "epoch: 4, batch: 48, loss: 0.3164740204811096\n",
      "epoch: 4, batch: 49, loss: 0.3582878112792969\n",
      "epoch: 4, batch: 50, loss: 0.21391047537326813\n",
      "epoch: 4, batch: 51, loss: 0.45572999119758606\n",
      "epoch: 4, batch: 52, loss: 0.6415205597877502\n",
      "epoch: 4, batch: 53, loss: 0.29169726371765137\n",
      "epoch: 4, batch: 54, loss: 0.2979719042778015\n",
      "epoch: 4, batch: 55, loss: 0.4206131398677826\n",
      "epoch: 4, batch: 56, loss: 0.20844785869121552\n",
      "epoch: 4, batch: 57, loss: 0.4483391046524048\n",
      "epoch: 4, batch: 58, loss: 0.3194073736667633\n",
      "epoch: 4, batch: 59, loss: 0.3099988102912903\n",
      "epoch: 4, batch: 60, loss: 0.5032415986061096\n",
      "epoch: 4, batch: 61, loss: 0.3401901423931122\n",
      "epoch: 4, batch: 62, loss: 0.6607402563095093\n",
      "epoch: 4, batch: 63, loss: 0.1768103688955307\n",
      "epoch: 4, batch: 64, loss: 0.19478458166122437\n",
      "epoch: 4, batch: 65, loss: 0.16229289770126343\n",
      "epoch: 4, batch: 66, loss: 0.18469516932964325\n",
      "epoch: 4, batch: 67, loss: 0.2419704645872116\n",
      "epoch: 4, batch: 68, loss: 0.6995794177055359\n",
      "epoch: 4, batch: 69, loss: 0.3141523599624634\n",
      "epoch: 4, batch: 70, loss: 0.2663607895374298\n",
      "epoch: 4, batch: 71, loss: 0.45380908250808716\n",
      "epoch: 4, batch: 72, loss: 0.2854699492454529\n",
      "epoch: 4, batch: 73, loss: 0.20938248932361603\n",
      "epoch: 4, batch: 74, loss: 0.32733869552612305\n",
      "epoch: 4, batch: 75, loss: 0.20686183869838715\n",
      "epoch: 4, batch: 76, loss: 0.46688005328178406\n",
      "epoch: 4, batch: 77, loss: 0.3829937279224396\n",
      "epoch: 4, batch: 78, loss: 0.36505311727523804\n",
      "epoch: 4, batch: 79, loss: 0.2163749635219574\n",
      "epoch: 4, batch: 80, loss: 0.2830367982387543\n",
      "epoch: 4, batch: 81, loss: 0.3110053837299347\n",
      "epoch: 4, batch: 82, loss: 0.311696320772171\n",
      "epoch: 4, batch: 83, loss: 0.3771340847015381\n",
      "epoch: 4, batch: 84, loss: 0.384079247713089\n",
      "epoch: 4, batch: 85, loss: 0.19184726476669312\n",
      "epoch: 4, batch: 86, loss: 0.3168027400970459\n",
      "epoch: 4, batch: 87, loss: 0.1656593233346939\n",
      "epoch: 4, batch: 88, loss: 0.2601528763771057\n",
      "epoch: 4, batch: 89, loss: 0.2231931984424591\n",
      "epoch: 4, batch: 90, loss: 0.25071975588798523\n",
      "epoch: 4, batch: 91, loss: 0.28091204166412354\n",
      "epoch: 4, batch: 92, loss: 0.12992627918720245\n",
      "epoch: 4, batch: 93, loss: 0.4838654100894928\n",
      "epoch: 4, batch: 94, loss: 0.21273820102214813\n",
      "epoch: 4, batch: 95, loss: 0.3453258275985718\n",
      "epoch: 4, batch: 96, loss: 0.32488813996315\n",
      "epoch: 4, batch: 97, loss: 0.24566075205802917\n",
      "epoch: 4, batch: 98, loss: 0.2584797143936157\n",
      "epoch: 4, batch: 99, loss: 0.33509576320648193\n",
      "epoch: 4, batch: 100, loss: 0.20984810590744019\n",
      "epoch: 4, batch: 101, loss: 0.38022559881210327\n",
      "epoch: 4, batch: 102, loss: 0.2765165865421295\n",
      "epoch: 4, batch: 103, loss: 0.2721266448497772\n",
      "epoch: 4, batch: 104, loss: 0.3195861876010895\n",
      "epoch: 4, batch: 105, loss: 0.4582657814025879\n",
      "epoch: 4, batch: 106, loss: 0.2898349463939667\n",
      "epoch: 4, batch: 107, loss: 0.23318937420845032\n",
      "epoch: 4, batch: 108, loss: 0.5131463408470154\n",
      "epoch: 4, batch: 109, loss: 0.34584271907806396\n",
      "epoch: 4, batch: 110, loss: 0.15350331366062164\n",
      "epoch: 4, batch: 111, loss: 0.34935301542282104\n",
      "epoch: 4, batch: 112, loss: 0.3205721080303192\n",
      "epoch: 4, batch: 113, loss: 0.28109118342399597\n",
      "epoch: 4, batch: 114, loss: 0.4006142318248749\n",
      "epoch: 4, batch: 115, loss: 0.3366759121417999\n",
      "epoch: 4, batch: 116, loss: 0.3494754135608673\n",
      "epoch: 4, batch: 117, loss: 0.32721006870269775\n",
      "epoch: 4, batch: 118, loss: 0.3045535385608673\n",
      "epoch: 4, batch: 119, loss: 0.31587111949920654\n",
      "epoch: 4, batch: 120, loss: 0.4691481292247772\n",
      "epoch: 4, batch: 121, loss: 0.23827336728572845\n",
      "epoch: 4, batch: 122, loss: 0.1794179230928421\n",
      "epoch: 4, batch: 123, loss: 0.25963008403778076\n",
      "epoch: 4, batch: 124, loss: 0.27516335248947144\n",
      "epoch: 4, batch: 125, loss: 0.4041915237903595\n",
      "epoch: 4, batch: 126, loss: 0.24848395586013794\n",
      "epoch: 4, batch: 127, loss: 0.45707541704177856\n",
      "epoch: 4, batch: 128, loss: 0.4543721377849579\n",
      "epoch: 4, batch: 129, loss: 0.29974454641342163\n",
      "epoch: 4, batch: 130, loss: 0.3674888610839844\n",
      "epoch: 4, batch: 131, loss: 0.46604451537132263\n",
      "epoch: 4, batch: 132, loss: 0.30424612760543823\n",
      "epoch: 4, batch: 133, loss: 0.46932774782180786\n",
      "epoch: 4, batch: 134, loss: 0.48972758650779724\n",
      "epoch: 4, batch: 135, loss: 0.33376139402389526\n",
      "epoch: 4, batch: 136, loss: 0.7121082544326782\n",
      "epoch: 4, batch: 137, loss: 0.3397314250469208\n",
      "epoch: 4, batch: 138, loss: 0.2597799301147461\n",
      "epoch: 4, batch: 139, loss: 0.2516704201698303\n",
      "epoch: 4, batch: 140, loss: 0.25949203968048096\n",
      "epoch: 4, batch: 141, loss: 0.26820316910743713\n",
      "epoch: 4, batch: 142, loss: 0.10229599475860596\n",
      "epoch: 4, batch: 143, loss: 0.36120283603668213\n",
      "epoch: 4, batch: 144, loss: 0.3964994251728058\n",
      "epoch: 4, batch: 145, loss: 0.39888766407966614\n",
      "epoch: 4, batch: 146, loss: 0.33538323640823364\n",
      "epoch: 4, batch: 147, loss: 0.3675702214241028\n",
      "epoch: 4, batch: 148, loss: 0.25247374176979065\n",
      "epoch: 4, batch: 149, loss: 0.3719189167022705\n",
      "epoch: 4, batch: 150, loss: 0.38967323303222656\n",
      "epoch: 4, batch: 151, loss: 0.30881330370903015\n",
      "epoch: 4, batch: 152, loss: 0.28375449776649475\n",
      "epoch: 4, batch: 153, loss: 0.31346920132637024\n",
      "epoch: 4, batch: 154, loss: 0.19078345596790314\n",
      "epoch: 4, batch: 155, loss: 0.34007927775382996\n",
      "epoch: 4, batch: 156, loss: 0.25040051341056824\n",
      "epoch: 4, batch: 157, loss: 0.36272087693214417\n",
      "epoch: 4, batch: 158, loss: 0.34587669372558594\n",
      "epoch: 4, batch: 159, loss: 0.28346407413482666\n",
      "epoch: 4, batch: 160, loss: 0.3050795793533325\n",
      "epoch: 4, batch: 161, loss: 0.36913973093032837\n",
      "epoch: 4, batch: 162, loss: 0.19158053398132324\n",
      "epoch: 4, batch: 163, loss: 0.43069082498550415\n",
      "epoch: 4, batch: 164, loss: 0.17230039834976196\n",
      "epoch: 4, batch: 165, loss: 0.2078528255224228\n",
      "epoch: 4, batch: 166, loss: 0.2967626750469208\n",
      "epoch: 4, batch: 167, loss: 0.3144485652446747\n",
      "epoch: 4, batch: 168, loss: 0.2790273427963257\n",
      "epoch: 4, batch: 169, loss: 0.40689146518707275\n",
      "epoch: 4, batch: 170, loss: 0.46377673745155334\n",
      "epoch: 4, batch: 171, loss: 0.3887789845466614\n",
      "epoch: 4, batch: 172, loss: 0.36186784505844116\n",
      "epoch: 4, batch: 173, loss: 0.3727889358997345\n",
      "epoch: 4, batch: 174, loss: 0.20942625403404236\n",
      "epoch: 4, batch: 175, loss: 0.21667148172855377\n",
      "epoch: 4, batch: 176, loss: 0.31975170969963074\n",
      "epoch: 4, batch: 177, loss: 0.4290477931499481\n",
      "epoch: 4, batch: 178, loss: 0.21127162873744965\n",
      "epoch: 4, batch: 179, loss: 0.2758519649505615\n",
      "epoch: 4, batch: 180, loss: 0.36315450072288513\n",
      "epoch: 4, batch: 181, loss: 0.16722580790519714\n",
      "epoch: 4, batch: 182, loss: 0.3345448672771454\n",
      "epoch: 4, batch: 183, loss: 0.260299950838089\n",
      "epoch: 4, batch: 184, loss: 0.3453580439090729\n",
      "epoch: 4, batch: 185, loss: 0.3143591582775116\n",
      "epoch: 4, batch: 186, loss: 0.35536015033721924\n",
      "epoch: 4, batch: 187, loss: 0.3572103977203369\n",
      "epoch: 4, batch: 188, loss: 0.18546463549137115\n",
      "epoch: 4, batch: 189, loss: 0.45142388343811035\n",
      "epoch: 4, batch: 190, loss: 0.2177399843931198\n",
      "epoch: 4, batch: 191, loss: 0.8328213691711426\n",
      "epoch: 4, batch: 192, loss: 0.3506481349468231\n",
      "epoch: 4, batch: 193, loss: 0.34010687470436096\n",
      "epoch: 4, batch: 194, loss: 0.3980783522129059\n",
      "epoch: 4, batch: 195, loss: 0.20295216143131256\n",
      "epoch: 4, batch: 196, loss: 0.6322298049926758\n",
      "epoch: 4, batch: 197, loss: 0.5519972443580627\n",
      "epoch: 4, batch: 198, loss: 0.41187840700149536\n",
      "epoch: 4, batch: 199, loss: 0.2902533710002899\n",
      "epoch: 4, batch: 200, loss: 0.2670857012271881\n",
      "epoch: 4, batch: 201, loss: 0.41423705220222473\n",
      "epoch: 4, batch: 202, loss: 0.32510876655578613\n",
      "epoch: 4, batch: 203, loss: 0.24468109011650085\n",
      "epoch: 4, batch: 204, loss: 0.36285334825515747\n",
      "epoch: 4, batch: 205, loss: 0.24981257319450378\n",
      "epoch: 4, batch: 206, loss: 0.22326630353927612\n",
      "epoch: 4, batch: 207, loss: 0.3324832320213318\n",
      "epoch: 4, batch: 208, loss: 0.26696914434432983\n",
      "epoch: 4, batch: 209, loss: 0.44434523582458496\n",
      "epoch: 4, batch: 210, loss: 0.26817163825035095\n",
      "epoch: 4, batch: 211, loss: 0.2852391302585602\n",
      "epoch: 4, batch: 212, loss: 0.2798498868942261\n",
      "epoch: 4, batch: 213, loss: 0.2673535943031311\n",
      "epoch: 4, batch: 214, loss: 0.3621750771999359\n",
      "epoch: 4, batch: 215, loss: 0.2865198254585266\n",
      "epoch: 4, batch: 216, loss: 0.2887653708457947\n",
      "epoch: 4, batch: 217, loss: 0.33502137660980225\n",
      "epoch: 4, batch: 218, loss: 0.33277052640914917\n",
      "epoch: 4, batch: 219, loss: 0.2848520874977112\n",
      "epoch: 4, batch: 220, loss: 0.31173115968704224\n",
      "epoch: 4, batch: 221, loss: 0.252116858959198\n",
      "epoch: 4, batch: 222, loss: 0.3424988090991974\n",
      "epoch: 4, batch: 223, loss: 0.27724674344062805\n",
      "epoch: 4, batch: 224, loss: 0.241524338722229\n",
      "epoch: 4, batch: 225, loss: 0.2897977828979492\n",
      "epoch: 4, batch: 226, loss: 0.381826251745224\n",
      "epoch: 4, batch: 227, loss: 0.2925790548324585\n",
      "epoch: 4, batch: 228, loss: 0.3506762981414795\n",
      "epoch: 4, batch: 229, loss: 0.4640273153781891\n",
      "epoch: 4, batch: 230, loss: 0.2582258880138397\n",
      "epoch: 4, batch: 231, loss: 0.26484444737434387\n",
      "epoch: 4, batch: 232, loss: 0.42292988300323486\n",
      "epoch: 4, batch: 233, loss: 0.4079263508319855\n",
      "epoch: 4, batch: 234, loss: 0.15848878026008606\n",
      "epoch: 4, batch: 235, loss: 0.381693959236145\n",
      "epoch: 4, batch: 236, loss: 0.31917357444763184\n",
      "epoch: 4, batch: 237, loss: 0.17226500809192657\n",
      "epoch: 4, batch: 238, loss: 0.30078360438346863\n",
      "epoch: 4, batch: 239, loss: 0.4092162251472473\n",
      "epoch: 4, batch: 240, loss: 0.3068855106830597\n",
      "epoch: 4, batch: 241, loss: 0.40140724182128906\n",
      "epoch: 4, batch: 242, loss: 0.32826027274131775\n",
      "epoch: 4, batch: 243, loss: 0.21127468347549438\n",
      "epoch: 4, batch: 244, loss: 0.27296382188796997\n",
      "epoch: 4, batch: 245, loss: 0.22856885194778442\n",
      "epoch: 4, batch: 246, loss: 0.33604153990745544\n",
      "epoch: 4, batch: 247, loss: 0.47037041187286377\n",
      "epoch: 4, batch: 248, loss: 0.2864557206630707\n",
      "epoch: 4, batch: 249, loss: 0.4142162799835205\n",
      "epoch: 4, batch: 250, loss: 0.3339959383010864\n",
      "epoch: 4, batch: 251, loss: 0.35031843185424805\n",
      "epoch: 4, batch: 252, loss: 0.4679299592971802\n",
      "epoch: 4, batch: 253, loss: 0.3233964145183563\n",
      "epoch: 4, batch: 254, loss: 0.6539092659950256\n",
      "epoch: 4, batch: 255, loss: 0.45773983001708984\n",
      "epoch: 4, batch: 256, loss: 0.2674903869628906\n",
      "epoch: 4, batch: 257, loss: 0.46757450699806213\n",
      "epoch: 4, batch: 258, loss: 0.44834962487220764\n",
      "epoch: 4, batch: 259, loss: 0.35987696051597595\n",
      "epoch: 4, batch: 260, loss: 0.33520570397377014\n",
      "epoch: 4, batch: 261, loss: 0.1378583461046219\n",
      "epoch: 4, batch: 262, loss: 0.3598628044128418\n",
      "epoch: 4, batch: 263, loss: 0.35422107577323914\n",
      "epoch: 4, batch: 264, loss: 0.2970861792564392\n",
      "epoch: 4, batch: 265, loss: 0.47007977962493896\n",
      "epoch: 4, batch: 266, loss: 0.22921742498874664\n",
      "epoch: 4, batch: 267, loss: 0.16817328333854675\n",
      "epoch: 4, batch: 268, loss: 0.4334350526332855\n",
      "epoch: 4, batch: 269, loss: 0.3147626221179962\n",
      "epoch: 4, batch: 270, loss: 0.31225067377090454\n",
      "epoch: 4, batch: 271, loss: 0.3349159359931946\n",
      "epoch: 4, batch: 272, loss: 0.3061968982219696\n",
      "epoch: 4, batch: 273, loss: 0.2520601451396942\n",
      "epoch: 4, batch: 274, loss: 0.19152048230171204\n",
      "epoch: 4, batch: 275, loss: 0.4612980782985687\n",
      "epoch: 4, batch: 276, loss: 0.2680559754371643\n",
      "epoch: 4, batch: 277, loss: 0.404605895280838\n",
      "epoch: 4, batch: 278, loss: 0.28227099776268005\n",
      "epoch: 4, batch: 279, loss: 0.29600971937179565\n",
      "epoch: 4, batch: 280, loss: 0.26154467463493347\n",
      "epoch: 4, batch: 281, loss: 0.21101059019565582\n",
      "epoch: 4, batch: 282, loss: 0.3583153784275055\n",
      "epoch: 4, batch: 283, loss: 0.4813052713871002\n",
      "epoch: 4, batch: 284, loss: 0.2021375447511673\n",
      "epoch: 4, batch: 285, loss: 0.333751916885376\n",
      "epoch: 4, batch: 286, loss: 0.3571629226207733\n",
      "epoch: 4, batch: 287, loss: 0.23433059453964233\n",
      "epoch: 4, batch: 288, loss: 0.19441407918930054\n",
      "epoch: 4, batch: 289, loss: 0.3845294117927551\n",
      "epoch: 4, batch: 290, loss: 0.31429293751716614\n",
      "epoch: 4, batch: 291, loss: 0.44828543066978455\n",
      "epoch: 4, batch: 292, loss: 0.32070693373680115\n",
      "epoch: 4, batch: 293, loss: 0.27858856320381165\n",
      "epoch: 4, batch: 294, loss: 0.3535993993282318\n",
      "epoch: 4, batch: 295, loss: 0.25421762466430664\n",
      "epoch: 4, batch: 296, loss: 0.21156145632266998\n",
      "epoch: 4, batch: 297, loss: 0.24598367512226105\n",
      "epoch: 4, batch: 298, loss: 0.3329121470451355\n",
      "epoch: 4, batch: 299, loss: 0.366827130317688\n",
      "epoch: 4, batch: 300, loss: 0.2738608121871948\n",
      "epoch: 4, batch: 301, loss: 0.2500993609428406\n",
      "epoch: 4, batch: 302, loss: 0.15397909283638\n",
      "epoch: 4, batch: 303, loss: 0.3160143792629242\n",
      "epoch: 4, batch: 304, loss: 0.1990097016096115\n",
      "epoch: 4, batch: 305, loss: 0.3520238697528839\n",
      "epoch: 4, batch: 306, loss: 0.40601876378059387\n",
      "epoch: 4, batch: 307, loss: 0.2474064975976944\n",
      "epoch: 4, batch: 308, loss: 0.5363389849662781\n",
      "epoch: 4, batch: 309, loss: 0.1510491669178009\n",
      "epoch: 4, batch: 310, loss: 0.47672218084335327\n",
      "epoch: 4, batch: 311, loss: 0.2848004102706909\n",
      "epoch: 4, batch: 312, loss: 0.20025573670864105\n",
      "epoch: 4, batch: 313, loss: 0.21955494582653046\n",
      "epoch: 4, batch: 314, loss: 0.22087080776691437\n",
      "epoch: 4, batch: 315, loss: 0.4276581108570099\n",
      "epoch: 4, batch: 316, loss: 0.33718571066856384\n",
      "epoch: 4, batch: 317, loss: 0.17981041967868805\n",
      "epoch: 4, batch: 318, loss: 0.5424546599388123\n",
      "epoch: 4, batch: 319, loss: 0.35418662428855896\n",
      "epoch: 4, batch: 320, loss: 0.3727378249168396\n",
      "epoch: 4, batch: 321, loss: 0.26566603779792786\n",
      "epoch: 4, batch: 322, loss: 0.13052357733249664\n",
      "epoch: 4, batch: 323, loss: 0.3619438707828522\n",
      "epoch: 4, batch: 324, loss: 0.3346306085586548\n",
      "epoch: 4, batch: 325, loss: 0.4492773115634918\n",
      "epoch: 4, batch: 326, loss: 0.1761096566915512\n",
      "epoch: 4, batch: 327, loss: 0.2914327383041382\n",
      "epoch: 4, batch: 328, loss: 0.19292621314525604\n",
      "epoch: 4, batch: 329, loss: 0.5701045393943787\n",
      "epoch: 4, batch: 330, loss: 0.4666784107685089\n",
      "epoch: 4, batch: 331, loss: 0.31840401887893677\n",
      "epoch: 4, batch: 332, loss: 0.13766789436340332\n",
      "epoch: 4, batch: 333, loss: 0.2558668553829193\n",
      "epoch: 4, batch: 334, loss: 0.2523668706417084\n",
      "epoch: 4, batch: 335, loss: 0.2634361684322357\n",
      "epoch: 4, batch: 336, loss: 0.39284712076187134\n",
      "epoch: 4, batch: 337, loss: 0.36165526509284973\n",
      "epoch: 4, batch: 338, loss: 0.3199103772640228\n",
      "epoch: 4, batch: 339, loss: 0.2226009964942932\n",
      "epoch: 4, batch: 340, loss: 0.47808679938316345\n",
      "epoch: 4, batch: 341, loss: 0.26094383001327515\n",
      "epoch: 4, batch: 342, loss: 0.2983117401599884\n",
      "epoch: 4, batch: 343, loss: 0.22539792954921722\n",
      "epoch: 4, batch: 344, loss: 0.34905940294265747\n",
      "epoch: 4, batch: 345, loss: 0.181657075881958\n",
      "epoch: 4, batch: 346, loss: 0.3385559022426605\n",
      "epoch: 4, batch: 347, loss: 0.24389208853244781\n",
      "epoch: 4, batch: 348, loss: 0.3246670365333557\n",
      "epoch: 4, batch: 349, loss: 0.3159278333187103\n",
      "epoch: 4, batch: 350, loss: 0.35956719517707825\n",
      "epoch: 4, batch: 351, loss: 0.5670424699783325\n",
      "epoch: 4, batch: 352, loss: 0.40328729152679443\n",
      "epoch: 4, batch: 353, loss: 0.23351553082466125\n",
      "epoch: 4, batch: 354, loss: 0.20529888570308685\n",
      "epoch: 4, batch: 355, loss: 0.28642016649246216\n",
      "epoch: 4, batch: 356, loss: 0.2871847152709961\n",
      "epoch: 4, batch: 357, loss: 0.18033486604690552\n",
      "epoch: 4, batch: 358, loss: 0.16495734453201294\n",
      "epoch: 4, batch: 359, loss: 0.22328566014766693\n",
      "epoch: 4, batch: 360, loss: 0.19863799214363098\n",
      "epoch: 4, batch: 361, loss: 0.43452170491218567\n",
      "epoch: 4, batch: 362, loss: 0.25772199034690857\n",
      "epoch: 4, batch: 363, loss: 0.37935930490493774\n",
      "epoch: 4, batch: 364, loss: 0.41082829236984253\n",
      "epoch: 4, batch: 365, loss: 0.33304736018180847\n",
      "epoch: 4, batch: 366, loss: 0.25361067056655884\n",
      "epoch: 4, batch: 367, loss: 0.3776300847530365\n",
      "epoch: 4, batch: 368, loss: 0.5250627994537354\n",
      "epoch: 4, batch: 369, loss: 0.3270096778869629\n",
      "epoch: 4, batch: 370, loss: 0.23206749558448792\n",
      "epoch: 4, batch: 371, loss: 0.3578929603099823\n",
      "epoch: 4, batch: 372, loss: 0.5258360505104065\n",
      "epoch: 4, batch: 373, loss: 0.2530727684497833\n",
      "epoch: 4, batch: 374, loss: 0.40034255385398865\n",
      "epoch: 4, batch: 375, loss: 0.19783997535705566\n",
      "epoch: 4, batch: 376, loss: 0.42372408509254456\n",
      "epoch: 4, batch: 377, loss: 0.2859014570713043\n",
      "epoch: 4, batch: 378, loss: 0.22432690858840942\n",
      "epoch: 4, batch: 379, loss: 0.4889090657234192\n",
      "epoch: 4, batch: 380, loss: 0.24683654308319092\n",
      "epoch: 4, batch: 381, loss: 0.4796628952026367\n",
      "epoch: 4, batch: 382, loss: 0.18742340803146362\n",
      "epoch: 4, batch: 383, loss: 0.15933175384998322\n",
      "epoch: 4, batch: 384, loss: 0.21508507430553436\n",
      "epoch: 4, batch: 385, loss: 0.4792703688144684\n",
      "epoch: 4, batch: 386, loss: 0.30585771799087524\n",
      "epoch: 4, batch: 387, loss: 0.3095112442970276\n",
      "epoch: 4, batch: 388, loss: 0.28381648659706116\n",
      "epoch: 4, batch: 389, loss: 0.4299115836620331\n",
      "epoch: 4, batch: 390, loss: 0.2481514811515808\n",
      "epoch: 4, batch: 391, loss: 0.4156663417816162\n",
      "epoch: 4, batch: 392, loss: 0.2616460919380188\n",
      "epoch: 4, batch: 393, loss: 0.3845936059951782\n",
      "epoch: 4, batch: 394, loss: 0.2284199446439743\n",
      "epoch: 4, batch: 395, loss: 0.45384955406188965\n",
      "epoch: 4, batch: 396, loss: 0.35481196641921997\n",
      "epoch: 4, batch: 397, loss: 0.45828378200531006\n",
      "epoch: 4, batch: 398, loss: 0.23259712755680084\n",
      "epoch: 4, batch: 399, loss: 0.2189456820487976\n",
      "epoch: 4, batch: 400, loss: 0.30147993564605713\n",
      "epoch: 4, batch: 401, loss: 0.3374675214290619\n",
      "epoch: 4, batch: 402, loss: 0.24240291118621826\n",
      "epoch: 4, batch: 403, loss: 0.2715626358985901\n",
      "epoch: 4, batch: 404, loss: 0.2863768935203552\n",
      "epoch: 4, batch: 405, loss: 0.22562400996685028\n",
      "epoch: 4, batch: 406, loss: 0.39456838369369507\n",
      "epoch: 4, batch: 407, loss: 0.3285423517227173\n",
      "epoch: 4, batch: 408, loss: 0.3965412676334381\n",
      "epoch: 4, batch: 409, loss: 0.22948536276817322\n",
      "epoch: 4, batch: 410, loss: 0.2855229377746582\n",
      "epoch: 4, batch: 411, loss: 0.28418710827827454\n",
      "epoch: 4, batch: 412, loss: 0.289121150970459\n",
      "epoch: 4, batch: 413, loss: 0.31142333149909973\n",
      "epoch: 4, batch: 414, loss: 0.2945667803287506\n",
      "epoch: 4, batch: 415, loss: 0.455926775932312\n",
      "epoch: 4, batch: 416, loss: 0.26072028279304504\n",
      "epoch: 4, batch: 417, loss: 0.29341310262680054\n",
      "epoch: 4, batch: 418, loss: 0.3550393581390381\n",
      "epoch: 4, batch: 419, loss: 0.398488312959671\n",
      "epoch: 4, batch: 420, loss: 0.22289682924747467\n",
      "epoch: 4, batch: 421, loss: 0.23146477341651917\n",
      "epoch: 4, batch: 422, loss: 0.2126644104719162\n",
      "epoch: 4, batch: 423, loss: 0.3207034468650818\n",
      "epoch: 4, batch: 424, loss: 0.33509522676467896\n",
      "epoch: 4, batch: 425, loss: 0.23930275440216064\n",
      "epoch: 4, batch: 426, loss: 0.2303846776485443\n",
      "epoch: 4, batch: 427, loss: 0.27392539381980896\n",
      "epoch: 4, batch: 428, loss: 0.25461530685424805\n",
      "epoch: 4, batch: 429, loss: 0.3074360489845276\n",
      "epoch: 4, batch: 430, loss: 0.3512905240058899\n",
      "epoch: 4, batch: 431, loss: 0.22462347149848938\n",
      "epoch: 4, batch: 432, loss: 0.3053407669067383\n",
      "epoch: 4, batch: 433, loss: 0.34170782566070557\n",
      "epoch: 4, batch: 434, loss: 0.24712610244750977\n",
      "epoch: 4, batch: 435, loss: 0.20112042129039764\n",
      "epoch: 4, batch: 436, loss: 0.2148536741733551\n",
      "epoch: 4, batch: 437, loss: 0.33057981729507446\n",
      "epoch: 4, batch: 438, loss: 0.2869158089160919\n",
      "epoch: 4, batch: 439, loss: 0.26468685269355774\n",
      "epoch: 4, batch: 440, loss: 0.30908846855163574\n",
      "epoch: 4, batch: 441, loss: 0.2776234745979309\n",
      "epoch: 4, batch: 442, loss: 0.2312024086713791\n",
      "epoch: 4, batch: 443, loss: 0.31694290041923523\n",
      "epoch: 4, batch: 444, loss: 0.20373646914958954\n",
      "epoch: 4, batch: 445, loss: 0.4255816638469696\n",
      "epoch: 4, batch: 446, loss: 0.21881237626075745\n",
      "epoch: 4, batch: 447, loss: 0.26584744453430176\n",
      "epoch: 4, batch: 448, loss: 0.30694645643234253\n",
      "epoch: 4, batch: 449, loss: 0.24201291799545288\n",
      "epoch: 4, batch: 450, loss: 0.2777043282985687\n",
      "epoch: 4, batch: 451, loss: 0.2094956785440445\n",
      "epoch: 4, batch: 452, loss: 0.47428134083747864\n",
      "epoch: 4, batch: 453, loss: 0.1549382358789444\n",
      "epoch: 4, batch: 454, loss: 0.3468788266181946\n",
      "epoch: 4, batch: 455, loss: 0.509657084941864\n",
      "epoch: 4, batch: 456, loss: 0.41399329900741577\n",
      "epoch: 4, batch: 457, loss: 0.41749703884124756\n",
      "epoch: 4, batch: 458, loss: 0.41703784465789795\n",
      "epoch: 4, batch: 459, loss: 0.2895817756652832\n",
      "epoch: 4, batch: 460, loss: 0.18459540605545044\n",
      "epoch: 4, batch: 461, loss: 0.37538251280784607\n",
      "epoch: 4, batch: 462, loss: 0.1853717863559723\n",
      "epoch: 4, batch: 463, loss: 0.2989892065525055\n",
      "epoch: 4, batch: 464, loss: 0.25872161984443665\n",
      "epoch: 4, batch: 465, loss: 0.46455609798431396\n",
      "epoch: 4, batch: 466, loss: 0.4321839213371277\n",
      "epoch: 4, batch: 467, loss: 0.19163420796394348\n",
      "epoch: 4, batch: 468, loss: 0.25358542799949646\n",
      "epoch: 4, batch: 469, loss: 0.31446781754493713\n",
      "epoch: 4, batch: 470, loss: 0.40170663595199585\n",
      "epoch: 4, batch: 471, loss: 0.32713383436203003\n",
      "epoch: 4, batch: 472, loss: 0.23280508816242218\n",
      "epoch: 4, batch: 473, loss: 0.3107748031616211\n",
      "epoch: 4, batch: 474, loss: 0.28092631697654724\n",
      "epoch: 4, batch: 475, loss: 0.3391716182231903\n",
      "epoch: 4, batch: 476, loss: 0.2059391438961029\n",
      "epoch: 4, batch: 477, loss: 0.27992379665374756\n",
      "epoch: 4, batch: 478, loss: 0.3200101852416992\n",
      "epoch: 4, batch: 479, loss: 0.42382150888442993\n",
      "epoch: 4, batch: 480, loss: 0.4233822822570801\n",
      "epoch: 4, batch: 481, loss: 0.4329077899456024\n",
      "epoch: 4, batch: 482, loss: 0.35424739122390747\n",
      "epoch: 4, batch: 483, loss: 0.24640706181526184\n",
      "epoch: 4, batch: 484, loss: 0.2272561937570572\n",
      "epoch: 4, batch: 485, loss: 0.40569818019866943\n",
      "epoch: 4, batch: 486, loss: 0.22622980177402496\n",
      "epoch: 4, batch: 487, loss: 0.3001878559589386\n",
      "epoch: 4, batch: 488, loss: 0.24001607298851013\n",
      "epoch: 4, batch: 489, loss: 0.23094354569911957\n",
      "epoch: 4, batch: 490, loss: 0.4252294898033142\n",
      "epoch: 4, batch: 491, loss: 0.3908022940158844\n",
      "epoch: 4, batch: 492, loss: 0.3746097981929779\n",
      "epoch: 4, batch: 493, loss: 0.2675839364528656\n",
      "epoch: 4, batch: 494, loss: 0.24327389895915985\n",
      "epoch: 4, batch: 495, loss: 0.21227049827575684\n",
      "epoch: 4, batch: 496, loss: 0.5107434391975403\n",
      "epoch: 4, batch: 497, loss: 0.21644212305545807\n",
      "epoch: 4, batch: 498, loss: 0.4796029329299927\n",
      "epoch: 4, batch: 499, loss: 0.5781944990158081\n",
      "epoch: 4, batch: 500, loss: 0.503947913646698\n",
      "epoch: 4, batch: 501, loss: 0.14798574149608612\n",
      "epoch: 4, batch: 502, loss: 0.41957443952560425\n",
      "epoch: 4, batch: 503, loss: 0.6095993518829346\n",
      "epoch: 4, batch: 504, loss: 0.5372833013534546\n",
      "epoch: 4, batch: 505, loss: 0.5715343356132507\n",
      "epoch: 4, batch: 506, loss: 0.3486807942390442\n",
      "epoch: 4, batch: 507, loss: 0.18481430411338806\n",
      "epoch: 4, batch: 508, loss: 0.14271993935108185\n",
      "epoch: 4, batch: 509, loss: 0.44100481271743774\n",
      "epoch: 4, batch: 510, loss: 0.49107614159584045\n",
      "epoch: 4, batch: 511, loss: 0.22594261169433594\n",
      "epoch: 4, batch: 512, loss: 0.2856970727443695\n",
      "epoch: 4, batch: 513, loss: 0.3769572377204895\n",
      "epoch: 4, batch: 514, loss: 0.2669844329357147\n",
      "epoch: 4, batch: 515, loss: 0.24210666120052338\n",
      "epoch: 4, batch: 516, loss: 0.3146362006664276\n",
      "epoch: 4, batch: 517, loss: 0.42066916823387146\n",
      "epoch: 4, batch: 518, loss: 0.30995646119117737\n",
      "epoch: 4, batch: 519, loss: 0.498695969581604\n",
      "epoch: 4, batch: 520, loss: 0.2604893147945404\n",
      "epoch: 4, batch: 521, loss: 0.16871187090873718\n",
      "epoch: 4, batch: 522, loss: 0.24840418994426727\n",
      "epoch: 4, batch: 523, loss: 0.2299846112728119\n",
      "epoch: 4, batch: 524, loss: 0.2866245210170746\n",
      "epoch: 4, batch: 525, loss: 0.5491740703582764\n",
      "epoch: 4, batch: 526, loss: 0.33217400312423706\n",
      "epoch: 4, batch: 527, loss: 0.2402278184890747\n",
      "epoch: 4, batch: 528, loss: 0.3983287215232849\n",
      "epoch: 4, batch: 529, loss: 0.3373551368713379\n",
      "epoch: 4, batch: 530, loss: 0.3367372751235962\n",
      "epoch: 4, batch: 531, loss: 0.2847993075847626\n",
      "epoch: 4, batch: 532, loss: 0.5135918855667114\n",
      "epoch: 4, batch: 533, loss: 0.35149744153022766\n",
      "epoch: 4, batch: 534, loss: 0.3287963271141052\n",
      "epoch: 4, batch: 535, loss: 0.3559156060218811\n",
      "epoch: 4, batch: 536, loss: 0.23437625169754028\n",
      "epoch: 4, batch: 537, loss: 0.35890159010887146\n",
      "epoch: 4, batch: 538, loss: 0.3237178921699524\n",
      "epoch: 4, batch: 539, loss: 0.3775681257247925\n",
      "epoch: 4, batch: 540, loss: 0.19551913440227509\n",
      "epoch: 4, batch: 541, loss: 0.4319184124469757\n",
      "epoch: 4, batch: 542, loss: 0.2264028936624527\n",
      "epoch: 4, batch: 543, loss: 0.1670459806919098\n",
      "epoch: 4, batch: 544, loss: 0.27966973185539246\n",
      "epoch: 4, batch: 545, loss: 0.2073460966348648\n",
      "epoch: 4, batch: 546, loss: 0.4370139241218567\n",
      "epoch: 4, batch: 547, loss: 0.23624828457832336\n",
      "epoch: 4, batch: 548, loss: 0.20003245770931244\n",
      "epoch: 4, batch: 549, loss: 0.25732606649398804\n",
      "epoch: 4, batch: 550, loss: 0.33969646692276\n",
      "epoch: 4, batch: 551, loss: 0.36520668864250183\n",
      "epoch: 4, batch: 552, loss: 0.28795647621154785\n",
      "epoch: 4, batch: 553, loss: 0.25434622168540955\n",
      "epoch: 4, batch: 554, loss: 0.2857758700847626\n",
      "epoch: 4, batch: 555, loss: 0.1724357008934021\n",
      "epoch: 4, batch: 556, loss: 0.35410016775131226\n",
      "epoch: 4, batch: 557, loss: 0.3674510419368744\n",
      "epoch: 4, batch: 558, loss: 0.34881293773651123\n",
      "epoch: 4, batch: 559, loss: 0.2985914945602417\n",
      "epoch: 4, batch: 560, loss: 0.4242668151855469\n",
      "epoch: 4, batch: 561, loss: 0.09384527802467346\n",
      "epoch: 4, batch: 562, loss: 0.2807753086090088\n",
      "epoch: 4, batch: 563, loss: 0.33167555928230286\n",
      "epoch: 4, batch: 564, loss: 0.23728570342063904\n",
      "epoch: 4, batch: 565, loss: 0.33975401520729065\n",
      "epoch: 4, batch: 566, loss: 0.2586241066455841\n",
      "epoch: 4, batch: 567, loss: 0.45756903290748596\n",
      "epoch: 4, batch: 568, loss: 0.24632248282432556\n",
      "epoch: 4, batch: 569, loss: 0.2924642264842987\n",
      "epoch: 4, batch: 570, loss: 0.2979077398777008\n",
      "epoch: 4, batch: 571, loss: 0.11050914973020554\n",
      "epoch: 4, batch: 572, loss: 0.31115439534187317\n",
      "epoch: 4, batch: 573, loss: 0.36710497736930847\n",
      "epoch: 4, batch: 574, loss: 0.14794376492500305\n",
      "epoch: 4, batch: 575, loss: 0.30220460891723633\n",
      "epoch: 4, batch: 576, loss: 0.22288312017917633\n",
      "epoch: 4, batch: 577, loss: 0.5133723616600037\n",
      "epoch: 4, batch: 578, loss: 0.1755959391593933\n",
      "epoch: 4, batch: 579, loss: 0.3018186390399933\n",
      "epoch: 4, batch: 580, loss: 0.2047768086194992\n",
      "epoch: 4, batch: 581, loss: 0.33482521772384644\n",
      "epoch: 4, batch: 582, loss: 0.2717367708683014\n",
      "epoch: 4, batch: 583, loss: 0.23223866522312164\n",
      "epoch: 4, batch: 584, loss: 0.29433673620224\n",
      "epoch: 4, batch: 585, loss: 0.5118035078048706\n",
      "epoch: 4, batch: 586, loss: 0.2979567050933838\n",
      "epoch: 4, batch: 587, loss: 0.3451829254627228\n",
      "epoch: 4, batch: 588, loss: 0.3256133198738098\n",
      "epoch: 4, batch: 589, loss: 0.31864869594573975\n",
      "epoch: 4, batch: 590, loss: 0.20342059433460236\n",
      "epoch: 4, batch: 591, loss: 0.24967582523822784\n",
      "epoch: 4, batch: 592, loss: 0.2955632209777832\n",
      "epoch: 4, batch: 593, loss: 0.40882033109664917\n",
      "epoch: 4, batch: 594, loss: 0.21419081091880798\n",
      "epoch: 4, batch: 595, loss: 0.17824521660804749\n",
      "epoch: 4, batch: 596, loss: 0.17648568749427795\n",
      "epoch: 4, batch: 597, loss: 0.18730781972408295\n",
      "epoch: 4, batch: 598, loss: 0.27539893984794617\n",
      "epoch: 4, batch: 599, loss: 0.1432701051235199\n",
      "epoch: 4, batch: 600, loss: 0.1674051284790039\n",
      "epoch: 4, batch: 601, loss: 0.3295263946056366\n",
      "epoch: 4, batch: 602, loss: 0.2443438619375229\n",
      "epoch: 4, batch: 603, loss: 0.4844728708267212\n",
      "epoch: 4, batch: 604, loss: 0.3178507387638092\n",
      "epoch: 4, batch: 605, loss: 0.43671467900276184\n",
      "epoch: 4, batch: 606, loss: 0.4457349479198456\n",
      "epoch: 4, batch: 607, loss: 0.2512696087360382\n",
      "epoch: 4, batch: 608, loss: 0.3645757734775543\n",
      "epoch: 4, batch: 609, loss: 0.34090250730514526\n",
      "epoch: 4, batch: 610, loss: 0.23237904906272888\n",
      "epoch: 4, batch: 611, loss: 0.3886203467845917\n",
      "epoch: 4, batch: 612, loss: 0.2503269910812378\n",
      "epoch: 4, batch: 613, loss: 0.18923309445381165\n",
      "epoch: 4, batch: 614, loss: 0.40364110469818115\n",
      "epoch: 4, batch: 615, loss: 0.2945806384086609\n",
      "epoch: 4, batch: 616, loss: 0.13627012073993683\n",
      "epoch: 4, batch: 617, loss: 0.42752066254615784\n",
      "epoch: 4, batch: 618, loss: 0.1909870207309723\n",
      "epoch: 4, batch: 619, loss: 0.18218860030174255\n",
      "epoch: 4, batch: 620, loss: 0.24444743990898132\n",
      "epoch: 4, batch: 621, loss: 0.3432113826274872\n",
      "epoch: 4, batch: 622, loss: 0.24776224792003632\n",
      "epoch: 4, batch: 623, loss: 0.3273591697216034\n",
      "epoch: 4, batch: 624, loss: 0.17667794227600098\n",
      "epoch: 4, batch: 625, loss: 0.23225530982017517\n",
      "epoch: 4, batch: 626, loss: 0.2711474895477295\n",
      "epoch: 4, batch: 627, loss: 0.3984904885292053\n",
      "epoch: 4, batch: 628, loss: 0.3892028033733368\n",
      "epoch: 4, batch: 629, loss: 0.2206021398305893\n",
      "epoch: 4, batch: 630, loss: 0.43421632051467896\n",
      "epoch: 4, batch: 631, loss: 0.4114380478858948\n",
      "epoch: 4, batch: 632, loss: 0.17592106759548187\n",
      "epoch: 4, batch: 633, loss: 0.29241177439689636\n",
      "epoch: 4, batch: 634, loss: 0.16013243794441223\n",
      "epoch: 4, batch: 635, loss: 0.2875313460826874\n",
      "epoch: 4, batch: 636, loss: 0.34569278359413147\n",
      "epoch: 4, batch: 637, loss: 0.3341534435749054\n",
      "epoch: 4, batch: 638, loss: 0.2025453895330429\n",
      "epoch: 4, batch: 639, loss: 0.12253901362419128\n",
      "epoch: 4, batch: 640, loss: 0.17829981446266174\n",
      "epoch: 4, batch: 641, loss: 0.340010404586792\n",
      "epoch: 4, batch: 642, loss: 0.37256941199302673\n",
      "epoch: 4, batch: 643, loss: 0.4892551600933075\n",
      "epoch: 4, batch: 644, loss: 0.32480788230895996\n",
      "epoch: 4, batch: 645, loss: 0.5523455142974854\n",
      "epoch: 4, batch: 646, loss: 0.30828484892845154\n",
      "epoch: 4, batch: 647, loss: 0.20613563060760498\n",
      "epoch: 4, batch: 648, loss: 0.29581165313720703\n",
      "epoch: 4, batch: 649, loss: 0.30894047021865845\n",
      "epoch: 4, batch: 650, loss: 0.2971205711364746\n",
      "epoch: 4, batch: 651, loss: 0.36926084756851196\n",
      "epoch: 4, batch: 652, loss: 0.3308345079421997\n",
      "epoch: 4, batch: 653, loss: 0.15477976202964783\n",
      "epoch: 4, batch: 654, loss: 0.3532373309135437\n",
      "epoch: 4, batch: 655, loss: 0.21762529015541077\n",
      "epoch: 4, batch: 656, loss: 0.2629378139972687\n",
      "epoch: 4, batch: 657, loss: 0.48485371470451355\n",
      "epoch: 4, batch: 658, loss: 0.4953557848930359\n",
      "epoch: 4, batch: 659, loss: 0.35576361417770386\n",
      "epoch: 4, batch: 660, loss: 0.16072972118854523\n",
      "epoch: 4, batch: 661, loss: 0.18142813444137573\n",
      "epoch: 4, batch: 662, loss: 0.3450992703437805\n",
      "epoch: 4, batch: 663, loss: 0.3223632872104645\n",
      "epoch: 4, batch: 664, loss: 0.25511524081230164\n",
      "epoch: 4, batch: 665, loss: 0.34700751304626465\n",
      "epoch: 4, batch: 666, loss: 0.18550758063793182\n",
      "epoch: 4, batch: 667, loss: 0.29998457431793213\n",
      "epoch: 4, batch: 668, loss: 0.3483767807483673\n",
      "epoch: 4, batch: 669, loss: 0.2438044548034668\n",
      "epoch: 4, batch: 670, loss: 0.22854021191596985\n",
      "epoch: 4, batch: 671, loss: 0.319907009601593\n",
      "epoch: 4, batch: 672, loss: 0.233792245388031\n",
      "epoch: 4, batch: 673, loss: 0.29721832275390625\n",
      "epoch: 4, batch: 674, loss: 0.4219982326030731\n",
      "epoch: 4, batch: 675, loss: 0.31846803426742554\n",
      "epoch: 4, batch: 676, loss: 0.2685626447200775\n",
      "epoch: 4, batch: 677, loss: 0.20907466113567352\n",
      "epoch: 4, batch: 678, loss: 0.3373977243900299\n",
      "epoch: 4, batch: 679, loss: 0.31691062450408936\n",
      "epoch: 4, batch: 680, loss: 0.22572508454322815\n",
      "epoch: 4, batch: 681, loss: 0.24220575392246246\n",
      "epoch: 4, batch: 682, loss: 0.3618583679199219\n",
      "epoch: 4, batch: 683, loss: 0.2638351321220398\n",
      "epoch: 4, batch: 684, loss: 0.35041189193725586\n",
      "epoch: 4, batch: 685, loss: 0.2618211805820465\n",
      "epoch: 4, batch: 686, loss: 0.3562732934951782\n",
      "epoch: 4, batch: 687, loss: 0.35744014382362366\n",
      "epoch: 4, batch: 688, loss: 0.322412371635437\n",
      "epoch: 4, batch: 689, loss: 0.5031601786613464\n",
      "epoch: 4, batch: 690, loss: 0.3568230867385864\n",
      "epoch: 4, batch: 691, loss: 0.2089703232049942\n",
      "epoch: 4, batch: 692, loss: 0.5054351687431335\n",
      "epoch: 4, batch: 693, loss: 0.3322967290878296\n",
      "epoch: 4, batch: 694, loss: 0.37710869312286377\n",
      "epoch: 4, batch: 695, loss: 0.2644098103046417\n",
      "epoch: 4, batch: 696, loss: 0.2166673243045807\n",
      "epoch: 4, batch: 697, loss: 0.22478774189949036\n",
      "epoch: 4, batch: 698, loss: 0.42231613397598267\n",
      "epoch: 4, batch: 699, loss: 0.14898748695850372\n",
      "epoch: 4, batch: 700, loss: 0.10736072063446045\n",
      "epoch: 4, batch: 701, loss: 0.20429396629333496\n",
      "epoch: 4, batch: 702, loss: 0.455466628074646\n",
      "epoch: 4, batch: 703, loss: 0.3131944239139557\n",
      "epoch: 4, batch: 704, loss: 0.2862686514854431\n",
      "epoch: 4, batch: 705, loss: 0.3446636497974396\n",
      "epoch: 4, batch: 706, loss: 0.4015379846096039\n",
      "epoch: 4, batch: 707, loss: 0.48029088973999023\n",
      "epoch: 4, batch: 708, loss: 0.30155935883522034\n",
      "epoch: 4, batch: 709, loss: 0.34563636779785156\n",
      "epoch: 4, batch: 710, loss: 0.2390657663345337\n",
      "epoch: 4, batch: 711, loss: 0.2931184768676758\n",
      "epoch: 4, batch: 712, loss: 0.4927447736263275\n",
      "epoch: 4, batch: 713, loss: 0.349629282951355\n",
      "epoch: 4, batch: 714, loss: 0.24352800846099854\n",
      "epoch: 4, batch: 715, loss: 0.29906848073005676\n",
      "epoch: 4, batch: 716, loss: 0.22555693984031677\n",
      "epoch: 4, batch: 717, loss: 0.35394513607025146\n",
      "epoch: 4, batch: 718, loss: 0.0960201770067215\n",
      "epoch: 4, batch: 719, loss: 0.21732017397880554\n",
      "epoch: 4, batch: 720, loss: 0.376503050327301\n",
      "epoch: 4, batch: 721, loss: 0.38225021958351135\n",
      "epoch: 4, batch: 722, loss: 0.18799442052841187\n",
      "epoch: 4, batch: 723, loss: 0.31098854541778564\n",
      "epoch: 4, batch: 724, loss: 0.332792729139328\n",
      "epoch: 4, batch: 725, loss: 0.2656116485595703\n",
      "epoch: 4, batch: 726, loss: 0.3089188039302826\n",
      "epoch: 4, batch: 727, loss: 0.3270602524280548\n",
      "epoch: 4, batch: 728, loss: 0.24864834547042847\n",
      "epoch: 4, batch: 729, loss: 0.29707497358322144\n",
      "epoch: 4, batch: 730, loss: 0.27355289459228516\n",
      "epoch: 4, batch: 731, loss: 0.20854923129081726\n",
      "epoch: 4, batch: 732, loss: 0.20755857229232788\n",
      "epoch: 4, batch: 733, loss: 0.2375207096338272\n",
      "epoch: 4, batch: 734, loss: 0.25256919860839844\n",
      "epoch: 4, batch: 735, loss: 0.2000771164894104\n",
      "epoch: 4, batch: 736, loss: 0.13249725103378296\n",
      "epoch: 4, batch: 737, loss: 0.40817731618881226\n",
      "epoch: 4, batch: 738, loss: 0.35155877470970154\n",
      "epoch: 4, batch: 739, loss: 0.2444618195295334\n",
      "epoch: 4, batch: 740, loss: 0.31141701340675354\n",
      "epoch: 4, batch: 741, loss: 0.24157391488552094\n",
      "epoch: 4, batch: 742, loss: 0.2400800585746765\n",
      "epoch: 4, batch: 743, loss: 0.3383565843105316\n",
      "epoch: 4, batch: 744, loss: 0.16212555766105652\n",
      "epoch: 4, batch: 745, loss: 0.5926043391227722\n",
      "epoch: 4, batch: 746, loss: 0.33730408549308777\n",
      "epoch: 4, batch: 747, loss: 0.34301939606666565\n",
      "epoch: 4, batch: 748, loss: 0.2974303662776947\n",
      "epoch: 4, batch: 749, loss: 0.2868855595588684\n",
      "epoch: 4, batch: 750, loss: 0.2582484781742096\n",
      "epoch: 4, batch: 751, loss: 0.2278153896331787\n",
      "epoch: 4, batch: 752, loss: 0.26425889134407043\n",
      "epoch: 4, batch: 753, loss: 0.25004154443740845\n",
      "epoch: 4, batch: 754, loss: 0.34559985995292664\n",
      "epoch: 4, batch: 755, loss: 0.3275759816169739\n",
      "epoch: 4, batch: 756, loss: 0.1772935539484024\n",
      "epoch: 4, batch: 757, loss: 0.258505254983902\n",
      "epoch: 4, batch: 758, loss: 0.2646815776824951\n",
      "epoch: 4, batch: 759, loss: 0.23739400506019592\n",
      "epoch: 4, batch: 760, loss: 0.2211480289697647\n",
      "epoch: 4, batch: 761, loss: 0.3172498345375061\n",
      "epoch: 4, batch: 762, loss: 0.3539780080318451\n",
      "epoch: 4, batch: 763, loss: 0.31273430585861206\n",
      "epoch: 4, batch: 764, loss: 0.2890275716781616\n",
      "epoch: 4, batch: 765, loss: 0.31485095620155334\n",
      "epoch: 4, batch: 766, loss: 0.3033630847930908\n",
      "epoch: 4, batch: 767, loss: 0.21167895197868347\n",
      "epoch: 4, batch: 768, loss: 0.29440778493881226\n",
      "epoch: 4, batch: 769, loss: 0.329214483499527\n",
      "epoch: 4, batch: 770, loss: 0.436853289604187\n",
      "epoch: 4, batch: 771, loss: 0.3385677933692932\n",
      "epoch: 4, batch: 772, loss: 0.3477362394332886\n",
      "epoch: 4, batch: 773, loss: 0.13856291770935059\n",
      "epoch: 4, batch: 774, loss: 0.3595033884048462\n",
      "epoch: 4, batch: 775, loss: 0.4170469641685486\n",
      "epoch: 4, batch: 776, loss: 0.22187542915344238\n",
      "epoch: 4, batch: 777, loss: 0.18666864931583405\n",
      "epoch: 4, batch: 778, loss: 0.4200810194015503\n",
      "epoch: 4, batch: 779, loss: 0.3611382246017456\n",
      "epoch: 4, batch: 780, loss: 0.23906704783439636\n",
      "epoch: 4, batch: 781, loss: 0.27987781167030334\n",
      "epoch: 4, batch: 782, loss: 0.2837907075881958\n",
      "epoch: 4, batch: 783, loss: 0.27108997106552124\n",
      "epoch: 4, batch: 784, loss: 0.5235587358474731\n",
      "epoch: 4, batch: 785, loss: 0.35944944620132446\n",
      "epoch: 4, batch: 786, loss: 0.17661799490451813\n",
      "epoch: 4, batch: 787, loss: 0.3407168984413147\n",
      "epoch: 4, batch: 788, loss: 0.26580190658569336\n",
      "epoch: 4, batch: 789, loss: 0.306488037109375\n",
      "epoch: 4, batch: 790, loss: 0.2227996587753296\n",
      "epoch: 4, batch: 791, loss: 0.43062904477119446\n",
      "epoch: 4, batch: 792, loss: 0.2127557247877121\n",
      "epoch: 4, batch: 793, loss: 0.30881041288375854\n",
      "epoch: 4, batch: 794, loss: 0.35109931230545044\n",
      "epoch: 4, batch: 795, loss: 0.21387135982513428\n",
      "epoch: 4, batch: 796, loss: 0.329740971326828\n",
      "epoch: 4, batch: 797, loss: 0.33031585812568665\n",
      "epoch: 4, batch: 798, loss: 0.32487213611602783\n",
      "epoch: 4, batch: 799, loss: 0.19005289673805237\n",
      "epoch: 4, batch: 800, loss: 0.44905921816825867\n",
      "epoch: 4, batch: 801, loss: 0.4022256135940552\n",
      "epoch: 4, batch: 802, loss: 0.4607788026332855\n",
      "epoch: 4, batch: 803, loss: 0.25970304012298584\n",
      "epoch: 4, batch: 804, loss: 0.3168998062610626\n",
      "epoch: 4, batch: 805, loss: 0.35110723972320557\n",
      "epoch: 4, batch: 806, loss: 0.349941611289978\n",
      "epoch: 4, batch: 807, loss: 0.3167974352836609\n",
      "epoch: 4, batch: 808, loss: 0.23002153635025024\n",
      "epoch: 4, batch: 809, loss: 0.35078033804893494\n",
      "epoch: 4, batch: 810, loss: 0.31497371196746826\n",
      "epoch: 4, batch: 811, loss: 0.39611300826072693\n",
      "epoch: 4, batch: 812, loss: 0.2004312425851822\n",
      "epoch: 4, batch: 813, loss: 0.3062726855278015\n",
      "epoch: 4, batch: 814, loss: 0.20231010019779205\n",
      "epoch: 4, batch: 815, loss: 0.24221108853816986\n",
      "epoch: 4, batch: 816, loss: 0.31762242317199707\n",
      "epoch: 4, batch: 817, loss: 0.32631567120552063\n",
      "epoch: 4, batch: 818, loss: 0.23534834384918213\n",
      "epoch: 4, batch: 819, loss: 0.14273804426193237\n",
      "epoch: 4, batch: 820, loss: 0.27872124314308167\n",
      "epoch: 4, batch: 821, loss: 0.3916241526603699\n",
      "epoch: 4, batch: 822, loss: 0.1827886551618576\n",
      "epoch: 4, batch: 823, loss: 0.360228568315506\n",
      "epoch: 4, batch: 824, loss: 0.1504143476486206\n",
      "epoch: 4, batch: 825, loss: 0.20772996544837952\n",
      "epoch: 4, batch: 826, loss: 0.2126326560974121\n",
      "epoch: 4, batch: 827, loss: 0.25786256790161133\n",
      "epoch: 4, batch: 828, loss: 0.3679907023906708\n",
      "epoch: 4, batch: 829, loss: 0.20711220800876617\n",
      "epoch: 4, batch: 830, loss: 0.2987402081489563\n",
      "epoch: 4, batch: 831, loss: 0.3207709491252899\n",
      "epoch: 4, batch: 832, loss: 0.22406429052352905\n",
      "epoch: 4, batch: 833, loss: 0.38082602620124817\n",
      "epoch: 4, batch: 834, loss: 0.34380042552948\n",
      "epoch: 4, batch: 835, loss: 0.3993258476257324\n",
      "epoch: 4, batch: 836, loss: 0.1495598703622818\n",
      "epoch: 4, batch: 837, loss: 0.28159278631210327\n",
      "epoch: 4, batch: 838, loss: 0.34664395451545715\n",
      "epoch: 4, batch: 839, loss: 0.40334460139274597\n",
      "epoch: 4, batch: 840, loss: 0.3510616719722748\n",
      "epoch: 4, batch: 841, loss: 0.36354860663414\n",
      "epoch: 4, batch: 842, loss: 0.09273944795131683\n",
      "epoch: 4, batch: 843, loss: 0.2502635717391968\n",
      "epoch: 4, batch: 844, loss: 0.35098156332969666\n",
      "epoch: 4, batch: 845, loss: 0.3102201521396637\n",
      "epoch: 4, batch: 846, loss: 0.4433571398258209\n",
      "epoch: 4, batch: 847, loss: 0.23133084177970886\n",
      "epoch: 4, batch: 848, loss: 0.23884756863117218\n",
      "epoch: 4, batch: 849, loss: 0.2655709683895111\n",
      "epoch: 4, batch: 850, loss: 0.2928098440170288\n",
      "epoch: 4, batch: 851, loss: 0.2388264536857605\n",
      "epoch: 4, batch: 852, loss: 0.34360331296920776\n",
      "epoch: 4, batch: 853, loss: 0.2570374310016632\n",
      "epoch: 4, batch: 854, loss: 0.19529591500759125\n",
      "epoch: 4, batch: 855, loss: 0.34509292244911194\n",
      "epoch: 4, batch: 856, loss: 0.3245375454425812\n",
      "epoch: 4, batch: 857, loss: 0.16987402737140656\n",
      "epoch: 4, batch: 858, loss: 0.2723660171031952\n",
      "epoch: 4, batch: 859, loss: 0.23356914520263672\n",
      "epoch: 4, batch: 860, loss: 0.319955050945282\n",
      "epoch: 4, batch: 861, loss: 0.27771449089050293\n",
      "epoch: 4, batch: 862, loss: 0.25489580631256104\n",
      "epoch: 4, batch: 863, loss: 0.2816114127635956\n",
      "epoch: 4, batch: 864, loss: 0.23440870642662048\n",
      "epoch: 4, batch: 865, loss: 0.32870686054229736\n",
      "epoch: 4, batch: 866, loss: 0.45078450441360474\n",
      "epoch: 4, batch: 867, loss: 0.3579546809196472\n",
      "epoch: 4, batch: 868, loss: 0.3359813094139099\n",
      "epoch: 4, batch: 869, loss: 0.2620866596698761\n",
      "epoch: 4, batch: 870, loss: 0.32634416222572327\n",
      "epoch: 4, batch: 871, loss: 0.21574667096138\n",
      "epoch: 4, batch: 872, loss: 0.1865740418434143\n",
      "epoch: 4, batch: 873, loss: 0.21728189289569855\n",
      "epoch: 4, batch: 874, loss: 0.22217823565006256\n",
      "epoch: 4, batch: 875, loss: 0.32685306668281555\n",
      "epoch: 4, batch: 876, loss: 0.21425390243530273\n",
      "epoch: 4, batch: 877, loss: 0.3479877710342407\n",
      "epoch: 4, batch: 878, loss: 0.2086472511291504\n",
      "epoch: 4, batch: 879, loss: 0.3333911895751953\n",
      "epoch: 4, batch: 880, loss: 0.4025559723377228\n",
      "epoch: 4, batch: 881, loss: 0.2952519357204437\n",
      "epoch: 4, batch: 882, loss: 0.3283846974372864\n",
      "epoch: 4, batch: 883, loss: 0.4338058531284332\n",
      "epoch: 4, batch: 884, loss: 0.2733435034751892\n",
      "epoch: 4, batch: 885, loss: 0.33169564604759216\n",
      "epoch: 4, batch: 886, loss: 0.317542165517807\n",
      "epoch: 4, batch: 887, loss: 0.3221169710159302\n",
      "epoch: 4, batch: 888, loss: 0.31378015875816345\n",
      "epoch: 4, batch: 889, loss: 0.2619085907936096\n",
      "epoch: 4, batch: 890, loss: 0.30317366123199463\n",
      "epoch: 4, batch: 891, loss: 0.3405778110027313\n",
      "epoch: 4, batch: 892, loss: 0.473936527967453\n",
      "epoch: 4, batch: 893, loss: 0.16472698748111725\n",
      "epoch: 4, batch: 894, loss: 0.16895908117294312\n",
      "epoch: 4, batch: 895, loss: 0.4244160056114197\n",
      "epoch: 4, batch: 896, loss: 0.15191489458084106\n",
      "epoch: 4, batch: 897, loss: 0.2021310180425644\n",
      "epoch: 4, batch: 898, loss: 0.3331480324268341\n",
      "epoch: 4, batch: 899, loss: 0.2804078757762909\n",
      "epoch: 4, batch: 900, loss: 0.3512348234653473\n",
      "epoch: 4, batch: 901, loss: 0.35617291927337646\n",
      "epoch: 4, batch: 902, loss: 0.443053275346756\n",
      "epoch: 4, batch: 903, loss: 0.21395665407180786\n",
      "epoch: 4, batch: 904, loss: 0.1564055234193802\n",
      "epoch: 4, batch: 905, loss: 0.3111402690410614\n",
      "epoch: 4, batch: 906, loss: 0.17649567127227783\n",
      "epoch: 4, batch: 907, loss: 0.3217104971408844\n",
      "epoch: 4, batch: 908, loss: 0.40689730644226074\n",
      "epoch: 4, batch: 909, loss: 0.20376019179821014\n",
      "epoch: 4, batch: 910, loss: 0.35961097478866577\n",
      "epoch: 4, batch: 911, loss: 0.14808881282806396\n",
      "epoch: 4, batch: 912, loss: 0.2044796198606491\n",
      "epoch: 4, batch: 913, loss: 0.20101958513259888\n",
      "epoch: 4, batch: 914, loss: 0.35948410630226135\n",
      "epoch: 4, batch: 915, loss: 0.254232257604599\n",
      "epoch: 4, batch: 916, loss: 0.32710736989974976\n",
      "epoch: 4, batch: 917, loss: 0.21568474173545837\n",
      "epoch: 4, batch: 918, loss: 0.2252499759197235\n",
      "epoch: 4, batch: 919, loss: 0.2072250247001648\n",
      "epoch: 4, batch: 920, loss: 0.44174253940582275\n",
      "epoch: 4, batch: 921, loss: 0.19710317254066467\n",
      "epoch: 4, batch: 922, loss: 0.5044664144515991\n",
      "epoch: 4, batch: 923, loss: 0.2761993110179901\n",
      "epoch: 4, batch: 924, loss: 0.15803708136081696\n",
      "epoch: 4, batch: 925, loss: 0.15807516872882843\n",
      "epoch: 4, batch: 926, loss: 0.2671360969543457\n",
      "epoch: 4, batch: 927, loss: 0.25656652450561523\n",
      "epoch: 4, batch: 928, loss: 0.3243890404701233\n",
      "epoch: 4, batch: 929, loss: 0.24625128507614136\n",
      "epoch: 4, batch: 930, loss: 0.26104703545570374\n",
      "epoch: 4, batch: 931, loss: 0.22833678126335144\n",
      "epoch: 4, batch: 932, loss: 0.25240615010261536\n",
      "epoch: 4, batch: 933, loss: 0.22719690203666687\n",
      "epoch: 4, batch: 934, loss: 0.3730524182319641\n",
      "epoch: 4, batch: 935, loss: 0.3491515815258026\n",
      "epoch: 4, batch: 936, loss: 0.35177671909332275\n",
      "epoch: 4, batch: 937, loss: 0.1858530342578888\n",
      "CPU times: total: 55.6 s\n",
      "Wall time: 47.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# get model to cuda if possible\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, target) in enumerate((train_datloader)):\n",
    "\n",
    "        # get data to cuda if possible\n",
    "        data = data.to(device)\n",
    "        # Squeeze the dim of data from (B X 1 X 28 X 28) to (B X 28 X 28)\n",
    "        data = data.squeeze(dim=1)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # feed forward the data to model\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, target)\n",
    "        print(f\"epoch: {epoch}, batch: {batch_idx}, loss: {loss}\")\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad() # This will flush the gradients from the last iteration\n",
    "        loss.backward()\n",
    "\n",
    "        # optimise the loss (gradient descent or Adam step)\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check accuracy on train and test data (Validate model accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy on training & test to see how good our model\n",
    "def check_accuracy(loader, model):\n",
    "    \"\"\"\n",
    "    Check accuracy of our trained model given a loader and a model\n",
    "\n",
    "    Parameters:\n",
    "        loader: torch.utils.data.DataLoader\n",
    "            A loader for the dataset you want to check accuracy on\n",
    "        model: nn.Module\n",
    "            The model you want to check accuracy on\n",
    "\n",
    "    Returns:\n",
    "        acc: float\n",
    "            The accuracy of the model on the dataset given by the loader\n",
    "    \"\"\"\n",
    "\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "\n",
    "    # We don't need to keep track of gradients here so we wrap it in torch.no_grad()\n",
    "    with torch.no_grad():\n",
    "        # Loop through the data\n",
    "        for x, y in loader:\n",
    "            # Move data to device\n",
    "            x = x.to(device=device)\n",
    "            # Squeeze the dim of x from (B X 1 X 28 X 28) to (B X 28 X 28)\n",
    "            x = x.squeeze(dim=1)\n",
    "            y = y.to(device=device)\n",
    "\n",
    "            # Forward pass\n",
    "            scores = model(x)\n",
    "            predictions = scores.argmax(1)\n",
    "\n",
    "            # Check how many we got correct\n",
    "            num_correct += (predictions == y).sum()\n",
    "\n",
    "            # Keep track of number of samples\n",
    "            num_samples += predictions.size(0)\n",
    "\n",
    "    model.train()\n",
    "    return num_correct / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 90.65\n",
      "Accuracy on test set: 90.64\n"
     ]
    }
   ],
   "source": [
    "# Check accuracy on training & test to see how good our model\n",
    "print(f\"Accuracy on training set: {check_accuracy(train_datloader, model)*100:.2f}\")\n",
    "print(f\"Accuracy on test set: {check_accuracy(test_datloader, model)*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
