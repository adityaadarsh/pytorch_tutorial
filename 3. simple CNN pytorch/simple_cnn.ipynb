{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Course Reference: https://learn.microsoft.com/en-us/training/modules/intro-machine-learning-pytorch/8-quickstart\n",
    "#### Learning objectives\n",
    "\n",
    "In this module you will:\n",
    "\n",
    "* Learn how create a asimple CNN using pytorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transformations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a CNN Network \n",
    "\n",
    "We define our neural network by subclassing nn.Module, and initialize the neural network layers in __init__. Every nn.Module subclass implements the operations on input data in the forward method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import nn module\n",
    "# Init super class\n",
    "# input  -> input channels\n",
    "# design a CONV layer\n",
    "# Check output size at each layer using pseudo forwaard\n",
    "# Add a max pool layer\n",
    "# what is the output of the maxpool?\n",
    "# conv 2\n",
    "# Fc1\n",
    "# Forward pass operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCnn(nn.Module):\n",
    "    def __init__(self, input_channel, output_class):\n",
    "        super(SimpleCnn, self).__init__()\n",
    "\n",
    "        # CNN layers\n",
    "        self.conv_layer_1 = nn.Conv2d(\n",
    "            in_channels=input_channel,\n",
    "            out_channels=8,\n",
    "            kernel_size=(3, 3),\n",
    "            padding=(1, 1),\n",
    "            stride=(1, 1),\n",
    "        )\n",
    "\n",
    "        self.pool_layer = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "\n",
    "        self.conv_layer_2 = nn.Conv2d(\n",
    "            in_channels=8,\n",
    "            out_channels=16,\n",
    "            kernel_size=(3, 3),\n",
    "            padding=(1, 1),\n",
    "            stride=(1, 1),\n",
    "        )\n",
    "    \n",
    "        self.fc1 = nn.Linear(in_features=16*7*7, out_features=output_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv_layer_1(x))\n",
    "        x = self.pool_layer(x)\n",
    "        x = F.relu(self.conv_layer_2(x))\n",
    "        x = self.pool_layer(x)\n",
    "        x = x.flatten(1,-1)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60, 1, 28, 28])\n",
      "torch.Size([60, 10])\n"
     ]
    }
   ],
   "source": [
    "# check the network graph\n",
    "model = SimpleCnn(1, 10)\n",
    "\n",
    "# create a random variable and pass it to the model to check the network graph\n",
    "x = torch.randn(60, 1, 28, 28)\n",
    "print(x.shape)\n",
    "print(model(x).shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_channel = 1\n",
    "n_class = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and load the data from pytorch sample datasets\n",
    "# https://pytorch.org/vision/0.8/datasets.html\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    root=\"../dataset/\", train=True, transform=transformations.ToTensor(), download=True\n",
    ")\n",
    "train_datloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels batch shape: torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 3\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Understand the dataloader\n",
    "plt.title(\"sample image\")\n",
    "# Display image and label.\n",
    "train_features, train_labels = next(iter(train_datloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset shape: torch.Size([60000, 28, 28])\n",
      "['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n"
     ]
    }
   ],
   "source": [
    "# train data shape and class labels\n",
    "print(\"train_dataset shape:\", train_dataset.data.shape)\n",
    "print(train_dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_dataset shape: torch.Size([10000, 28, 28])\n",
      "['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n"
     ]
    }
   ],
   "source": [
    "test_dataset = datasets.MNIST(\n",
    "    root=\"../dataset/\", train=False, transform=transformations.ToTensor(), download=True\n",
    ")\n",
    "test_datloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(\"test_dataset shape:\", test_dataset.data.shape)\n",
    "print(test_dataset.classes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleCnn(\n",
      "  (conv_layer_1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool_layer): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv_layer_2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=784, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = SimpleCnn(input_channel, n_class)\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise Loss And Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps to train the model\n",
    "1. For each epoch, iterate through the batch\n",
    "2. For each batch\n",
    "    * feed forward the input and target data of train to the model\n",
    "    * Calculate the loss and score\n",
    "    * Backpropogate the loss\n",
    "    * optimise the loss using optimiser() (gradient descent is one such optimiser) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch: 0, loss: 2.29600191116333\n",
      "epoch: 0, batch: 1, loss: 2.2864458560943604\n",
      "epoch: 0, batch: 2, loss: 2.3019285202026367\n",
      "epoch: 0, batch: 3, loss: 2.3110599517822266\n",
      "epoch: 0, batch: 4, loss: 2.295490026473999\n",
      "epoch: 0, batch: 5, loss: 2.296154260635376\n",
      "epoch: 0, batch: 6, loss: 2.286017417907715\n",
      "epoch: 0, batch: 7, loss: 2.260921001434326\n",
      "epoch: 0, batch: 8, loss: 2.2701685428619385\n",
      "epoch: 0, batch: 9, loss: 2.2430338859558105\n",
      "epoch: 0, batch: 10, loss: 2.2481653690338135\n",
      "epoch: 0, batch: 11, loss: 2.24849009513855\n",
      "epoch: 0, batch: 12, loss: 2.221688747406006\n",
      "epoch: 0, batch: 13, loss: 2.2503716945648193\n",
      "epoch: 0, batch: 14, loss: 2.224527597427368\n",
      "epoch: 0, batch: 15, loss: 2.2003748416900635\n",
      "epoch: 0, batch: 16, loss: 2.189391613006592\n",
      "epoch: 0, batch: 17, loss: 2.18888783454895\n",
      "epoch: 0, batch: 18, loss: 2.1638879776000977\n",
      "epoch: 0, batch: 19, loss: 2.1706151962280273\n",
      "epoch: 0, batch: 20, loss: 2.129042625427246\n",
      "epoch: 0, batch: 21, loss: 2.14713978767395\n",
      "epoch: 0, batch: 22, loss: 2.1251938343048096\n",
      "epoch: 0, batch: 23, loss: 2.0691843032836914\n",
      "epoch: 0, batch: 24, loss: 2.0256617069244385\n",
      "epoch: 0, batch: 25, loss: 2.028257131576538\n",
      "epoch: 0, batch: 26, loss: 2.01212215423584\n",
      "epoch: 0, batch: 27, loss: 2.0054891109466553\n",
      "epoch: 0, batch: 28, loss: 1.9513376951217651\n",
      "epoch: 0, batch: 29, loss: 2.0096607208251953\n",
      "epoch: 0, batch: 30, loss: 1.942038893699646\n",
      "epoch: 0, batch: 31, loss: 1.9192852973937988\n",
      "epoch: 0, batch: 32, loss: 1.9430820941925049\n",
      "epoch: 0, batch: 33, loss: 1.863980770111084\n",
      "epoch: 0, batch: 34, loss: 1.8604116439819336\n",
      "epoch: 0, batch: 35, loss: 1.7790147066116333\n",
      "epoch: 0, batch: 36, loss: 1.7779760360717773\n",
      "epoch: 0, batch: 37, loss: 1.6675570011138916\n",
      "epoch: 0, batch: 38, loss: 1.6839205026626587\n",
      "epoch: 0, batch: 39, loss: 1.653571367263794\n",
      "epoch: 0, batch: 40, loss: 1.5365244150161743\n",
      "epoch: 0, batch: 41, loss: 1.514511227607727\n",
      "epoch: 0, batch: 42, loss: 1.5759063959121704\n",
      "epoch: 0, batch: 43, loss: 1.678518295288086\n",
      "epoch: 0, batch: 44, loss: 1.4199647903442383\n",
      "epoch: 0, batch: 45, loss: 1.3719557523727417\n",
      "epoch: 0, batch: 46, loss: 1.3786966800689697\n",
      "epoch: 0, batch: 47, loss: 1.4524706602096558\n",
      "epoch: 0, batch: 48, loss: 1.413134217262268\n",
      "epoch: 0, batch: 49, loss: 1.1104090213775635\n",
      "epoch: 0, batch: 50, loss: 1.1610002517700195\n",
      "epoch: 0, batch: 51, loss: 1.2173380851745605\n",
      "epoch: 0, batch: 52, loss: 1.0941036939620972\n",
      "epoch: 0, batch: 53, loss: 1.1542894840240479\n",
      "epoch: 0, batch: 54, loss: 1.0655620098114014\n",
      "epoch: 0, batch: 55, loss: 1.084704041481018\n",
      "epoch: 0, batch: 56, loss: 1.1222312450408936\n",
      "epoch: 0, batch: 57, loss: 0.9135311841964722\n",
      "epoch: 0, batch: 58, loss: 0.8730984926223755\n",
      "epoch: 0, batch: 59, loss: 0.9323734045028687\n",
      "epoch: 0, batch: 60, loss: 0.775364100933075\n",
      "epoch: 0, batch: 61, loss: 0.906909167766571\n",
      "epoch: 0, batch: 62, loss: 0.9811554551124573\n",
      "epoch: 0, batch: 63, loss: 0.8777105212211609\n",
      "epoch: 0, batch: 64, loss: 0.7449252009391785\n",
      "epoch: 0, batch: 65, loss: 0.7512496709823608\n",
      "epoch: 0, batch: 66, loss: 0.7543399930000305\n",
      "epoch: 0, batch: 67, loss: 0.7226375937461853\n",
      "epoch: 0, batch: 68, loss: 0.620858371257782\n",
      "epoch: 0, batch: 69, loss: 0.5524749755859375\n",
      "epoch: 0, batch: 70, loss: 0.5963271856307983\n",
      "epoch: 0, batch: 71, loss: 0.6455803513526917\n",
      "epoch: 0, batch: 72, loss: 0.7167067527770996\n",
      "epoch: 0, batch: 73, loss: 0.5148475170135498\n",
      "epoch: 0, batch: 74, loss: 0.4977293312549591\n",
      "epoch: 0, batch: 75, loss: 0.5210347175598145\n",
      "epoch: 0, batch: 76, loss: 0.7446874976158142\n",
      "epoch: 0, batch: 77, loss: 0.7084974050521851\n",
      "epoch: 0, batch: 78, loss: 0.5917644500732422\n",
      "epoch: 0, batch: 79, loss: 0.5660789608955383\n",
      "epoch: 0, batch: 80, loss: 0.635254442691803\n",
      "epoch: 0, batch: 81, loss: 0.4785655736923218\n",
      "epoch: 0, batch: 82, loss: 0.6819292306900024\n",
      "epoch: 0, batch: 83, loss: 0.647493302822113\n",
      "epoch: 0, batch: 84, loss: 0.637089192867279\n",
      "epoch: 0, batch: 85, loss: 0.6134856939315796\n",
      "epoch: 0, batch: 86, loss: 0.7731726169586182\n",
      "epoch: 0, batch: 87, loss: 0.39609742164611816\n",
      "epoch: 0, batch: 88, loss: 0.5148225426673889\n",
      "epoch: 0, batch: 89, loss: 0.44416937232017517\n",
      "epoch: 0, batch: 90, loss: 0.6622546315193176\n",
      "epoch: 0, batch: 91, loss: 0.6629916429519653\n",
      "epoch: 0, batch: 92, loss: 0.5463548898696899\n",
      "epoch: 0, batch: 93, loss: 0.44332829117774963\n",
      "epoch: 0, batch: 94, loss: 0.3753392994403839\n",
      "epoch: 0, batch: 95, loss: 0.5466700792312622\n",
      "epoch: 0, batch: 96, loss: 0.8231250643730164\n",
      "epoch: 0, batch: 97, loss: 0.4036227762699127\n",
      "epoch: 0, batch: 98, loss: 0.44939154386520386\n",
      "epoch: 0, batch: 99, loss: 0.6005843877792358\n",
      "epoch: 0, batch: 100, loss: 0.46919122338294983\n",
      "epoch: 0, batch: 101, loss: 0.5482392907142639\n",
      "epoch: 0, batch: 102, loss: 0.4685065746307373\n",
      "epoch: 0, batch: 103, loss: 0.38698822259902954\n",
      "epoch: 0, batch: 104, loss: 0.5518396496772766\n",
      "epoch: 0, batch: 105, loss: 0.5816650986671448\n",
      "epoch: 0, batch: 106, loss: 0.6409280896186829\n",
      "epoch: 0, batch: 107, loss: 0.3571307361125946\n",
      "epoch: 0, batch: 108, loss: 0.5026225447654724\n",
      "epoch: 0, batch: 109, loss: 0.34501686692237854\n",
      "epoch: 0, batch: 110, loss: 0.7194820642471313\n",
      "epoch: 0, batch: 111, loss: 0.5685549974441528\n",
      "epoch: 0, batch: 112, loss: 0.6435537934303284\n",
      "epoch: 0, batch: 113, loss: 0.48561376333236694\n",
      "epoch: 0, batch: 114, loss: 0.4200383126735687\n",
      "epoch: 0, batch: 115, loss: 0.3560210168361664\n",
      "epoch: 0, batch: 116, loss: 0.6480379700660706\n",
      "epoch: 0, batch: 117, loss: 0.2794845402240753\n",
      "epoch: 0, batch: 118, loss: 0.6152880787849426\n",
      "epoch: 0, batch: 119, loss: 0.3086044192314148\n",
      "epoch: 0, batch: 120, loss: 0.6868031620979309\n",
      "epoch: 0, batch: 121, loss: 0.48125261068344116\n",
      "epoch: 0, batch: 122, loss: 0.4609561562538147\n",
      "epoch: 0, batch: 123, loss: 0.44796571135520935\n",
      "epoch: 0, batch: 124, loss: 0.35131731629371643\n",
      "epoch: 0, batch: 125, loss: 0.40248972177505493\n",
      "epoch: 0, batch: 126, loss: 0.41873812675476074\n",
      "epoch: 0, batch: 127, loss: 0.489566832780838\n",
      "epoch: 0, batch: 128, loss: 0.46501874923706055\n",
      "epoch: 0, batch: 129, loss: 0.6889504790306091\n",
      "epoch: 0, batch: 130, loss: 0.4943922758102417\n",
      "epoch: 0, batch: 131, loss: 0.3401152193546295\n",
      "epoch: 0, batch: 132, loss: 0.4836518466472626\n",
      "epoch: 0, batch: 133, loss: 0.4325520992279053\n",
      "epoch: 0, batch: 134, loss: 0.4749634563922882\n",
      "epoch: 0, batch: 135, loss: 0.42059603333473206\n",
      "epoch: 0, batch: 136, loss: 0.39031267166137695\n",
      "epoch: 0, batch: 137, loss: 0.4899662733078003\n",
      "epoch: 0, batch: 138, loss: 0.4278584122657776\n",
      "epoch: 0, batch: 139, loss: 0.43616631627082825\n",
      "epoch: 0, batch: 140, loss: 0.36476659774780273\n",
      "epoch: 0, batch: 141, loss: 0.5623855590820312\n",
      "epoch: 0, batch: 142, loss: 0.32676076889038086\n",
      "epoch: 0, batch: 143, loss: 0.5667561888694763\n",
      "epoch: 0, batch: 144, loss: 0.5012224316596985\n",
      "epoch: 0, batch: 145, loss: 0.3834266662597656\n",
      "epoch: 0, batch: 146, loss: 0.5264512300491333\n",
      "epoch: 0, batch: 147, loss: 0.3507784903049469\n",
      "epoch: 0, batch: 148, loss: 0.4702068567276001\n",
      "epoch: 0, batch: 149, loss: 0.24114342033863068\n",
      "epoch: 0, batch: 150, loss: 0.5391055345535278\n",
      "epoch: 0, batch: 151, loss: 0.3249013125896454\n",
      "epoch: 0, batch: 152, loss: 0.3680250942707062\n",
      "epoch: 0, batch: 153, loss: 0.29794538021087646\n",
      "epoch: 0, batch: 154, loss: 0.42040473222732544\n",
      "epoch: 0, batch: 155, loss: 0.35023191571235657\n",
      "epoch: 0, batch: 156, loss: 0.31615149974823\n",
      "epoch: 0, batch: 157, loss: 0.4599616527557373\n",
      "epoch: 0, batch: 158, loss: 0.2582768499851227\n",
      "epoch: 0, batch: 159, loss: 0.37049388885498047\n",
      "epoch: 0, batch: 160, loss: 0.3225872814655304\n",
      "epoch: 0, batch: 161, loss: 0.46162208914756775\n",
      "epoch: 0, batch: 162, loss: 0.3924963176250458\n",
      "epoch: 0, batch: 163, loss: 0.6651339530944824\n",
      "epoch: 0, batch: 164, loss: 0.27215903997421265\n",
      "epoch: 0, batch: 165, loss: 0.29329338669776917\n",
      "epoch: 0, batch: 166, loss: 0.2611907720565796\n",
      "epoch: 0, batch: 167, loss: 0.26686930656433105\n",
      "epoch: 0, batch: 168, loss: 0.3804457187652588\n",
      "epoch: 0, batch: 169, loss: 0.2406255304813385\n",
      "epoch: 0, batch: 170, loss: 0.3313602805137634\n",
      "epoch: 0, batch: 171, loss: 0.41680169105529785\n",
      "epoch: 0, batch: 172, loss: 0.3879322409629822\n",
      "epoch: 0, batch: 173, loss: 0.24523501098155975\n",
      "epoch: 0, batch: 174, loss: 0.19822357594966888\n",
      "epoch: 0, batch: 175, loss: 0.3005216121673584\n",
      "epoch: 0, batch: 176, loss: 0.3439944386482239\n",
      "epoch: 0, batch: 177, loss: 0.29868200421333313\n",
      "epoch: 0, batch: 178, loss: 0.2755323052406311\n",
      "epoch: 0, batch: 179, loss: 0.25203585624694824\n",
      "epoch: 0, batch: 180, loss: 0.17414887249469757\n",
      "epoch: 0, batch: 181, loss: 0.42393097281455994\n",
      "epoch: 0, batch: 182, loss: 0.28077319264411926\n",
      "epoch: 0, batch: 183, loss: 0.17314130067825317\n",
      "epoch: 0, batch: 184, loss: 0.2528272569179535\n",
      "epoch: 0, batch: 185, loss: 0.233018696308136\n",
      "epoch: 0, batch: 186, loss: 0.4588344693183899\n",
      "epoch: 0, batch: 187, loss: 0.25426676869392395\n",
      "epoch: 0, batch: 188, loss: 0.564094066619873\n",
      "epoch: 0, batch: 189, loss: 0.29660481214523315\n",
      "epoch: 0, batch: 190, loss: 0.2939574420452118\n",
      "epoch: 0, batch: 191, loss: 0.3694342076778412\n",
      "epoch: 0, batch: 192, loss: 0.26611340045928955\n",
      "epoch: 0, batch: 193, loss: 0.5371333956718445\n",
      "epoch: 0, batch: 194, loss: 0.37485820055007935\n",
      "epoch: 0, batch: 195, loss: 0.30141526460647583\n",
      "epoch: 0, batch: 196, loss: 0.1707627922296524\n",
      "epoch: 0, batch: 197, loss: 0.3474908173084259\n",
      "epoch: 0, batch: 198, loss: 0.42776474356651306\n",
      "epoch: 0, batch: 199, loss: 0.28365978598594666\n",
      "epoch: 0, batch: 200, loss: 0.29404354095458984\n",
      "epoch: 0, batch: 201, loss: 0.20365332067012787\n",
      "epoch: 0, batch: 202, loss: 0.4474252462387085\n",
      "epoch: 0, batch: 203, loss: 0.32120102643966675\n",
      "epoch: 0, batch: 204, loss: 0.20793883502483368\n",
      "epoch: 0, batch: 205, loss: 0.302715539932251\n",
      "epoch: 0, batch: 206, loss: 0.30277591943740845\n",
      "epoch: 0, batch: 207, loss: 0.2857520878314972\n",
      "epoch: 0, batch: 208, loss: 0.421487033367157\n",
      "epoch: 0, batch: 209, loss: 0.34088507294654846\n",
      "epoch: 0, batch: 210, loss: 0.2751297056674957\n",
      "epoch: 0, batch: 211, loss: 0.3630138635635376\n",
      "epoch: 0, batch: 212, loss: 0.48458144068717957\n",
      "epoch: 0, batch: 213, loss: 0.19771665334701538\n",
      "epoch: 0, batch: 214, loss: 0.3443155884742737\n",
      "epoch: 0, batch: 215, loss: 0.38960695266723633\n",
      "epoch: 0, batch: 216, loss: 0.38845083117485046\n",
      "epoch: 0, batch: 217, loss: 0.42025014758110046\n",
      "epoch: 0, batch: 218, loss: 0.19291917979717255\n",
      "epoch: 0, batch: 219, loss: 0.2783430218696594\n",
      "epoch: 0, batch: 220, loss: 0.36587730050086975\n",
      "epoch: 0, batch: 221, loss: 0.17693358659744263\n",
      "epoch: 0, batch: 222, loss: 0.4009924530982971\n",
      "epoch: 0, batch: 223, loss: 0.21405500173568726\n",
      "epoch: 0, batch: 224, loss: 0.2242038995027542\n",
      "epoch: 0, batch: 225, loss: 0.3310043215751648\n",
      "epoch: 0, batch: 226, loss: 0.22830571234226227\n",
      "epoch: 0, batch: 227, loss: 0.37441396713256836\n",
      "epoch: 0, batch: 228, loss: 0.5046120285987854\n",
      "epoch: 0, batch: 229, loss: 0.5318210124969482\n",
      "epoch: 0, batch: 230, loss: 0.3243459463119507\n",
      "epoch: 0, batch: 231, loss: 0.2663912773132324\n",
      "epoch: 0, batch: 232, loss: 0.3450166583061218\n",
      "epoch: 0, batch: 233, loss: 0.26075056195259094\n",
      "epoch: 0, batch: 234, loss: 0.27289843559265137\n",
      "epoch: 0, batch: 235, loss: 0.40689513087272644\n",
      "epoch: 0, batch: 236, loss: 0.5006937980651855\n",
      "epoch: 0, batch: 237, loss: 0.27783623337745667\n",
      "epoch: 0, batch: 238, loss: 0.29312336444854736\n",
      "epoch: 0, batch: 239, loss: 0.21208742260932922\n",
      "epoch: 0, batch: 240, loss: 0.18750834465026855\n",
      "epoch: 0, batch: 241, loss: 0.4068973958492279\n",
      "epoch: 0, batch: 242, loss: 0.1898650825023651\n",
      "epoch: 0, batch: 243, loss: 0.26273274421691895\n",
      "epoch: 0, batch: 244, loss: 0.17052876949310303\n",
      "epoch: 0, batch: 245, loss: 0.4019685685634613\n",
      "epoch: 0, batch: 246, loss: 0.24589432775974274\n",
      "epoch: 0, batch: 247, loss: 0.4626266062259674\n",
      "epoch: 0, batch: 248, loss: 0.21474964916706085\n",
      "epoch: 0, batch: 249, loss: 0.40940797328948975\n",
      "epoch: 0, batch: 250, loss: 0.4318966269493103\n",
      "epoch: 0, batch: 251, loss: 0.2202884554862976\n",
      "epoch: 0, batch: 252, loss: 0.16949179768562317\n",
      "epoch: 0, batch: 253, loss: 0.18463309109210968\n",
      "epoch: 0, batch: 254, loss: 0.24599440395832062\n",
      "epoch: 0, batch: 255, loss: 0.249355286359787\n",
      "epoch: 0, batch: 256, loss: 0.2852382957935333\n",
      "epoch: 0, batch: 257, loss: 0.42231491208076477\n",
      "epoch: 0, batch: 258, loss: 0.18903936445713043\n",
      "epoch: 0, batch: 259, loss: 0.09557747095823288\n",
      "epoch: 0, batch: 260, loss: 0.43081381916999817\n",
      "epoch: 0, batch: 261, loss: 0.27061259746551514\n",
      "epoch: 0, batch: 262, loss: 0.2718607783317566\n",
      "epoch: 0, batch: 263, loss: 0.17033836245536804\n",
      "epoch: 0, batch: 264, loss: 0.3399707078933716\n",
      "epoch: 0, batch: 265, loss: 0.497153639793396\n",
      "epoch: 0, batch: 266, loss: 0.4796501398086548\n",
      "epoch: 0, batch: 267, loss: 0.44187241792678833\n",
      "epoch: 0, batch: 268, loss: 0.20725539326667786\n",
      "epoch: 0, batch: 269, loss: 0.4384642243385315\n",
      "epoch: 0, batch: 270, loss: 0.3733832836151123\n",
      "epoch: 0, batch: 271, loss: 0.35621359944343567\n",
      "epoch: 0, batch: 272, loss: 0.2546193301677704\n",
      "epoch: 0, batch: 273, loss: 0.33697035908699036\n",
      "epoch: 0, batch: 274, loss: 0.2276521474123001\n",
      "epoch: 0, batch: 275, loss: 0.2980245351791382\n",
      "epoch: 0, batch: 276, loss: 0.35566726326942444\n",
      "epoch: 0, batch: 277, loss: 0.1966850906610489\n",
      "epoch: 0, batch: 278, loss: 0.28131476044654846\n",
      "epoch: 0, batch: 279, loss: 0.1587105095386505\n",
      "epoch: 0, batch: 280, loss: 0.2654624879360199\n",
      "epoch: 0, batch: 281, loss: 0.19247902929782867\n",
      "epoch: 0, batch: 282, loss: 0.3608105182647705\n",
      "epoch: 0, batch: 283, loss: 0.18954512476921082\n",
      "epoch: 0, batch: 284, loss: 0.2235098034143448\n",
      "epoch: 0, batch: 285, loss: 0.3347910940647125\n",
      "epoch: 0, batch: 286, loss: 0.5103151202201843\n",
      "epoch: 0, batch: 287, loss: 0.221017986536026\n",
      "epoch: 0, batch: 288, loss: 0.19123917818069458\n",
      "epoch: 0, batch: 289, loss: 0.28396517038345337\n",
      "epoch: 0, batch: 290, loss: 0.39503660798072815\n",
      "epoch: 0, batch: 291, loss: 0.21214233338832855\n",
      "epoch: 0, batch: 292, loss: 0.29401659965515137\n",
      "epoch: 0, batch: 293, loss: 0.17798283696174622\n",
      "epoch: 0, batch: 294, loss: 0.28681740164756775\n",
      "epoch: 0, batch: 295, loss: 0.33750540018081665\n",
      "epoch: 0, batch: 296, loss: 0.31807956099510193\n",
      "epoch: 0, batch: 297, loss: 0.23318898677825928\n",
      "epoch: 0, batch: 298, loss: 0.1444864422082901\n",
      "epoch: 0, batch: 299, loss: 0.171504408121109\n",
      "epoch: 0, batch: 300, loss: 0.26113361120224\n",
      "epoch: 0, batch: 301, loss: 0.2532815933227539\n",
      "epoch: 0, batch: 302, loss: 0.32468733191490173\n",
      "epoch: 0, batch: 303, loss: 0.24922755360603333\n",
      "epoch: 0, batch: 304, loss: 0.18658584356307983\n",
      "epoch: 0, batch: 305, loss: 0.2059779167175293\n",
      "epoch: 0, batch: 306, loss: 0.199808731675148\n",
      "epoch: 0, batch: 307, loss: 0.15883956849575043\n",
      "epoch: 0, batch: 308, loss: 0.23133084177970886\n",
      "epoch: 0, batch: 309, loss: 0.35019633173942566\n",
      "epoch: 0, batch: 310, loss: 0.17507612705230713\n",
      "epoch: 0, batch: 311, loss: 0.3158530294895172\n",
      "epoch: 0, batch: 312, loss: 0.09821255505084991\n",
      "epoch: 0, batch: 313, loss: 0.15185055136680603\n",
      "epoch: 0, batch: 314, loss: 0.2584057152271271\n",
      "epoch: 0, batch: 315, loss: 0.29741162061691284\n",
      "epoch: 0, batch: 316, loss: 0.10402840375900269\n",
      "epoch: 0, batch: 317, loss: 0.21937191486358643\n",
      "epoch: 0, batch: 318, loss: 0.2504003047943115\n",
      "epoch: 0, batch: 319, loss: 0.2414494752883911\n",
      "epoch: 0, batch: 320, loss: 0.1321711540222168\n",
      "epoch: 0, batch: 321, loss: 0.16736885905265808\n",
      "epoch: 0, batch: 322, loss: 0.1543816328048706\n",
      "epoch: 0, batch: 323, loss: 0.30106788873672485\n",
      "epoch: 0, batch: 324, loss: 0.3062988817691803\n",
      "epoch: 0, batch: 325, loss: 0.11095425486564636\n",
      "epoch: 0, batch: 326, loss: 0.18951331079006195\n",
      "epoch: 0, batch: 327, loss: 0.45475658774375916\n",
      "epoch: 0, batch: 328, loss: 0.31035560369491577\n",
      "epoch: 0, batch: 329, loss: 0.22817951440811157\n",
      "epoch: 0, batch: 330, loss: 0.4008738100528717\n",
      "epoch: 0, batch: 331, loss: 0.24186190962791443\n",
      "epoch: 0, batch: 332, loss: 0.2282770872116089\n",
      "epoch: 0, batch: 333, loss: 0.1996792107820511\n",
      "epoch: 0, batch: 334, loss: 0.16864146292209625\n",
      "epoch: 0, batch: 335, loss: 0.17235830426216125\n",
      "epoch: 0, batch: 336, loss: 0.3579029142856598\n",
      "epoch: 0, batch: 337, loss: 0.15866079926490784\n",
      "epoch: 0, batch: 338, loss: 0.23516228795051575\n",
      "epoch: 0, batch: 339, loss: 0.26351386308670044\n",
      "epoch: 0, batch: 340, loss: 0.29327917098999023\n",
      "epoch: 0, batch: 341, loss: 0.24054643511772156\n",
      "epoch: 0, batch: 342, loss: 0.27783817052841187\n",
      "epoch: 0, batch: 343, loss: 0.3075478971004486\n",
      "epoch: 0, batch: 344, loss: 0.313724547624588\n",
      "epoch: 0, batch: 345, loss: 0.23778371512889862\n",
      "epoch: 0, batch: 346, loss: 0.24429041147232056\n",
      "epoch: 0, batch: 347, loss: 0.13616642355918884\n",
      "epoch: 0, batch: 348, loss: 0.5127934217453003\n",
      "epoch: 0, batch: 349, loss: 0.28784698247909546\n",
      "epoch: 0, batch: 350, loss: 0.15533210337162018\n",
      "epoch: 0, batch: 351, loss: 0.15874193608760834\n",
      "epoch: 0, batch: 352, loss: 0.16266360878944397\n",
      "epoch: 0, batch: 353, loss: 0.13838538527488708\n",
      "epoch: 0, batch: 354, loss: 0.20743797719478607\n",
      "epoch: 0, batch: 355, loss: 0.09966205060482025\n",
      "epoch: 0, batch: 356, loss: 0.08422528207302094\n",
      "epoch: 0, batch: 357, loss: 0.43478018045425415\n",
      "epoch: 0, batch: 358, loss: 0.3432037830352783\n",
      "epoch: 0, batch: 359, loss: 0.1857372373342514\n",
      "epoch: 0, batch: 360, loss: 0.17604370415210724\n",
      "epoch: 0, batch: 361, loss: 0.15552882850170135\n",
      "epoch: 0, batch: 362, loss: 0.2496984750032425\n",
      "epoch: 0, batch: 363, loss: 0.37600064277648926\n",
      "epoch: 0, batch: 364, loss: 0.24653182923793793\n",
      "epoch: 0, batch: 365, loss: 0.24084007740020752\n",
      "epoch: 0, batch: 366, loss: 0.163355752825737\n",
      "epoch: 0, batch: 367, loss: 0.3445362448692322\n",
      "epoch: 0, batch: 368, loss: 0.27728721499443054\n",
      "epoch: 0, batch: 369, loss: 0.08642864972352982\n",
      "epoch: 0, batch: 370, loss: 0.15783704817295074\n",
      "epoch: 0, batch: 371, loss: 0.1585773080587387\n",
      "epoch: 0, batch: 372, loss: 0.2376902550458908\n",
      "epoch: 0, batch: 373, loss: 0.2660762667655945\n",
      "epoch: 0, batch: 374, loss: 0.10784639418125153\n",
      "epoch: 0, batch: 375, loss: 0.11780288815498352\n",
      "epoch: 0, batch: 376, loss: 0.1621771603822708\n",
      "epoch: 0, batch: 377, loss: 0.1560434252023697\n",
      "epoch: 0, batch: 378, loss: 0.3796318471431732\n",
      "epoch: 0, batch: 379, loss: 0.4285445511341095\n",
      "epoch: 0, batch: 380, loss: 0.3211860656738281\n",
      "epoch: 0, batch: 381, loss: 0.20551390945911407\n",
      "epoch: 0, batch: 382, loss: 0.20851078629493713\n",
      "epoch: 0, batch: 383, loss: 0.1003129854798317\n",
      "epoch: 0, batch: 384, loss: 0.09318600594997406\n",
      "epoch: 0, batch: 385, loss: 0.0795522928237915\n",
      "epoch: 0, batch: 386, loss: 0.21702958643436432\n",
      "epoch: 0, batch: 387, loss: 0.15629154443740845\n",
      "epoch: 0, batch: 388, loss: 0.1910039633512497\n",
      "epoch: 0, batch: 389, loss: 0.15657778084278107\n",
      "epoch: 0, batch: 390, loss: 0.2684996426105499\n",
      "epoch: 0, batch: 391, loss: 0.08592801541090012\n",
      "epoch: 0, batch: 392, loss: 0.27170780301094055\n",
      "epoch: 0, batch: 393, loss: 0.1929529458284378\n",
      "epoch: 0, batch: 394, loss: 0.15162767469882965\n",
      "epoch: 0, batch: 395, loss: 0.2517460882663727\n",
      "epoch: 0, batch: 396, loss: 0.25218653678894043\n",
      "epoch: 0, batch: 397, loss: 0.21877481043338776\n",
      "epoch: 0, batch: 398, loss: 0.1316109597682953\n",
      "epoch: 0, batch: 399, loss: 0.19846564531326294\n",
      "epoch: 0, batch: 400, loss: 0.16786997020244598\n",
      "epoch: 0, batch: 401, loss: 0.27112799882888794\n",
      "epoch: 0, batch: 402, loss: 0.18586020171642303\n",
      "epoch: 0, batch: 403, loss: 0.22550736367702484\n",
      "epoch: 0, batch: 404, loss: 0.3616722822189331\n",
      "epoch: 0, batch: 405, loss: 0.29168665409088135\n",
      "epoch: 0, batch: 406, loss: 0.2375606745481491\n",
      "epoch: 0, batch: 407, loss: 0.09721366316080093\n",
      "epoch: 0, batch: 408, loss: 0.13269208371639252\n",
      "epoch: 0, batch: 409, loss: 0.32204487919807434\n",
      "epoch: 0, batch: 410, loss: 0.10221431404352188\n",
      "epoch: 0, batch: 411, loss: 0.14258423447608948\n",
      "epoch: 0, batch: 412, loss: 0.17058899998664856\n",
      "epoch: 0, batch: 413, loss: 0.19055362045764923\n",
      "epoch: 0, batch: 414, loss: 0.229630708694458\n",
      "epoch: 0, batch: 415, loss: 0.1360091120004654\n",
      "epoch: 0, batch: 416, loss: 0.1550660878419876\n",
      "epoch: 0, batch: 417, loss: 0.10537419468164444\n",
      "epoch: 0, batch: 418, loss: 0.19920037686824799\n",
      "epoch: 0, batch: 419, loss: 0.1842997670173645\n",
      "epoch: 0, batch: 420, loss: 0.15757077932357788\n",
      "epoch: 0, batch: 421, loss: 0.19052466750144958\n",
      "epoch: 0, batch: 422, loss: 0.21083679795265198\n",
      "epoch: 0, batch: 423, loss: 0.19582708179950714\n",
      "epoch: 0, batch: 424, loss: 0.13852927088737488\n",
      "epoch: 0, batch: 425, loss: 0.19488877058029175\n",
      "epoch: 0, batch: 426, loss: 0.12845787405967712\n",
      "epoch: 0, batch: 427, loss: 0.15569467842578888\n",
      "epoch: 0, batch: 428, loss: 0.3058876097202301\n",
      "epoch: 0, batch: 429, loss: 0.1282777488231659\n",
      "epoch: 0, batch: 430, loss: 0.15373827517032623\n",
      "epoch: 0, batch: 431, loss: 0.13343362510204315\n",
      "epoch: 0, batch: 432, loss: 0.13871124386787415\n",
      "epoch: 0, batch: 433, loss: 0.21263329684734344\n",
      "epoch: 0, batch: 434, loss: 0.13607481122016907\n",
      "epoch: 0, batch: 435, loss: 0.13860327005386353\n",
      "epoch: 0, batch: 436, loss: 0.24567019939422607\n",
      "epoch: 0, batch: 437, loss: 0.2885407507419586\n",
      "epoch: 0, batch: 438, loss: 0.3297134041786194\n",
      "epoch: 0, batch: 439, loss: 0.09224012494087219\n",
      "epoch: 0, batch: 440, loss: 0.11810094118118286\n",
      "epoch: 0, batch: 441, loss: 0.18112649023532867\n",
      "epoch: 0, batch: 442, loss: 0.30721792578697205\n",
      "epoch: 0, batch: 443, loss: 0.22646525502204895\n",
      "epoch: 0, batch: 444, loss: 0.22177720069885254\n",
      "epoch: 0, batch: 445, loss: 0.1992458999156952\n",
      "epoch: 0, batch: 446, loss: 0.26884084939956665\n",
      "epoch: 0, batch: 447, loss: 0.23549294471740723\n",
      "epoch: 0, batch: 448, loss: 0.2571408450603485\n",
      "epoch: 0, batch: 449, loss: 0.14461451768875122\n",
      "epoch: 0, batch: 450, loss: 0.2322700023651123\n",
      "epoch: 0, batch: 451, loss: 0.08336436003446579\n",
      "epoch: 0, batch: 452, loss: 0.13697554171085358\n",
      "epoch: 0, batch: 453, loss: 0.3168986737728119\n",
      "epoch: 0, batch: 454, loss: 0.2637435793876648\n",
      "epoch: 0, batch: 455, loss: 0.12116867303848267\n",
      "epoch: 0, batch: 456, loss: 0.22600769996643066\n",
      "epoch: 0, batch: 457, loss: 0.13084492087364197\n",
      "epoch: 0, batch: 458, loss: 0.29387685656547546\n",
      "epoch: 0, batch: 459, loss: 0.22371117770671844\n",
      "epoch: 0, batch: 460, loss: 0.11161977052688599\n",
      "epoch: 0, batch: 461, loss: 0.24386776983737946\n",
      "epoch: 0, batch: 462, loss: 0.12297062575817108\n",
      "epoch: 0, batch: 463, loss: 0.1280861794948578\n",
      "epoch: 0, batch: 464, loss: 0.2912119925022125\n",
      "epoch: 0, batch: 465, loss: 0.19220088422298431\n",
      "epoch: 0, batch: 466, loss: 0.07120955735445023\n",
      "epoch: 0, batch: 467, loss: 0.2022608518600464\n",
      "epoch: 0, batch: 468, loss: 0.24793505668640137\n",
      "epoch: 0, batch: 469, loss: 0.13338527083396912\n",
      "epoch: 0, batch: 470, loss: 0.11912171542644501\n",
      "epoch: 0, batch: 471, loss: 0.1891547590494156\n",
      "epoch: 0, batch: 472, loss: 0.14967988431453705\n",
      "epoch: 0, batch: 473, loss: 0.21392002701759338\n",
      "epoch: 0, batch: 474, loss: 0.16177749633789062\n",
      "epoch: 0, batch: 475, loss: 0.22824908792972565\n",
      "epoch: 0, batch: 476, loss: 0.16417665779590607\n",
      "epoch: 0, batch: 477, loss: 0.1412312388420105\n",
      "epoch: 0, batch: 478, loss: 0.12664449214935303\n",
      "epoch: 0, batch: 479, loss: 0.16534659266471863\n",
      "epoch: 0, batch: 480, loss: 0.122763492166996\n",
      "epoch: 0, batch: 481, loss: 0.11439758539199829\n",
      "epoch: 0, batch: 482, loss: 0.13187718391418457\n",
      "epoch: 0, batch: 483, loss: 0.20860588550567627\n",
      "epoch: 0, batch: 484, loss: 0.23945574462413788\n",
      "epoch: 0, batch: 485, loss: 0.07570569962263107\n",
      "epoch: 0, batch: 486, loss: 0.13077902793884277\n",
      "epoch: 0, batch: 487, loss: 0.1689198911190033\n",
      "epoch: 0, batch: 488, loss: 0.17421908676624298\n",
      "epoch: 0, batch: 489, loss: 0.27092286944389343\n",
      "epoch: 0, batch: 490, loss: 0.2647092044353485\n",
      "epoch: 0, batch: 491, loss: 0.08762834221124649\n",
      "epoch: 0, batch: 492, loss: 0.11286254972219467\n",
      "epoch: 0, batch: 493, loss: 0.15385234355926514\n",
      "epoch: 0, batch: 494, loss: 0.08800744265317917\n",
      "epoch: 0, batch: 495, loss: 0.11632420122623444\n",
      "epoch: 0, batch: 496, loss: 0.22045783698558807\n",
      "epoch: 0, batch: 497, loss: 0.16583535075187683\n",
      "epoch: 0, batch: 498, loss: 0.11440161615610123\n",
      "epoch: 0, batch: 499, loss: 0.22117048501968384\n",
      "epoch: 0, batch: 500, loss: 0.16930808126926422\n",
      "epoch: 0, batch: 501, loss: 0.14447511732578278\n",
      "epoch: 0, batch: 502, loss: 0.1308986246585846\n",
      "epoch: 0, batch: 503, loss: 0.2559620440006256\n",
      "epoch: 0, batch: 504, loss: 0.09958426654338837\n",
      "epoch: 0, batch: 505, loss: 0.09698323160409927\n",
      "epoch: 0, batch: 506, loss: 0.2071724534034729\n",
      "epoch: 0, batch: 507, loss: 0.44338297843933105\n",
      "epoch: 0, batch: 508, loss: 0.18379132449626923\n",
      "epoch: 0, batch: 509, loss: 0.12005200982093811\n",
      "epoch: 0, batch: 510, loss: 0.1259889006614685\n",
      "epoch: 0, batch: 511, loss: 0.2732672095298767\n",
      "epoch: 0, batch: 512, loss: 0.2162204384803772\n",
      "epoch: 0, batch: 513, loss: 0.271109014749527\n",
      "epoch: 0, batch: 514, loss: 0.24301777780056\n",
      "epoch: 0, batch: 515, loss: 0.3176088333129883\n",
      "epoch: 0, batch: 516, loss: 0.17910125851631165\n",
      "epoch: 0, batch: 517, loss: 0.26286670565605164\n",
      "epoch: 0, batch: 518, loss: 0.2753528952598572\n",
      "epoch: 0, batch: 519, loss: 0.31099969148635864\n",
      "epoch: 0, batch: 520, loss: 0.07285279780626297\n",
      "epoch: 0, batch: 521, loss: 0.20583826303482056\n",
      "epoch: 0, batch: 522, loss: 0.14026625454425812\n",
      "epoch: 0, batch: 523, loss: 0.17755341529846191\n",
      "epoch: 0, batch: 524, loss: 0.11749963462352753\n",
      "epoch: 0, batch: 525, loss: 0.06631297618150711\n",
      "epoch: 0, batch: 526, loss: 0.14870768785476685\n",
      "epoch: 0, batch: 527, loss: 0.06747329980134964\n",
      "epoch: 0, batch: 528, loss: 0.2070285826921463\n",
      "epoch: 0, batch: 529, loss: 0.17290909588336945\n",
      "epoch: 0, batch: 530, loss: 0.1384413242340088\n",
      "epoch: 0, batch: 531, loss: 0.11837366968393326\n",
      "epoch: 0, batch: 532, loss: 0.23891466856002808\n",
      "epoch: 0, batch: 533, loss: 0.0631125420331955\n",
      "epoch: 0, batch: 534, loss: 0.11964443325996399\n",
      "epoch: 0, batch: 535, loss: 0.1289849877357483\n",
      "epoch: 0, batch: 536, loss: 0.12963750958442688\n",
      "epoch: 0, batch: 537, loss: 0.20385724306106567\n",
      "epoch: 0, batch: 538, loss: 0.17062103748321533\n",
      "epoch: 0, batch: 539, loss: 0.1834719330072403\n",
      "epoch: 0, batch: 540, loss: 0.1753566563129425\n",
      "epoch: 0, batch: 541, loss: 0.08526462316513062\n",
      "epoch: 0, batch: 542, loss: 0.13877014815807343\n",
      "epoch: 0, batch: 543, loss: 0.10016065835952759\n",
      "epoch: 0, batch: 544, loss: 0.22607924044132233\n",
      "epoch: 0, batch: 545, loss: 0.0665224939584732\n",
      "epoch: 0, batch: 546, loss: 0.08903458714485168\n",
      "epoch: 0, batch: 547, loss: 0.09602221846580505\n",
      "epoch: 0, batch: 548, loss: 0.1308698207139969\n",
      "epoch: 0, batch: 549, loss: 0.2769581079483032\n",
      "epoch: 0, batch: 550, loss: 0.1671135127544403\n",
      "epoch: 0, batch: 551, loss: 0.11432821303606033\n",
      "epoch: 0, batch: 552, loss: 0.11147962510585785\n",
      "epoch: 0, batch: 553, loss: 0.17666460573673248\n",
      "epoch: 0, batch: 554, loss: 0.1986614465713501\n",
      "epoch: 0, batch: 555, loss: 0.05281087011098862\n",
      "epoch: 0, batch: 556, loss: 0.1469913274049759\n",
      "epoch: 0, batch: 557, loss: 0.04549473896622658\n",
      "epoch: 0, batch: 558, loss: 0.20036457479000092\n",
      "epoch: 0, batch: 559, loss: 0.3370465338230133\n",
      "epoch: 0, batch: 560, loss: 0.11751770973205566\n",
      "epoch: 0, batch: 561, loss: 0.06624886393547058\n",
      "epoch: 0, batch: 562, loss: 0.07263646274805069\n",
      "epoch: 0, batch: 563, loss: 0.1721259504556656\n",
      "epoch: 0, batch: 564, loss: 0.2195729911327362\n",
      "epoch: 0, batch: 565, loss: 0.15587396919727325\n",
      "epoch: 0, batch: 566, loss: 0.2349630445241928\n",
      "epoch: 0, batch: 567, loss: 0.055668123066425323\n",
      "epoch: 0, batch: 568, loss: 0.15861177444458008\n",
      "epoch: 0, batch: 569, loss: 0.03654898703098297\n",
      "epoch: 0, batch: 570, loss: 0.1743772327899933\n",
      "epoch: 0, batch: 571, loss: 0.09442014247179031\n",
      "epoch: 0, batch: 572, loss: 0.12215753644704819\n",
      "epoch: 0, batch: 573, loss: 0.08473633229732513\n",
      "epoch: 0, batch: 574, loss: 0.0923304632306099\n",
      "epoch: 0, batch: 575, loss: 0.4001695215702057\n",
      "epoch: 0, batch: 576, loss: 0.1737298220396042\n",
      "epoch: 0, batch: 577, loss: 0.22083055973052979\n",
      "epoch: 0, batch: 578, loss: 0.198271244764328\n",
      "epoch: 0, batch: 579, loss: 0.10979443043470383\n",
      "epoch: 0, batch: 580, loss: 0.1437191516160965\n",
      "epoch: 0, batch: 581, loss: 0.22677922248840332\n",
      "epoch: 0, batch: 582, loss: 0.16168099641799927\n",
      "epoch: 0, batch: 583, loss: 0.30249494314193726\n",
      "epoch: 0, batch: 584, loss: 0.15664279460906982\n",
      "epoch: 0, batch: 585, loss: 0.19491806626319885\n",
      "epoch: 0, batch: 586, loss: 0.22022263705730438\n",
      "epoch: 0, batch: 587, loss: 0.08163507282733917\n",
      "epoch: 0, batch: 588, loss: 0.1430388242006302\n",
      "epoch: 0, batch: 589, loss: 0.11118737608194351\n",
      "epoch: 0, batch: 590, loss: 0.03800089657306671\n",
      "epoch: 0, batch: 591, loss: 0.1687982678413391\n",
      "epoch: 0, batch: 592, loss: 0.10138488560914993\n",
      "epoch: 0, batch: 593, loss: 0.24209384620189667\n",
      "epoch: 0, batch: 594, loss: 0.1496940702199936\n",
      "epoch: 0, batch: 595, loss: 0.25176680088043213\n",
      "epoch: 0, batch: 596, loss: 0.055060237646102905\n",
      "epoch: 0, batch: 597, loss: 0.18168382346630096\n",
      "epoch: 0, batch: 598, loss: 0.15971753001213074\n",
      "epoch: 0, batch: 599, loss: 0.12161625921726227\n",
      "epoch: 0, batch: 600, loss: 0.18345002830028534\n",
      "epoch: 0, batch: 601, loss: 0.04212488979101181\n",
      "epoch: 0, batch: 602, loss: 0.15237843990325928\n",
      "epoch: 0, batch: 603, loss: 0.23346532881259918\n",
      "epoch: 0, batch: 604, loss: 0.11036952584981918\n",
      "epoch: 0, batch: 605, loss: 0.08477232605218887\n",
      "epoch: 0, batch: 606, loss: 0.3123960494995117\n",
      "epoch: 0, batch: 607, loss: 0.12411923706531525\n",
      "epoch: 0, batch: 608, loss: 0.20331957936286926\n",
      "epoch: 0, batch: 609, loss: 0.20004354417324066\n",
      "epoch: 0, batch: 610, loss: 0.10540518164634705\n",
      "epoch: 0, batch: 611, loss: 0.10737615078687668\n",
      "epoch: 0, batch: 612, loss: 0.140635684132576\n",
      "epoch: 0, batch: 613, loss: 0.06550714373588562\n",
      "epoch: 0, batch: 614, loss: 0.056850794702768326\n",
      "epoch: 0, batch: 615, loss: 0.20081424713134766\n",
      "epoch: 0, batch: 616, loss: 0.1743796467781067\n",
      "epoch: 0, batch: 617, loss: 0.32602667808532715\n",
      "epoch: 0, batch: 618, loss: 0.17284169793128967\n",
      "epoch: 0, batch: 619, loss: 0.0955631360411644\n",
      "epoch: 0, batch: 620, loss: 0.23948465287685394\n",
      "epoch: 0, batch: 621, loss: 0.11172209680080414\n",
      "epoch: 0, batch: 622, loss: 0.19175580143928528\n",
      "epoch: 0, batch: 623, loss: 0.1354094296693802\n",
      "epoch: 0, batch: 624, loss: 0.26656171679496765\n",
      "epoch: 0, batch: 625, loss: 0.09753544628620148\n",
      "epoch: 0, batch: 626, loss: 0.0832686498761177\n",
      "epoch: 0, batch: 627, loss: 0.19611915946006775\n",
      "epoch: 0, batch: 628, loss: 0.15497224032878876\n",
      "epoch: 0, batch: 629, loss: 0.25480416417121887\n",
      "epoch: 0, batch: 630, loss: 0.04828011244535446\n",
      "epoch: 0, batch: 631, loss: 0.14651602506637573\n",
      "epoch: 0, batch: 632, loss: 0.06154967099428177\n",
      "epoch: 0, batch: 633, loss: 0.10324712842702866\n",
      "epoch: 0, batch: 634, loss: 0.07263278216123581\n",
      "epoch: 0, batch: 635, loss: 0.07851535081863403\n",
      "epoch: 0, batch: 636, loss: 0.2886679172515869\n",
      "epoch: 0, batch: 637, loss: 0.220780149102211\n",
      "epoch: 0, batch: 638, loss: 0.09559575468301773\n",
      "epoch: 0, batch: 639, loss: 0.18157435953617096\n",
      "epoch: 0, batch: 640, loss: 0.20128636062145233\n",
      "epoch: 0, batch: 641, loss: 0.05414772406220436\n",
      "epoch: 0, batch: 642, loss: 0.09767452627420425\n",
      "epoch: 0, batch: 643, loss: 0.1687464863061905\n",
      "epoch: 0, batch: 644, loss: 0.12848250567913055\n",
      "epoch: 0, batch: 645, loss: 0.14481033384799957\n",
      "epoch: 0, batch: 646, loss: 0.11942403763532639\n",
      "epoch: 0, batch: 647, loss: 0.10355520993471146\n",
      "epoch: 0, batch: 648, loss: 0.24258874356746674\n",
      "epoch: 0, batch: 649, loss: 0.17534062266349792\n",
      "epoch: 0, batch: 650, loss: 0.2484917789697647\n",
      "epoch: 0, batch: 651, loss: 0.2581709325313568\n",
      "epoch: 0, batch: 652, loss: 0.1388150453567505\n",
      "epoch: 0, batch: 653, loss: 0.08839428424835205\n",
      "epoch: 0, batch: 654, loss: 0.25674012303352356\n",
      "epoch: 0, batch: 655, loss: 0.17904315888881683\n",
      "epoch: 0, batch: 656, loss: 0.1258191615343094\n",
      "epoch: 0, batch: 657, loss: 0.06978706270456314\n",
      "epoch: 0, batch: 658, loss: 0.19403386116027832\n",
      "epoch: 0, batch: 659, loss: 0.07576221227645874\n",
      "epoch: 0, batch: 660, loss: 0.08148995041847229\n",
      "epoch: 0, batch: 661, loss: 0.08993733674287796\n",
      "epoch: 0, batch: 662, loss: 0.13418318331241608\n",
      "epoch: 0, batch: 663, loss: 0.09114305675029755\n",
      "epoch: 0, batch: 664, loss: 0.061756934970617294\n",
      "epoch: 0, batch: 665, loss: 0.0990142971277237\n",
      "epoch: 0, batch: 666, loss: 0.31512582302093506\n",
      "epoch: 0, batch: 667, loss: 0.13700911402702332\n",
      "epoch: 0, batch: 668, loss: 0.09138749539852142\n",
      "epoch: 0, batch: 669, loss: 0.17588694393634796\n",
      "epoch: 0, batch: 670, loss: 0.2070462703704834\n",
      "epoch: 0, batch: 671, loss: 0.23963060975074768\n",
      "epoch: 0, batch: 672, loss: 0.08263292908668518\n",
      "epoch: 0, batch: 673, loss: 0.13719996809959412\n",
      "epoch: 0, batch: 674, loss: 0.09925764054059982\n",
      "epoch: 0, batch: 675, loss: 0.15270958840847015\n",
      "epoch: 0, batch: 676, loss: 0.18656060099601746\n",
      "epoch: 0, batch: 677, loss: 0.12077498435974121\n",
      "epoch: 0, batch: 678, loss: 0.08491349220275879\n",
      "epoch: 0, batch: 679, loss: 0.06522277742624283\n",
      "epoch: 0, batch: 680, loss: 0.08304549753665924\n",
      "epoch: 0, batch: 681, loss: 0.10960701107978821\n",
      "epoch: 0, batch: 682, loss: 0.08493395149707794\n",
      "epoch: 0, batch: 683, loss: 0.15736429393291473\n",
      "epoch: 0, batch: 684, loss: 0.2934405207633972\n",
      "epoch: 0, batch: 685, loss: 0.21937136352062225\n",
      "epoch: 0, batch: 686, loss: 0.07565983384847641\n",
      "epoch: 0, batch: 687, loss: 0.08497737348079681\n",
      "epoch: 0, batch: 688, loss: 0.11520425975322723\n",
      "epoch: 0, batch: 689, loss: 0.20059743523597717\n",
      "epoch: 0, batch: 690, loss: 0.10288209468126297\n",
      "epoch: 0, batch: 691, loss: 0.19907188415527344\n",
      "epoch: 0, batch: 692, loss: 0.2333897203207016\n",
      "epoch: 0, batch: 693, loss: 0.12754540145397186\n",
      "epoch: 0, batch: 694, loss: 0.15512406826019287\n",
      "epoch: 0, batch: 695, loss: 0.08027472347021103\n",
      "epoch: 0, batch: 696, loss: 0.1343601793050766\n",
      "epoch: 0, batch: 697, loss: 0.16152742505073547\n",
      "epoch: 0, batch: 698, loss: 0.1614547222852707\n",
      "epoch: 0, batch: 699, loss: 0.12270207703113556\n",
      "epoch: 0, batch: 700, loss: 0.10575937479734421\n",
      "epoch: 0, batch: 701, loss: 0.08166169375181198\n",
      "epoch: 0, batch: 702, loss: 0.13637416064739227\n",
      "epoch: 0, batch: 703, loss: 0.10015007108449936\n",
      "epoch: 0, batch: 704, loss: 0.38377925753593445\n",
      "epoch: 0, batch: 705, loss: 0.13818952441215515\n",
      "epoch: 0, batch: 706, loss: 0.11221987009048462\n",
      "epoch: 0, batch: 707, loss: 0.09629189968109131\n",
      "epoch: 0, batch: 708, loss: 0.20012059807777405\n",
      "epoch: 0, batch: 709, loss: 0.11547164618968964\n",
      "epoch: 0, batch: 710, loss: 0.1302809864282608\n",
      "epoch: 0, batch: 711, loss: 0.191961407661438\n",
      "epoch: 0, batch: 712, loss: 0.11669134348630905\n",
      "epoch: 0, batch: 713, loss: 0.12273887544870377\n",
      "epoch: 0, batch: 714, loss: 0.03226673975586891\n",
      "epoch: 0, batch: 715, loss: 0.1510613113641739\n",
      "epoch: 0, batch: 716, loss: 0.20299524068832397\n",
      "epoch: 0, batch: 717, loss: 0.16824698448181152\n",
      "epoch: 0, batch: 718, loss: 0.2054700255393982\n",
      "epoch: 0, batch: 719, loss: 0.12045365571975708\n",
      "epoch: 0, batch: 720, loss: 0.1728585660457611\n",
      "epoch: 0, batch: 721, loss: 0.09475334733724594\n",
      "epoch: 0, batch: 722, loss: 0.09989089518785477\n",
      "epoch: 0, batch: 723, loss: 0.13217628002166748\n",
      "epoch: 0, batch: 724, loss: 0.08979765325784683\n",
      "epoch: 0, batch: 725, loss: 0.11798247694969177\n",
      "epoch: 0, batch: 726, loss: 0.0722867026925087\n",
      "epoch: 0, batch: 727, loss: 0.1892688125371933\n",
      "epoch: 0, batch: 728, loss: 0.18034283816814423\n",
      "epoch: 0, batch: 729, loss: 0.24126940965652466\n",
      "epoch: 0, batch: 730, loss: 0.08348690718412399\n",
      "epoch: 0, batch: 731, loss: 0.0888650044798851\n",
      "epoch: 0, batch: 732, loss: 0.17209559679031372\n",
      "epoch: 0, batch: 733, loss: 0.08685318380594254\n",
      "epoch: 0, batch: 734, loss: 0.025086427107453346\n",
      "epoch: 0, batch: 735, loss: 0.14417801797389984\n",
      "epoch: 0, batch: 736, loss: 0.1784227341413498\n",
      "epoch: 0, batch: 737, loss: 0.08363496512174606\n",
      "epoch: 0, batch: 738, loss: 0.0825669914484024\n",
      "epoch: 0, batch: 739, loss: 0.11313054710626602\n",
      "epoch: 0, batch: 740, loss: 0.16158761084079742\n",
      "epoch: 0, batch: 741, loss: 0.1863652616739273\n",
      "epoch: 0, batch: 742, loss: 0.0786772146821022\n",
      "epoch: 0, batch: 743, loss: 0.20638228952884674\n",
      "epoch: 0, batch: 744, loss: 0.03724998980760574\n",
      "epoch: 0, batch: 745, loss: 0.08158794790506363\n",
      "epoch: 0, batch: 746, loss: 0.11409031599760056\n",
      "epoch: 0, batch: 747, loss: 0.19474603235721588\n",
      "epoch: 0, batch: 748, loss: 0.07845372706651688\n",
      "epoch: 0, batch: 749, loss: 0.1140386164188385\n",
      "epoch: 0, batch: 750, loss: 0.03673141822218895\n",
      "epoch: 0, batch: 751, loss: 0.038026660680770874\n",
      "epoch: 0, batch: 752, loss: 0.14217828214168549\n",
      "epoch: 0, batch: 753, loss: 0.21073105931282043\n",
      "epoch: 0, batch: 754, loss: 0.07987097650766373\n",
      "epoch: 0, batch: 755, loss: 0.08010784536600113\n",
      "epoch: 0, batch: 756, loss: 0.07923424988985062\n",
      "epoch: 0, batch: 757, loss: 0.13702788949012756\n",
      "epoch: 0, batch: 758, loss: 0.06293857097625732\n",
      "epoch: 0, batch: 759, loss: 0.1856861412525177\n",
      "epoch: 0, batch: 760, loss: 0.08725691586732864\n",
      "epoch: 0, batch: 761, loss: 0.20821379125118256\n",
      "epoch: 0, batch: 762, loss: 0.10430289804935455\n",
      "epoch: 0, batch: 763, loss: 0.12851697206497192\n",
      "epoch: 0, batch: 764, loss: 0.2403220683336258\n",
      "epoch: 0, batch: 765, loss: 0.056526582688093185\n",
      "epoch: 0, batch: 766, loss: 0.1269555389881134\n",
      "epoch: 0, batch: 767, loss: 0.11416646093130112\n",
      "epoch: 0, batch: 768, loss: 0.06184019893407822\n",
      "epoch: 0, batch: 769, loss: 0.1412738561630249\n",
      "epoch: 0, batch: 770, loss: 0.09943900257349014\n",
      "epoch: 0, batch: 771, loss: 0.18274645507335663\n",
      "epoch: 0, batch: 772, loss: 0.09440130740404129\n",
      "epoch: 0, batch: 773, loss: 0.19584184885025024\n",
      "epoch: 0, batch: 774, loss: 0.07669387012720108\n",
      "epoch: 0, batch: 775, loss: 0.2168564796447754\n",
      "epoch: 0, batch: 776, loss: 0.19061996042728424\n",
      "epoch: 0, batch: 777, loss: 0.16925644874572754\n",
      "epoch: 0, batch: 778, loss: 0.07220423966646194\n",
      "epoch: 0, batch: 779, loss: 0.1981998085975647\n",
      "epoch: 0, batch: 780, loss: 0.14703555405139923\n",
      "epoch: 0, batch: 781, loss: 0.13628801703453064\n",
      "epoch: 0, batch: 782, loss: 0.1178482323884964\n",
      "epoch: 0, batch: 783, loss: 0.05303265526890755\n",
      "epoch: 0, batch: 784, loss: 0.1497030407190323\n",
      "epoch: 0, batch: 785, loss: 0.1839871108531952\n",
      "epoch: 0, batch: 786, loss: 0.14737413823604584\n",
      "epoch: 0, batch: 787, loss: 0.10577144473791122\n",
      "epoch: 0, batch: 788, loss: 0.07075503468513489\n",
      "epoch: 0, batch: 789, loss: 0.16041800379753113\n",
      "epoch: 0, batch: 790, loss: 0.05981837213039398\n",
      "epoch: 0, batch: 791, loss: 0.2344905436038971\n",
      "epoch: 0, batch: 792, loss: 0.07332552969455719\n",
      "epoch: 0, batch: 793, loss: 0.06913101673126221\n",
      "epoch: 0, batch: 794, loss: 0.05547697842121124\n",
      "epoch: 0, batch: 795, loss: 0.04032690450549126\n",
      "epoch: 0, batch: 796, loss: 0.17988161742687225\n",
      "epoch: 0, batch: 797, loss: 0.1869542896747589\n",
      "epoch: 0, batch: 798, loss: 0.08547335863113403\n",
      "epoch: 0, batch: 799, loss: 0.1828620433807373\n",
      "epoch: 0, batch: 800, loss: 0.09784701466560364\n",
      "epoch: 0, batch: 801, loss: 0.07343962043523788\n",
      "epoch: 0, batch: 802, loss: 0.17825520038604736\n",
      "epoch: 0, batch: 803, loss: 0.20731084048748016\n",
      "epoch: 0, batch: 804, loss: 0.1433820277452469\n",
      "epoch: 0, batch: 805, loss: 0.07576920837163925\n",
      "epoch: 0, batch: 806, loss: 0.09141314029693604\n",
      "epoch: 0, batch: 807, loss: 0.18794330954551697\n",
      "epoch: 0, batch: 808, loss: 0.05361566320061684\n",
      "epoch: 0, batch: 809, loss: 0.17151804268360138\n",
      "epoch: 0, batch: 810, loss: 0.12064679712057114\n",
      "epoch: 0, batch: 811, loss: 0.09349378198385239\n",
      "epoch: 0, batch: 812, loss: 0.1735917627811432\n",
      "epoch: 0, batch: 813, loss: 0.07045669853687286\n",
      "epoch: 0, batch: 814, loss: 0.055869102478027344\n",
      "epoch: 0, batch: 815, loss: 0.08806947618722916\n",
      "epoch: 0, batch: 816, loss: 0.10045892745256424\n",
      "epoch: 0, batch: 817, loss: 0.07290437072515488\n",
      "epoch: 0, batch: 818, loss: 0.09104457497596741\n",
      "epoch: 0, batch: 819, loss: 0.08217546343803406\n",
      "epoch: 0, batch: 820, loss: 0.3341849446296692\n",
      "epoch: 0, batch: 821, loss: 0.1778620481491089\n",
      "epoch: 0, batch: 822, loss: 0.019088033586740494\n",
      "epoch: 0, batch: 823, loss: 0.10238374769687653\n",
      "epoch: 0, batch: 824, loss: 0.06062910333275795\n",
      "epoch: 0, batch: 825, loss: 0.10239841043949127\n",
      "epoch: 0, batch: 826, loss: 0.03355589136481285\n",
      "epoch: 0, batch: 827, loss: 0.3492804169654846\n",
      "epoch: 0, batch: 828, loss: 0.166436567902565\n",
      "epoch: 0, batch: 829, loss: 0.028441889211535454\n",
      "epoch: 0, batch: 830, loss: 0.18599839508533478\n",
      "epoch: 0, batch: 831, loss: 0.12859776616096497\n",
      "epoch: 0, batch: 832, loss: 0.13538183271884918\n",
      "epoch: 0, batch: 833, loss: 0.08548890799283981\n",
      "epoch: 0, batch: 834, loss: 0.08430986106395721\n",
      "epoch: 0, batch: 835, loss: 0.12375087291002274\n",
      "epoch: 0, batch: 836, loss: 0.025138303637504578\n",
      "epoch: 0, batch: 837, loss: 0.09386283159255981\n",
      "epoch: 0, batch: 838, loss: 0.04350768402218819\n",
      "epoch: 0, batch: 839, loss: 0.10509204864501953\n",
      "epoch: 0, batch: 840, loss: 0.20900124311447144\n",
      "epoch: 0, batch: 841, loss: 0.13470859825611115\n",
      "epoch: 0, batch: 842, loss: 0.09469189494848251\n",
      "epoch: 0, batch: 843, loss: 0.043157245963811874\n",
      "epoch: 0, batch: 844, loss: 0.17053724825382233\n",
      "epoch: 0, batch: 845, loss: 0.1666869819164276\n",
      "epoch: 0, batch: 846, loss: 0.10918617248535156\n",
      "epoch: 0, batch: 847, loss: 0.09643150866031647\n",
      "epoch: 0, batch: 848, loss: 0.08584536612033844\n",
      "epoch: 0, batch: 849, loss: 0.038468487560749054\n",
      "epoch: 0, batch: 850, loss: 0.11216699331998825\n",
      "epoch: 0, batch: 851, loss: 0.06040600314736366\n",
      "epoch: 0, batch: 852, loss: 0.1535688191652298\n",
      "epoch: 0, batch: 853, loss: 0.05214851722121239\n",
      "epoch: 0, batch: 854, loss: 0.20908121764659882\n",
      "epoch: 0, batch: 855, loss: 0.09602798521518707\n",
      "epoch: 0, batch: 856, loss: 0.10421515256166458\n",
      "epoch: 0, batch: 857, loss: 0.12285884469747543\n",
      "epoch: 0, batch: 858, loss: 0.10305825620889664\n",
      "epoch: 0, batch: 859, loss: 0.07143306732177734\n",
      "epoch: 0, batch: 860, loss: 0.11775859445333481\n",
      "epoch: 0, batch: 861, loss: 0.15254202485084534\n",
      "epoch: 0, batch: 862, loss: 0.10320018976926804\n",
      "epoch: 0, batch: 863, loss: 0.04480466619133949\n",
      "epoch: 0, batch: 864, loss: 0.11448479443788528\n",
      "epoch: 0, batch: 865, loss: 0.04496340826153755\n",
      "epoch: 0, batch: 866, loss: 0.16230084002017975\n",
      "epoch: 0, batch: 867, loss: 0.14555007219314575\n",
      "epoch: 0, batch: 868, loss: 0.19434942305088043\n",
      "epoch: 0, batch: 869, loss: 0.15330025553703308\n",
      "epoch: 0, batch: 870, loss: 0.058729927986860275\n",
      "epoch: 0, batch: 871, loss: 0.1497614085674286\n",
      "epoch: 0, batch: 872, loss: 0.07540496438741684\n",
      "epoch: 0, batch: 873, loss: 0.04565034806728363\n",
      "epoch: 0, batch: 874, loss: 0.18887214362621307\n",
      "epoch: 0, batch: 875, loss: 0.03413933143019676\n",
      "epoch: 0, batch: 876, loss: 0.15272393822669983\n",
      "epoch: 0, batch: 877, loss: 0.12624473869800568\n",
      "epoch: 0, batch: 878, loss: 0.06230901926755905\n",
      "epoch: 0, batch: 879, loss: 0.030817696824669838\n",
      "epoch: 0, batch: 880, loss: 0.08462720364332199\n",
      "epoch: 0, batch: 881, loss: 0.07034949958324432\n",
      "epoch: 0, batch: 882, loss: 0.0932067409157753\n",
      "epoch: 0, batch: 883, loss: 0.07221360504627228\n",
      "epoch: 0, batch: 884, loss: 0.1292959451675415\n",
      "epoch: 0, batch: 885, loss: 0.1259353905916214\n",
      "epoch: 0, batch: 886, loss: 0.15738213062286377\n",
      "epoch: 0, batch: 887, loss: 0.18716929852962494\n",
      "epoch: 0, batch: 888, loss: 0.06590971350669861\n",
      "epoch: 0, batch: 889, loss: 0.16581718623638153\n",
      "epoch: 0, batch: 890, loss: 0.0836421474814415\n",
      "epoch: 0, batch: 891, loss: 0.10680726915597916\n",
      "epoch: 0, batch: 892, loss: 0.24025721848011017\n",
      "epoch: 0, batch: 893, loss: 0.19739384949207306\n",
      "epoch: 0, batch: 894, loss: 0.1262454241514206\n",
      "epoch: 0, batch: 895, loss: 0.16223685443401337\n",
      "epoch: 0, batch: 896, loss: 0.07183940708637238\n",
      "epoch: 0, batch: 897, loss: 0.06802169233560562\n",
      "epoch: 0, batch: 898, loss: 0.13163690268993378\n",
      "epoch: 0, batch: 899, loss: 0.07223118096590042\n",
      "epoch: 0, batch: 900, loss: 0.060820322483778\n",
      "epoch: 0, batch: 901, loss: 0.17146946489810944\n",
      "epoch: 0, batch: 902, loss: 0.07589083909988403\n",
      "epoch: 0, batch: 903, loss: 0.04229665547609329\n",
      "epoch: 0, batch: 904, loss: 0.1436300277709961\n",
      "epoch: 0, batch: 905, loss: 0.14946652948856354\n",
      "epoch: 0, batch: 906, loss: 0.08205035328865051\n",
      "epoch: 0, batch: 907, loss: 0.1673080325126648\n",
      "epoch: 0, batch: 908, loss: 0.19748364388942719\n",
      "epoch: 0, batch: 909, loss: 0.42085087299346924\n",
      "epoch: 0, batch: 910, loss: 0.060551099479198456\n",
      "epoch: 0, batch: 911, loss: 0.0969332903623581\n",
      "epoch: 0, batch: 912, loss: 0.15654435753822327\n",
      "epoch: 0, batch: 913, loss: 0.08147721737623215\n",
      "epoch: 0, batch: 914, loss: 0.11093312501907349\n",
      "epoch: 0, batch: 915, loss: 0.0890142172574997\n",
      "epoch: 0, batch: 916, loss: 0.16743408143520355\n",
      "epoch: 0, batch: 917, loss: 0.17600147426128387\n",
      "epoch: 0, batch: 918, loss: 0.1385001391172409\n",
      "epoch: 0, batch: 919, loss: 0.26562097668647766\n",
      "epoch: 0, batch: 920, loss: 0.1638728380203247\n",
      "epoch: 0, batch: 921, loss: 0.07738856226205826\n",
      "epoch: 0, batch: 922, loss: 0.06235220283269882\n",
      "epoch: 0, batch: 923, loss: 0.061396870762109756\n",
      "epoch: 0, batch: 924, loss: 0.189945250749588\n",
      "epoch: 0, batch: 925, loss: 0.1291903853416443\n",
      "epoch: 0, batch: 926, loss: 0.057964544743299484\n",
      "epoch: 0, batch: 927, loss: 0.05205036327242851\n",
      "epoch: 0, batch: 928, loss: 0.12068396061658859\n",
      "epoch: 0, batch: 929, loss: 0.10315096378326416\n",
      "epoch: 0, batch: 930, loss: 0.07592057436704636\n",
      "epoch: 0, batch: 931, loss: 0.07721606642007828\n",
      "epoch: 0, batch: 932, loss: 0.028586015105247498\n",
      "epoch: 0, batch: 933, loss: 0.1943003088235855\n",
      "epoch: 0, batch: 934, loss: 0.14512455463409424\n",
      "epoch: 0, batch: 935, loss: 0.23502972722053528\n",
      "epoch: 0, batch: 936, loss: 0.1271039843559265\n",
      "epoch: 0, batch: 937, loss: 0.05571357160806656\n",
      "epoch: 1, batch: 0, loss: 0.04883027821779251\n",
      "epoch: 1, batch: 1, loss: 0.20409350097179413\n",
      "epoch: 1, batch: 2, loss: 0.0867827832698822\n",
      "epoch: 1, batch: 3, loss: 0.06539943069219589\n",
      "epoch: 1, batch: 4, loss: 0.0869869589805603\n",
      "epoch: 1, batch: 5, loss: 0.15110552310943604\n",
      "epoch: 1, batch: 6, loss: 0.1288403570652008\n",
      "epoch: 1, batch: 7, loss: 0.11106846481561661\n",
      "epoch: 1, batch: 8, loss: 0.063844695687294\n",
      "epoch: 1, batch: 9, loss: 0.02080739475786686\n",
      "epoch: 1, batch: 10, loss: 0.0679665058851242\n",
      "epoch: 1, batch: 11, loss: 0.1221921443939209\n",
      "epoch: 1, batch: 12, loss: 0.1467006355524063\n",
      "epoch: 1, batch: 13, loss: 0.04869664087891579\n",
      "epoch: 1, batch: 14, loss: 0.06192345172166824\n",
      "epoch: 1, batch: 15, loss: 0.03958573564887047\n",
      "epoch: 1, batch: 16, loss: 0.1940966695547104\n",
      "epoch: 1, batch: 17, loss: 0.08359947800636292\n",
      "epoch: 1, batch: 18, loss: 0.17502373456954956\n",
      "epoch: 1, batch: 19, loss: 0.05113281309604645\n",
      "epoch: 1, batch: 20, loss: 0.05903485044836998\n",
      "epoch: 1, batch: 21, loss: 0.07723527401685715\n",
      "epoch: 1, batch: 22, loss: 0.03390713036060333\n",
      "epoch: 1, batch: 23, loss: 0.10486705601215363\n",
      "epoch: 1, batch: 24, loss: 0.17664867639541626\n",
      "epoch: 1, batch: 25, loss: 0.05607405677437782\n",
      "epoch: 1, batch: 26, loss: 0.1292906403541565\n",
      "epoch: 1, batch: 27, loss: 0.09819827973842621\n",
      "epoch: 1, batch: 28, loss: 0.06985090672969818\n",
      "epoch: 1, batch: 29, loss: 0.043510787189006805\n",
      "epoch: 1, batch: 30, loss: 0.20693808794021606\n",
      "epoch: 1, batch: 31, loss: 0.06981153786182404\n",
      "epoch: 1, batch: 32, loss: 0.0542142353951931\n",
      "epoch: 1, batch: 33, loss: 0.044813115149736404\n",
      "epoch: 1, batch: 34, loss: 0.10317374765872955\n",
      "epoch: 1, batch: 35, loss: 0.045305803418159485\n",
      "epoch: 1, batch: 36, loss: 0.08845892548561096\n",
      "epoch: 1, batch: 37, loss: 0.1301930695772171\n",
      "epoch: 1, batch: 38, loss: 0.10647113621234894\n",
      "epoch: 1, batch: 39, loss: 0.23911461234092712\n",
      "epoch: 1, batch: 40, loss: 0.14973101019859314\n",
      "epoch: 1, batch: 41, loss: 0.24411806464195251\n",
      "epoch: 1, batch: 42, loss: 0.05648017302155495\n",
      "epoch: 1, batch: 43, loss: 0.12121269106864929\n",
      "epoch: 1, batch: 44, loss: 0.2574629783630371\n",
      "epoch: 1, batch: 45, loss: 0.031291812658309937\n",
      "epoch: 1, batch: 46, loss: 0.037467923015356064\n",
      "epoch: 1, batch: 47, loss: 0.07997173815965652\n",
      "epoch: 1, batch: 48, loss: 0.14652220904827118\n",
      "epoch: 1, batch: 49, loss: 0.1536969095468521\n",
      "epoch: 1, batch: 50, loss: 0.037840910255908966\n",
      "epoch: 1, batch: 51, loss: 0.18972563743591309\n",
      "epoch: 1, batch: 52, loss: 0.04395855590701103\n",
      "epoch: 1, batch: 53, loss: 0.08234061300754547\n",
      "epoch: 1, batch: 54, loss: 0.10428524017333984\n",
      "epoch: 1, batch: 55, loss: 0.011669887229800224\n",
      "epoch: 1, batch: 56, loss: 0.09456687420606613\n",
      "epoch: 1, batch: 57, loss: 0.09972448647022247\n",
      "epoch: 1, batch: 58, loss: 0.06507568061351776\n",
      "epoch: 1, batch: 59, loss: 0.04212288185954094\n",
      "epoch: 1, batch: 60, loss: 0.23549379408359528\n",
      "epoch: 1, batch: 61, loss: 0.10752460360527039\n",
      "epoch: 1, batch: 62, loss: 0.04739284887909889\n",
      "epoch: 1, batch: 63, loss: 0.09288414567708969\n",
      "epoch: 1, batch: 64, loss: 0.17646265029907227\n",
      "epoch: 1, batch: 65, loss: 0.20742137730121613\n",
      "epoch: 1, batch: 66, loss: 0.0416375994682312\n",
      "epoch: 1, batch: 67, loss: 0.060834579169750214\n",
      "epoch: 1, batch: 68, loss: 0.062042225152254105\n",
      "epoch: 1, batch: 69, loss: 0.12306078523397446\n",
      "epoch: 1, batch: 70, loss: 0.12384714186191559\n",
      "epoch: 1, batch: 71, loss: 0.06568808853626251\n",
      "epoch: 1, batch: 72, loss: 0.07928904891014099\n",
      "epoch: 1, batch: 73, loss: 0.15361341834068298\n",
      "epoch: 1, batch: 74, loss: 0.1727476418018341\n",
      "epoch: 1, batch: 75, loss: 0.12388897687196732\n",
      "epoch: 1, batch: 76, loss: 0.1665123850107193\n",
      "epoch: 1, batch: 77, loss: 0.13748174905776978\n",
      "epoch: 1, batch: 78, loss: 0.5199874639511108\n",
      "epoch: 1, batch: 79, loss: 0.2998475432395935\n",
      "epoch: 1, batch: 80, loss: 0.13767795264720917\n",
      "epoch: 1, batch: 81, loss: 0.16134369373321533\n",
      "epoch: 1, batch: 82, loss: 0.10056160390377045\n",
      "epoch: 1, batch: 83, loss: 0.08608392626047134\n",
      "epoch: 1, batch: 84, loss: 0.2159348577260971\n",
      "epoch: 1, batch: 85, loss: 0.1310679167509079\n",
      "epoch: 1, batch: 86, loss: 0.13162881135940552\n",
      "epoch: 1, batch: 87, loss: 0.08306620270013809\n",
      "epoch: 1, batch: 88, loss: 0.08518794924020767\n",
      "epoch: 1, batch: 89, loss: 0.04967840760946274\n",
      "epoch: 1, batch: 90, loss: 0.06729932129383087\n",
      "epoch: 1, batch: 91, loss: 0.11865531653165817\n",
      "epoch: 1, batch: 92, loss: 0.04471202194690704\n",
      "epoch: 1, batch: 93, loss: 0.21758021414279938\n",
      "epoch: 1, batch: 94, loss: 0.11219541728496552\n",
      "epoch: 1, batch: 95, loss: 0.18326939642429352\n",
      "epoch: 1, batch: 96, loss: 0.03240720182657242\n",
      "epoch: 1, batch: 97, loss: 0.030638249590992928\n",
      "epoch: 1, batch: 98, loss: 0.05960070714354515\n",
      "epoch: 1, batch: 99, loss: 0.07271062582731247\n",
      "epoch: 1, batch: 100, loss: 0.020338838919997215\n",
      "epoch: 1, batch: 101, loss: 0.10234810411930084\n",
      "epoch: 1, batch: 102, loss: 0.11468586325645447\n",
      "epoch: 1, batch: 103, loss: 0.08753655850887299\n",
      "epoch: 1, batch: 104, loss: 0.08851920813322067\n",
      "epoch: 1, batch: 105, loss: 0.054621513932943344\n",
      "epoch: 1, batch: 106, loss: 0.07722204178571701\n",
      "epoch: 1, batch: 107, loss: 0.2648032307624817\n",
      "epoch: 1, batch: 108, loss: 0.07142478972673416\n",
      "epoch: 1, batch: 109, loss: 0.04678299278020859\n",
      "epoch: 1, batch: 110, loss: 0.0800558477640152\n",
      "epoch: 1, batch: 111, loss: 0.12667672336101532\n",
      "epoch: 1, batch: 112, loss: 0.1107308566570282\n",
      "epoch: 1, batch: 113, loss: 0.04655749350786209\n",
      "epoch: 1, batch: 114, loss: 0.10647914558649063\n",
      "epoch: 1, batch: 115, loss: 0.08996953070163727\n",
      "epoch: 1, batch: 116, loss: 0.06356638669967651\n",
      "epoch: 1, batch: 117, loss: 0.03687875717878342\n",
      "epoch: 1, batch: 118, loss: 0.12188717722892761\n",
      "epoch: 1, batch: 119, loss: 0.15753990411758423\n",
      "epoch: 1, batch: 120, loss: 0.05311160162091255\n",
      "epoch: 1, batch: 121, loss: 0.11645834892988205\n",
      "epoch: 1, batch: 122, loss: 0.062010716646909714\n",
      "epoch: 1, batch: 123, loss: 0.12631095945835114\n",
      "epoch: 1, batch: 124, loss: 0.08543087542057037\n",
      "epoch: 1, batch: 125, loss: 0.09556964039802551\n",
      "epoch: 1, batch: 126, loss: 0.08600184321403503\n",
      "epoch: 1, batch: 127, loss: 0.29660722613334656\n",
      "epoch: 1, batch: 128, loss: 0.029703479260206223\n",
      "epoch: 1, batch: 129, loss: 0.08934064954519272\n",
      "epoch: 1, batch: 130, loss: 0.0234054084867239\n",
      "epoch: 1, batch: 131, loss: 0.14093229174613953\n",
      "epoch: 1, batch: 132, loss: 0.05068163201212883\n",
      "epoch: 1, batch: 133, loss: 0.0576137974858284\n",
      "epoch: 1, batch: 134, loss: 0.07134631276130676\n",
      "epoch: 1, batch: 135, loss: 0.0641607716679573\n",
      "epoch: 1, batch: 136, loss: 0.04633636027574539\n",
      "epoch: 1, batch: 137, loss: 0.08396374434232712\n",
      "epoch: 1, batch: 138, loss: 0.1618790328502655\n",
      "epoch: 1, batch: 139, loss: 0.08201742172241211\n",
      "epoch: 1, batch: 140, loss: 0.07098503410816193\n",
      "epoch: 1, batch: 141, loss: 0.20332062244415283\n",
      "epoch: 1, batch: 142, loss: 0.05079701915383339\n",
      "epoch: 1, batch: 143, loss: 0.11133436113595963\n",
      "epoch: 1, batch: 144, loss: 0.07191282510757446\n",
      "epoch: 1, batch: 145, loss: 0.08898761868476868\n",
      "epoch: 1, batch: 146, loss: 0.03162556514143944\n",
      "epoch: 1, batch: 147, loss: 0.04543016478419304\n",
      "epoch: 1, batch: 148, loss: 0.06249144673347473\n",
      "epoch: 1, batch: 149, loss: 0.07351166009902954\n",
      "epoch: 1, batch: 150, loss: 0.07665500044822693\n",
      "epoch: 1, batch: 151, loss: 0.0496302992105484\n",
      "epoch: 1, batch: 152, loss: 0.06360934674739838\n",
      "epoch: 1, batch: 153, loss: 0.182859867811203\n",
      "epoch: 1, batch: 154, loss: 0.15979869663715363\n",
      "epoch: 1, batch: 155, loss: 0.0620945543050766\n",
      "epoch: 1, batch: 156, loss: 0.06679997593164444\n",
      "epoch: 1, batch: 157, loss: 0.026431946083903313\n",
      "epoch: 1, batch: 158, loss: 0.07525147497653961\n",
      "epoch: 1, batch: 159, loss: 0.0812547504901886\n",
      "epoch: 1, batch: 160, loss: 0.1579020470380783\n",
      "epoch: 1, batch: 161, loss: 0.08803906291723251\n",
      "epoch: 1, batch: 162, loss: 0.057008884847164154\n",
      "epoch: 1, batch: 163, loss: 0.09698522835969925\n",
      "epoch: 1, batch: 164, loss: 0.021673908457159996\n",
      "epoch: 1, batch: 165, loss: 0.06646426767110825\n",
      "epoch: 1, batch: 166, loss: 0.06649363040924072\n",
      "epoch: 1, batch: 167, loss: 0.08282838761806488\n",
      "epoch: 1, batch: 168, loss: 0.03714478015899658\n",
      "epoch: 1, batch: 169, loss: 0.11605324596166611\n",
      "epoch: 1, batch: 170, loss: 0.10242144763469696\n",
      "epoch: 1, batch: 171, loss: 0.01550987921655178\n",
      "epoch: 1, batch: 172, loss: 0.09153971076011658\n",
      "epoch: 1, batch: 173, loss: 0.07646311819553375\n",
      "epoch: 1, batch: 174, loss: 0.07641955465078354\n",
      "epoch: 1, batch: 175, loss: 0.2591075599193573\n",
      "epoch: 1, batch: 176, loss: 0.06644058227539062\n",
      "epoch: 1, batch: 177, loss: 0.08266865462064743\n",
      "epoch: 1, batch: 178, loss: 0.1765260547399521\n",
      "epoch: 1, batch: 179, loss: 0.022751668468117714\n",
      "epoch: 1, batch: 180, loss: 0.06685531884431839\n",
      "epoch: 1, batch: 181, loss: 0.06781551986932755\n",
      "epoch: 1, batch: 182, loss: 0.020209353417158127\n",
      "epoch: 1, batch: 183, loss: 0.2315053790807724\n",
      "epoch: 1, batch: 184, loss: 0.04462571069598198\n",
      "epoch: 1, batch: 185, loss: 0.0865606889128685\n",
      "epoch: 1, batch: 186, loss: 0.2613810896873474\n",
      "epoch: 1, batch: 187, loss: 0.09343551099300385\n",
      "epoch: 1, batch: 188, loss: 0.04104020446538925\n",
      "epoch: 1, batch: 189, loss: 0.1015196442604065\n",
      "epoch: 1, batch: 190, loss: 0.09833458811044693\n",
      "epoch: 1, batch: 191, loss: 0.0885021835565567\n",
      "epoch: 1, batch: 192, loss: 0.1322672814130783\n",
      "epoch: 1, batch: 193, loss: 0.077208511531353\n",
      "epoch: 1, batch: 194, loss: 0.2760559320449829\n",
      "epoch: 1, batch: 195, loss: 0.12013466656208038\n",
      "epoch: 1, batch: 196, loss: 0.21532884240150452\n",
      "epoch: 1, batch: 197, loss: 0.09068065881729126\n",
      "epoch: 1, batch: 198, loss: 0.019321486353874207\n",
      "epoch: 1, batch: 199, loss: 0.15096931159496307\n",
      "epoch: 1, batch: 200, loss: 0.08665427565574646\n",
      "epoch: 1, batch: 201, loss: 0.07405149191617966\n",
      "epoch: 1, batch: 202, loss: 0.02348284423351288\n",
      "epoch: 1, batch: 203, loss: 0.043659456074237823\n",
      "epoch: 1, batch: 204, loss: 0.03661082684993744\n",
      "epoch: 1, batch: 205, loss: 0.1554783135652542\n",
      "epoch: 1, batch: 206, loss: 0.16488119959831238\n",
      "epoch: 1, batch: 207, loss: 0.14821451902389526\n",
      "epoch: 1, batch: 208, loss: 0.061555806547403336\n",
      "epoch: 1, batch: 209, loss: 0.060329947620630264\n",
      "epoch: 1, batch: 210, loss: 0.1420994997024536\n",
      "epoch: 1, batch: 211, loss: 0.051108259707689285\n",
      "epoch: 1, batch: 212, loss: 0.142119362950325\n",
      "epoch: 1, batch: 213, loss: 0.04150291159749031\n",
      "epoch: 1, batch: 214, loss: 0.06251108646392822\n",
      "epoch: 1, batch: 215, loss: 0.03778858855366707\n",
      "epoch: 1, batch: 216, loss: 0.06393555551767349\n",
      "epoch: 1, batch: 217, loss: 0.09873779118061066\n",
      "epoch: 1, batch: 218, loss: 0.1671888381242752\n",
      "epoch: 1, batch: 219, loss: 0.08162633329629898\n",
      "epoch: 1, batch: 220, loss: 0.03726175054907799\n",
      "epoch: 1, batch: 221, loss: 0.0737520158290863\n",
      "epoch: 1, batch: 222, loss: 0.06108582764863968\n",
      "epoch: 1, batch: 223, loss: 0.011111393570899963\n",
      "epoch: 1, batch: 224, loss: 0.07250265777111053\n",
      "epoch: 1, batch: 225, loss: 0.12957675755023956\n",
      "epoch: 1, batch: 226, loss: 0.08902653306722641\n",
      "epoch: 1, batch: 227, loss: 0.149276465177536\n",
      "epoch: 1, batch: 228, loss: 0.09296400845050812\n",
      "epoch: 1, batch: 229, loss: 0.24026376008987427\n",
      "epoch: 1, batch: 230, loss: 0.07555047422647476\n",
      "epoch: 1, batch: 231, loss: 0.07359954714775085\n",
      "epoch: 1, batch: 232, loss: 0.08545400202274323\n",
      "epoch: 1, batch: 233, loss: 0.09700820595026016\n",
      "epoch: 1, batch: 234, loss: 0.25418898463249207\n",
      "epoch: 1, batch: 235, loss: 0.056961625814437866\n",
      "epoch: 1, batch: 236, loss: 0.037857819348573685\n",
      "epoch: 1, batch: 237, loss: 0.0881757065653801\n",
      "epoch: 1, batch: 238, loss: 0.15623725950717926\n",
      "epoch: 1, batch: 239, loss: 0.04783525690436363\n",
      "epoch: 1, batch: 240, loss: 0.06605260074138641\n",
      "epoch: 1, batch: 241, loss: 0.07906058430671692\n",
      "epoch: 1, batch: 242, loss: 0.07081321626901627\n",
      "epoch: 1, batch: 243, loss: 0.08241468667984009\n",
      "epoch: 1, batch: 244, loss: 0.02758907340466976\n",
      "epoch: 1, batch: 245, loss: 0.06678410619497299\n",
      "epoch: 1, batch: 246, loss: 0.011804169975221157\n",
      "epoch: 1, batch: 247, loss: 0.08861785382032394\n",
      "epoch: 1, batch: 248, loss: 0.06274405866861343\n",
      "epoch: 1, batch: 249, loss: 0.03355681151151657\n",
      "epoch: 1, batch: 250, loss: 0.17514944076538086\n",
      "epoch: 1, batch: 251, loss: 0.40343746542930603\n",
      "epoch: 1, batch: 252, loss: 0.16532647609710693\n",
      "epoch: 1, batch: 253, loss: 0.1464344561100006\n",
      "epoch: 1, batch: 254, loss: 0.07150086760520935\n",
      "epoch: 1, batch: 255, loss: 0.0689425989985466\n",
      "epoch: 1, batch: 256, loss: 0.020938606932759285\n",
      "epoch: 1, batch: 257, loss: 0.2085583359003067\n",
      "epoch: 1, batch: 258, loss: 0.06384184956550598\n",
      "epoch: 1, batch: 259, loss: 0.08984796702861786\n",
      "epoch: 1, batch: 260, loss: 0.1232001855969429\n",
      "epoch: 1, batch: 261, loss: 0.17538148164749146\n",
      "epoch: 1, batch: 262, loss: 0.10002561658620834\n",
      "epoch: 1, batch: 263, loss: 0.06939945369958878\n",
      "epoch: 1, batch: 264, loss: 0.07107765227556229\n",
      "epoch: 1, batch: 265, loss: 0.22484862804412842\n",
      "epoch: 1, batch: 266, loss: 0.23654133081436157\n",
      "epoch: 1, batch: 267, loss: 0.19043594598770142\n",
      "epoch: 1, batch: 268, loss: 0.05197381600737572\n",
      "epoch: 1, batch: 269, loss: 0.2464914619922638\n",
      "epoch: 1, batch: 270, loss: 0.044687669724226\n",
      "epoch: 1, batch: 271, loss: 0.07112899422645569\n",
      "epoch: 1, batch: 272, loss: 0.08737698942422867\n",
      "epoch: 1, batch: 273, loss: 0.10577327013015747\n",
      "epoch: 1, batch: 274, loss: 0.11868599057197571\n",
      "epoch: 1, batch: 275, loss: 0.08917343616485596\n",
      "epoch: 1, batch: 276, loss: 0.20947599411010742\n",
      "epoch: 1, batch: 277, loss: 0.13266941905021667\n",
      "epoch: 1, batch: 278, loss: 0.07123546302318573\n",
      "epoch: 1, batch: 279, loss: 0.08586316555738449\n",
      "epoch: 1, batch: 280, loss: 0.03563537448644638\n",
      "epoch: 1, batch: 281, loss: 0.02726149559020996\n",
      "epoch: 1, batch: 282, loss: 0.041457679122686386\n",
      "epoch: 1, batch: 283, loss: 0.09338584542274475\n",
      "epoch: 1, batch: 284, loss: 0.07750716060400009\n",
      "epoch: 1, batch: 285, loss: 0.0924285426735878\n",
      "epoch: 1, batch: 286, loss: 0.0935949981212616\n",
      "epoch: 1, batch: 287, loss: 0.08245323598384857\n",
      "epoch: 1, batch: 288, loss: 0.2338402271270752\n",
      "epoch: 1, batch: 289, loss: 0.06514673680067062\n",
      "epoch: 1, batch: 290, loss: 0.06512156128883362\n",
      "epoch: 1, batch: 291, loss: 0.016700267791748047\n",
      "epoch: 1, batch: 292, loss: 0.02585667558014393\n",
      "epoch: 1, batch: 293, loss: 0.10057202726602554\n",
      "epoch: 1, batch: 294, loss: 0.0508183091878891\n",
      "epoch: 1, batch: 295, loss: 0.0731147974729538\n",
      "epoch: 1, batch: 296, loss: 0.16182199120521545\n",
      "epoch: 1, batch: 297, loss: 0.06601941585540771\n",
      "epoch: 1, batch: 298, loss: 0.12484326958656311\n",
      "epoch: 1, batch: 299, loss: 0.15472330152988434\n",
      "epoch: 1, batch: 300, loss: 0.03669309988617897\n",
      "epoch: 1, batch: 301, loss: 0.05105706304311752\n",
      "epoch: 1, batch: 302, loss: 0.05934201553463936\n",
      "epoch: 1, batch: 303, loss: 0.20078670978546143\n",
      "epoch: 1, batch: 304, loss: 0.12891755998134613\n",
      "epoch: 1, batch: 305, loss: 0.04364927113056183\n",
      "epoch: 1, batch: 306, loss: 0.15358662605285645\n",
      "epoch: 1, batch: 307, loss: 0.10436155647039413\n",
      "epoch: 1, batch: 308, loss: 0.015447823330760002\n",
      "epoch: 1, batch: 309, loss: 0.04945116490125656\n",
      "epoch: 1, batch: 310, loss: 0.1590559184551239\n",
      "epoch: 1, batch: 311, loss: 0.10843069106340408\n",
      "epoch: 1, batch: 312, loss: 0.05890916660428047\n",
      "epoch: 1, batch: 313, loss: 0.30170005559921265\n",
      "epoch: 1, batch: 314, loss: 0.05687803030014038\n",
      "epoch: 1, batch: 315, loss: 0.10118941962718964\n",
      "epoch: 1, batch: 316, loss: 0.09398500621318817\n",
      "epoch: 1, batch: 317, loss: 0.06306397914886475\n",
      "epoch: 1, batch: 318, loss: 0.09227599203586578\n",
      "epoch: 1, batch: 319, loss: 0.08186466991901398\n",
      "epoch: 1, batch: 320, loss: 0.11002930253744125\n",
      "epoch: 1, batch: 321, loss: 0.06774197518825531\n",
      "epoch: 1, batch: 322, loss: 0.03423247113823891\n",
      "epoch: 1, batch: 323, loss: 0.11463767290115356\n",
      "epoch: 1, batch: 324, loss: 0.04049386829137802\n",
      "epoch: 1, batch: 325, loss: 0.07014240324497223\n",
      "epoch: 1, batch: 326, loss: 0.03745906800031662\n",
      "epoch: 1, batch: 327, loss: 0.06530922651290894\n",
      "epoch: 1, batch: 328, loss: 0.03370026499032974\n",
      "epoch: 1, batch: 329, loss: 0.05852287635207176\n",
      "epoch: 1, batch: 330, loss: 0.06178911402821541\n",
      "epoch: 1, batch: 331, loss: 0.05043041706085205\n",
      "epoch: 1, batch: 332, loss: 0.1857779622077942\n",
      "epoch: 1, batch: 333, loss: 0.13974294066429138\n",
      "epoch: 1, batch: 334, loss: 0.1710796356201172\n",
      "epoch: 1, batch: 335, loss: 0.11396621912717819\n",
      "epoch: 1, batch: 336, loss: 0.03671064227819443\n",
      "epoch: 1, batch: 337, loss: 0.05964813753962517\n",
      "epoch: 1, batch: 338, loss: 0.08881436288356781\n",
      "epoch: 1, batch: 339, loss: 0.26623138785362244\n",
      "epoch: 1, batch: 340, loss: 0.12425991892814636\n",
      "epoch: 1, batch: 341, loss: 0.11322519928216934\n",
      "epoch: 1, batch: 342, loss: 0.225606769323349\n",
      "epoch: 1, batch: 343, loss: 0.11943786591291428\n",
      "epoch: 1, batch: 344, loss: 0.047911763191223145\n",
      "epoch: 1, batch: 345, loss: 0.10203737765550613\n",
      "epoch: 1, batch: 346, loss: 0.08339592814445496\n",
      "epoch: 1, batch: 347, loss: 0.11593642085790634\n",
      "epoch: 1, batch: 348, loss: 0.07361535727977753\n",
      "epoch: 1, batch: 349, loss: 0.08214663714170456\n",
      "epoch: 1, batch: 350, loss: 0.18179680407047272\n",
      "epoch: 1, batch: 351, loss: 0.06475991755723953\n",
      "epoch: 1, batch: 352, loss: 0.0428176186978817\n",
      "epoch: 1, batch: 353, loss: 0.22409148514270782\n",
      "epoch: 1, batch: 354, loss: 0.054909221827983856\n",
      "epoch: 1, batch: 355, loss: 0.043952737003564835\n",
      "epoch: 1, batch: 356, loss: 0.030763277783989906\n",
      "epoch: 1, batch: 357, loss: 0.05834522843360901\n",
      "epoch: 1, batch: 358, loss: 0.12054990977048874\n",
      "epoch: 1, batch: 359, loss: 0.098628930747509\n",
      "epoch: 1, batch: 360, loss: 0.18442335724830627\n",
      "epoch: 1, batch: 361, loss: 0.05487031862139702\n",
      "epoch: 1, batch: 362, loss: 0.18652009963989258\n",
      "epoch: 1, batch: 363, loss: 0.09165257215499878\n",
      "epoch: 1, batch: 364, loss: 0.1562526822090149\n",
      "epoch: 1, batch: 365, loss: 0.10527263581752777\n",
      "epoch: 1, batch: 366, loss: 0.03010239452123642\n",
      "epoch: 1, batch: 367, loss: 0.05964292958378792\n",
      "epoch: 1, batch: 368, loss: 0.08075113594532013\n",
      "epoch: 1, batch: 369, loss: 0.0739176794886589\n",
      "epoch: 1, batch: 370, loss: 0.061576999723911285\n",
      "epoch: 1, batch: 371, loss: 0.12884776294231415\n",
      "epoch: 1, batch: 372, loss: 0.08901585638523102\n",
      "epoch: 1, batch: 373, loss: 0.06477665901184082\n",
      "epoch: 1, batch: 374, loss: 0.09040743112564087\n",
      "epoch: 1, batch: 375, loss: 0.12905849516391754\n",
      "epoch: 1, batch: 376, loss: 0.13808280229568481\n",
      "epoch: 1, batch: 377, loss: 0.14654113352298737\n",
      "epoch: 1, batch: 378, loss: 0.08514131605625153\n",
      "epoch: 1, batch: 379, loss: 0.10251009464263916\n",
      "epoch: 1, batch: 380, loss: 0.13509109616279602\n",
      "epoch: 1, batch: 381, loss: 0.06643560528755188\n",
      "epoch: 1, batch: 382, loss: 0.10475928336381912\n",
      "epoch: 1, batch: 383, loss: 0.07188547402620316\n",
      "epoch: 1, batch: 384, loss: 0.05495624616742134\n",
      "epoch: 1, batch: 385, loss: 0.16810835897922516\n",
      "epoch: 1, batch: 386, loss: 0.10465941578149796\n",
      "epoch: 1, batch: 387, loss: 0.04174716770648956\n",
      "epoch: 1, batch: 388, loss: 0.13266810774803162\n",
      "epoch: 1, batch: 389, loss: 0.18236492574214935\n",
      "epoch: 1, batch: 390, loss: 0.019905539229512215\n",
      "epoch: 1, batch: 391, loss: 0.03801494836807251\n",
      "epoch: 1, batch: 392, loss: 0.13714466989040375\n",
      "epoch: 1, batch: 393, loss: 0.03618545085191727\n",
      "epoch: 1, batch: 394, loss: 0.05281965062022209\n",
      "epoch: 1, batch: 395, loss: 0.05755508691072464\n",
      "epoch: 1, batch: 396, loss: 0.06682635843753815\n",
      "epoch: 1, batch: 397, loss: 0.0520925372838974\n",
      "epoch: 1, batch: 398, loss: 0.02825375832617283\n",
      "epoch: 1, batch: 399, loss: 0.023938218131661415\n",
      "epoch: 1, batch: 400, loss: 0.047032374888658524\n",
      "epoch: 1, batch: 401, loss: 0.10914342850446701\n",
      "epoch: 1, batch: 402, loss: 0.08886542916297913\n",
      "epoch: 1, batch: 403, loss: 0.033757008612155914\n",
      "epoch: 1, batch: 404, loss: 0.0878094807267189\n",
      "epoch: 1, batch: 405, loss: 0.2120998650789261\n",
      "epoch: 1, batch: 406, loss: 0.10159028321504593\n",
      "epoch: 1, batch: 407, loss: 0.10561860352754593\n",
      "epoch: 1, batch: 408, loss: 0.09995010495185852\n",
      "epoch: 1, batch: 409, loss: 0.02070588991045952\n",
      "epoch: 1, batch: 410, loss: 0.05288130044937134\n",
      "epoch: 1, batch: 411, loss: 0.0382809042930603\n",
      "epoch: 1, batch: 412, loss: 0.1236877590417862\n",
      "epoch: 1, batch: 413, loss: 0.10659237205982208\n",
      "epoch: 1, batch: 414, loss: 0.11683984100818634\n",
      "epoch: 1, batch: 415, loss: 0.04047394171357155\n",
      "epoch: 1, batch: 416, loss: 0.20252053439617157\n",
      "epoch: 1, batch: 417, loss: 0.05281179025769234\n",
      "epoch: 1, batch: 418, loss: 0.09570520371198654\n",
      "epoch: 1, batch: 419, loss: 0.07718180865049362\n",
      "epoch: 1, batch: 420, loss: 0.061910368502140045\n",
      "epoch: 1, batch: 421, loss: 0.06233440339565277\n",
      "epoch: 1, batch: 422, loss: 0.08884619176387787\n",
      "epoch: 1, batch: 423, loss: 0.07403241842985153\n",
      "epoch: 1, batch: 424, loss: 0.07794898003339767\n",
      "epoch: 1, batch: 425, loss: 0.050739649683237076\n",
      "epoch: 1, batch: 426, loss: 0.06565895676612854\n",
      "epoch: 1, batch: 427, loss: 0.04608657583594322\n",
      "epoch: 1, batch: 428, loss: 0.1657915860414505\n",
      "epoch: 1, batch: 429, loss: 0.04878309741616249\n",
      "epoch: 1, batch: 430, loss: 0.06596989184617996\n",
      "epoch: 1, batch: 431, loss: 0.045328665524721146\n",
      "epoch: 1, batch: 432, loss: 0.02441379800438881\n",
      "epoch: 1, batch: 433, loss: 0.031724635511636734\n",
      "epoch: 1, batch: 434, loss: 0.12903276085853577\n",
      "epoch: 1, batch: 435, loss: 0.03328506648540497\n",
      "epoch: 1, batch: 436, loss: 0.14428119361400604\n",
      "epoch: 1, batch: 437, loss: 0.11147157102823257\n",
      "epoch: 1, batch: 438, loss: 0.2110072523355484\n",
      "epoch: 1, batch: 439, loss: 0.08376646041870117\n",
      "epoch: 1, batch: 440, loss: 0.12840352952480316\n",
      "epoch: 1, batch: 441, loss: 0.03966964781284332\n",
      "epoch: 1, batch: 442, loss: 0.11223755031824112\n",
      "epoch: 1, batch: 443, loss: 0.11529481410980225\n",
      "epoch: 1, batch: 444, loss: 0.05375447869300842\n",
      "epoch: 1, batch: 445, loss: 0.14023487269878387\n",
      "epoch: 1, batch: 446, loss: 0.03148507699370384\n",
      "epoch: 1, batch: 447, loss: 0.2943398654460907\n",
      "epoch: 1, batch: 448, loss: 0.06026053801178932\n",
      "epoch: 1, batch: 449, loss: 0.05165499076247215\n",
      "epoch: 1, batch: 450, loss: 0.15983615815639496\n",
      "epoch: 1, batch: 451, loss: 0.09280043840408325\n",
      "epoch: 1, batch: 452, loss: 0.16130682826042175\n",
      "epoch: 1, batch: 453, loss: 0.0777469351887703\n",
      "epoch: 1, batch: 454, loss: 0.09533858299255371\n",
      "epoch: 1, batch: 455, loss: 0.04829326644539833\n",
      "epoch: 1, batch: 456, loss: 0.071712426841259\n",
      "epoch: 1, batch: 457, loss: 0.06650497019290924\n",
      "epoch: 1, batch: 458, loss: 0.08472508192062378\n",
      "epoch: 1, batch: 459, loss: 0.060442715883255005\n",
      "epoch: 1, batch: 460, loss: 0.11881890147924423\n",
      "epoch: 1, batch: 461, loss: 0.12275534868240356\n",
      "epoch: 1, batch: 462, loss: 0.03421780839562416\n",
      "epoch: 1, batch: 463, loss: 0.06560099869966507\n",
      "epoch: 1, batch: 464, loss: 0.08166968077421188\n",
      "epoch: 1, batch: 465, loss: 0.093944251537323\n",
      "epoch: 1, batch: 466, loss: 0.06236739084124565\n",
      "epoch: 1, batch: 467, loss: 0.062348537147045135\n",
      "epoch: 1, batch: 468, loss: 0.19483539462089539\n",
      "epoch: 1, batch: 469, loss: 0.18517819046974182\n",
      "epoch: 1, batch: 470, loss: 0.02150772511959076\n",
      "epoch: 1, batch: 471, loss: 0.045281119644641876\n",
      "epoch: 1, batch: 472, loss: 0.030912281945347786\n",
      "epoch: 1, batch: 473, loss: 0.07686033099889755\n",
      "epoch: 1, batch: 474, loss: 0.062439821660518646\n",
      "epoch: 1, batch: 475, loss: 0.33480656147003174\n",
      "epoch: 1, batch: 476, loss: 0.0652865618467331\n",
      "epoch: 1, batch: 477, loss: 0.09824687242507935\n",
      "epoch: 1, batch: 478, loss: 0.05571691691875458\n",
      "epoch: 1, batch: 479, loss: 0.04388085752725601\n",
      "epoch: 1, batch: 480, loss: 0.08700120449066162\n",
      "epoch: 1, batch: 481, loss: 0.026470806449651718\n",
      "epoch: 1, batch: 482, loss: 0.19731570780277252\n",
      "epoch: 1, batch: 483, loss: 0.10851738601922989\n",
      "epoch: 1, batch: 484, loss: 0.06289742887020111\n",
      "epoch: 1, batch: 485, loss: 0.11303699016571045\n",
      "epoch: 1, batch: 486, loss: 0.0659160166978836\n",
      "epoch: 1, batch: 487, loss: 0.07769563049077988\n",
      "epoch: 1, batch: 488, loss: 0.04933478310704231\n",
      "epoch: 1, batch: 489, loss: 0.031334344297647476\n",
      "epoch: 1, batch: 490, loss: 0.025140581652522087\n",
      "epoch: 1, batch: 491, loss: 0.03772301971912384\n",
      "epoch: 1, batch: 492, loss: 0.03100154921412468\n",
      "epoch: 1, batch: 493, loss: 0.08433951437473297\n",
      "epoch: 1, batch: 494, loss: 0.035886000841856\n",
      "epoch: 1, batch: 495, loss: 0.05604099854826927\n",
      "epoch: 1, batch: 496, loss: 0.14633071422576904\n",
      "epoch: 1, batch: 497, loss: 0.1393274962902069\n",
      "epoch: 1, batch: 498, loss: 0.01282569020986557\n",
      "epoch: 1, batch: 499, loss: 0.14617902040481567\n",
      "epoch: 1, batch: 500, loss: 0.0593070313334465\n",
      "epoch: 1, batch: 501, loss: 0.0675952285528183\n",
      "epoch: 1, batch: 502, loss: 0.11282490938901901\n",
      "epoch: 1, batch: 503, loss: 0.09091378003358841\n",
      "epoch: 1, batch: 504, loss: 0.05569564178586006\n",
      "epoch: 1, batch: 505, loss: 0.09431816637516022\n",
      "epoch: 1, batch: 506, loss: 0.10039698332548141\n",
      "epoch: 1, batch: 507, loss: 0.1338224858045578\n",
      "epoch: 1, batch: 508, loss: 0.0670715719461441\n",
      "epoch: 1, batch: 509, loss: 0.048563454300165176\n",
      "epoch: 1, batch: 510, loss: 0.06496407091617584\n",
      "epoch: 1, batch: 511, loss: 0.05245836079120636\n",
      "epoch: 1, batch: 512, loss: 0.06125533580780029\n",
      "epoch: 1, batch: 513, loss: 0.04206232726573944\n",
      "epoch: 1, batch: 514, loss: 0.08412852883338928\n",
      "epoch: 1, batch: 515, loss: 0.01805424690246582\n",
      "epoch: 1, batch: 516, loss: 0.04794833064079285\n",
      "epoch: 1, batch: 517, loss: 0.13563314080238342\n",
      "epoch: 1, batch: 518, loss: 0.03236965090036392\n",
      "epoch: 1, batch: 519, loss: 0.1059206873178482\n",
      "epoch: 1, batch: 520, loss: 0.1551506668329239\n",
      "epoch: 1, batch: 521, loss: 0.11638157814741135\n",
      "epoch: 1, batch: 522, loss: 0.05958773195743561\n",
      "epoch: 1, batch: 523, loss: 0.07868316024541855\n",
      "epoch: 1, batch: 524, loss: 0.2024196982383728\n",
      "epoch: 1, batch: 525, loss: 0.16749556362628937\n",
      "epoch: 1, batch: 526, loss: 0.08058895170688629\n",
      "epoch: 1, batch: 527, loss: 0.2033989429473877\n",
      "epoch: 1, batch: 528, loss: 0.06076616421341896\n",
      "epoch: 1, batch: 529, loss: 0.14227262139320374\n",
      "epoch: 1, batch: 530, loss: 0.04826660081744194\n",
      "epoch: 1, batch: 531, loss: 0.11678767204284668\n",
      "epoch: 1, batch: 532, loss: 0.03851153701543808\n",
      "epoch: 1, batch: 533, loss: 0.08458258211612701\n",
      "epoch: 1, batch: 534, loss: 0.20542967319488525\n",
      "epoch: 1, batch: 535, loss: 0.054204415529966354\n",
      "epoch: 1, batch: 536, loss: 0.08617767691612244\n",
      "epoch: 1, batch: 537, loss: 0.2290223091840744\n",
      "epoch: 1, batch: 538, loss: 0.0563608817756176\n",
      "epoch: 1, batch: 539, loss: 0.02250554971396923\n",
      "epoch: 1, batch: 540, loss: 0.0684746578335762\n",
      "epoch: 1, batch: 541, loss: 0.044180456548929214\n",
      "epoch: 1, batch: 542, loss: 0.06209799647331238\n",
      "epoch: 1, batch: 543, loss: 0.2820168733596802\n",
      "epoch: 1, batch: 544, loss: 0.11667367815971375\n",
      "epoch: 1, batch: 545, loss: 0.013603831641376019\n",
      "epoch: 1, batch: 546, loss: 0.10259592533111572\n",
      "epoch: 1, batch: 547, loss: 0.05995624512434006\n",
      "epoch: 1, batch: 548, loss: 0.07249931991100311\n",
      "epoch: 1, batch: 549, loss: 0.12512700259685516\n",
      "epoch: 1, batch: 550, loss: 0.0258015263825655\n",
      "epoch: 1, batch: 551, loss: 0.05951375141739845\n",
      "epoch: 1, batch: 552, loss: 0.1294887214899063\n",
      "epoch: 1, batch: 553, loss: 0.09438306093215942\n",
      "epoch: 1, batch: 554, loss: 0.0945671945810318\n",
      "epoch: 1, batch: 555, loss: 0.13824385404586792\n",
      "epoch: 1, batch: 556, loss: 0.05479905754327774\n",
      "epoch: 1, batch: 557, loss: 0.10466229915618896\n",
      "epoch: 1, batch: 558, loss: 0.09303721785545349\n",
      "epoch: 1, batch: 559, loss: 0.10861515998840332\n",
      "epoch: 1, batch: 560, loss: 0.0831298828125\n",
      "epoch: 1, batch: 561, loss: 0.03459872305393219\n",
      "epoch: 1, batch: 562, loss: 0.03622671216726303\n",
      "epoch: 1, batch: 563, loss: 0.05257032811641693\n",
      "epoch: 1, batch: 564, loss: 0.07079894840717316\n",
      "epoch: 1, batch: 565, loss: 0.05247299745678902\n",
      "epoch: 1, batch: 566, loss: 0.13689903914928436\n",
      "epoch: 1, batch: 567, loss: 0.08177485316991806\n",
      "epoch: 1, batch: 568, loss: 0.09593649953603745\n",
      "epoch: 1, batch: 569, loss: 0.04312218353152275\n",
      "epoch: 1, batch: 570, loss: 0.1090254858136177\n",
      "epoch: 1, batch: 571, loss: 0.058798085898160934\n",
      "epoch: 1, batch: 572, loss: 0.1136590912938118\n",
      "epoch: 1, batch: 573, loss: 0.05375191196799278\n",
      "epoch: 1, batch: 574, loss: 0.15461602807044983\n",
      "epoch: 1, batch: 575, loss: 0.08819811046123505\n",
      "epoch: 1, batch: 576, loss: 0.061523787677288055\n",
      "epoch: 1, batch: 577, loss: 0.28699445724487305\n",
      "epoch: 1, batch: 578, loss: 0.1445571333169937\n",
      "epoch: 1, batch: 579, loss: 0.033530496060848236\n",
      "epoch: 1, batch: 580, loss: 0.08887450397014618\n",
      "epoch: 1, batch: 581, loss: 0.05476483702659607\n",
      "epoch: 1, batch: 582, loss: 0.02302105911076069\n",
      "epoch: 1, batch: 583, loss: 0.15451255440711975\n",
      "epoch: 1, batch: 584, loss: 0.10746955871582031\n",
      "epoch: 1, batch: 585, loss: 0.06229768693447113\n",
      "epoch: 1, batch: 586, loss: 0.08914497494697571\n",
      "epoch: 1, batch: 587, loss: 0.11893928796052933\n",
      "epoch: 1, batch: 588, loss: 0.06291057169437408\n",
      "epoch: 1, batch: 589, loss: 0.03852507472038269\n",
      "epoch: 1, batch: 590, loss: 0.10870222002267838\n",
      "epoch: 1, batch: 591, loss: 0.05968967825174332\n",
      "epoch: 1, batch: 592, loss: 0.02200346812605858\n",
      "epoch: 1, batch: 593, loss: 0.11453039944171906\n",
      "epoch: 1, batch: 594, loss: 0.03933090344071388\n",
      "epoch: 1, batch: 595, loss: 0.054021019488573074\n",
      "epoch: 1, batch: 596, loss: 0.11460573226213455\n",
      "epoch: 1, batch: 597, loss: 0.1572587639093399\n",
      "epoch: 1, batch: 598, loss: 0.11810857802629471\n",
      "epoch: 1, batch: 599, loss: 0.05781693384051323\n",
      "epoch: 1, batch: 600, loss: 0.016820166260004044\n",
      "epoch: 1, batch: 601, loss: 0.057840846478939056\n",
      "epoch: 1, batch: 602, loss: 0.03570165857672691\n",
      "epoch: 1, batch: 603, loss: 0.07127486914396286\n",
      "epoch: 1, batch: 604, loss: 0.06678315252065659\n",
      "epoch: 1, batch: 605, loss: 0.018451660871505737\n",
      "epoch: 1, batch: 606, loss: 0.035714056342840195\n",
      "epoch: 1, batch: 607, loss: 0.17082127928733826\n",
      "epoch: 1, batch: 608, loss: 0.05893240496516228\n",
      "epoch: 1, batch: 609, loss: 0.014315430074930191\n",
      "epoch: 1, batch: 610, loss: 0.1600024402141571\n",
      "epoch: 1, batch: 611, loss: 0.07597345113754272\n",
      "epoch: 1, batch: 612, loss: 0.02715609408915043\n",
      "epoch: 1, batch: 613, loss: 0.0966714397072792\n",
      "epoch: 1, batch: 614, loss: 0.059475917369127274\n",
      "epoch: 1, batch: 615, loss: 0.07267224043607712\n",
      "epoch: 1, batch: 616, loss: 0.02644813060760498\n",
      "epoch: 1, batch: 617, loss: 0.1481454074382782\n",
      "epoch: 1, batch: 618, loss: 0.10603432357311249\n",
      "epoch: 1, batch: 619, loss: 0.03367377072572708\n",
      "epoch: 1, batch: 620, loss: 0.03784650191664696\n",
      "epoch: 1, batch: 621, loss: 0.17181240022182465\n",
      "epoch: 1, batch: 622, loss: 0.08019036054611206\n",
      "epoch: 1, batch: 623, loss: 0.02915019728243351\n",
      "epoch: 1, batch: 624, loss: 0.1115649864077568\n",
      "epoch: 1, batch: 625, loss: 0.11631074547767639\n",
      "epoch: 1, batch: 626, loss: 0.1782979667186737\n",
      "epoch: 1, batch: 627, loss: 0.10023988038301468\n",
      "epoch: 1, batch: 628, loss: 0.09999598562717438\n",
      "epoch: 1, batch: 629, loss: 0.04037623479962349\n",
      "epoch: 1, batch: 630, loss: 0.06482511013746262\n",
      "epoch: 1, batch: 631, loss: 0.09423372894525528\n",
      "epoch: 1, batch: 632, loss: 0.22729051113128662\n",
      "epoch: 1, batch: 633, loss: 0.036901626735925674\n",
      "epoch: 1, batch: 634, loss: 0.09653368592262268\n",
      "epoch: 1, batch: 635, loss: 0.04199543222784996\n",
      "epoch: 1, batch: 636, loss: 0.12130621075630188\n",
      "epoch: 1, batch: 637, loss: 0.09102029353380203\n",
      "epoch: 1, batch: 638, loss: 0.037635404616594315\n",
      "epoch: 1, batch: 639, loss: 0.06617389619350433\n",
      "epoch: 1, batch: 640, loss: 0.13793905079364777\n",
      "epoch: 1, batch: 641, loss: 0.07089663296937943\n",
      "epoch: 1, batch: 642, loss: 0.021615751087665558\n",
      "epoch: 1, batch: 643, loss: 0.04014429450035095\n",
      "epoch: 1, batch: 644, loss: 0.19996748864650726\n",
      "epoch: 1, batch: 645, loss: 0.09621977806091309\n",
      "epoch: 1, batch: 646, loss: 0.05242568999528885\n",
      "epoch: 1, batch: 647, loss: 0.046460941433906555\n",
      "epoch: 1, batch: 648, loss: 0.18107402324676514\n",
      "epoch: 1, batch: 649, loss: 0.152919203042984\n",
      "epoch: 1, batch: 650, loss: 0.06274330615997314\n",
      "epoch: 1, batch: 651, loss: 0.04802107810974121\n",
      "epoch: 1, batch: 652, loss: 0.21881790459156036\n",
      "epoch: 1, batch: 653, loss: 0.06248512491583824\n",
      "epoch: 1, batch: 654, loss: 0.06385444849729538\n",
      "epoch: 1, batch: 655, loss: 0.1057073101401329\n",
      "epoch: 1, batch: 656, loss: 0.05912492051720619\n",
      "epoch: 1, batch: 657, loss: 0.05155603587627411\n",
      "epoch: 1, batch: 658, loss: 0.05279618129134178\n",
      "epoch: 1, batch: 659, loss: 0.034033533185720444\n",
      "epoch: 1, batch: 660, loss: 0.027997324243187904\n",
      "epoch: 1, batch: 661, loss: 0.040748026221990585\n",
      "epoch: 1, batch: 662, loss: 0.06613466888666153\n",
      "epoch: 1, batch: 663, loss: 0.1307435780763626\n",
      "epoch: 1, batch: 664, loss: 0.04432622715830803\n",
      "epoch: 1, batch: 665, loss: 0.017573542892932892\n",
      "epoch: 1, batch: 666, loss: 0.026723312214016914\n",
      "epoch: 1, batch: 667, loss: 0.04902218282222748\n",
      "epoch: 1, batch: 668, loss: 0.06367690861225128\n",
      "epoch: 1, batch: 669, loss: 0.045246563851833344\n",
      "epoch: 1, batch: 670, loss: 0.18326911330223083\n",
      "epoch: 1, batch: 671, loss: 0.02199203334748745\n",
      "epoch: 1, batch: 672, loss: 0.06689010560512543\n",
      "epoch: 1, batch: 673, loss: 0.13009703159332275\n",
      "epoch: 1, batch: 674, loss: 0.1268724501132965\n",
      "epoch: 1, batch: 675, loss: 0.20272250473499298\n",
      "epoch: 1, batch: 676, loss: 0.06627021729946136\n",
      "epoch: 1, batch: 677, loss: 0.06659304350614548\n",
      "epoch: 1, batch: 678, loss: 0.17197535932064056\n",
      "epoch: 1, batch: 679, loss: 0.07450545579195023\n",
      "epoch: 1, batch: 680, loss: 0.11881440877914429\n",
      "epoch: 1, batch: 681, loss: 0.14066900312900543\n",
      "epoch: 1, batch: 682, loss: 0.010848128236830235\n",
      "epoch: 1, batch: 683, loss: 0.03791806474328041\n",
      "epoch: 1, batch: 684, loss: 0.1266186386346817\n",
      "epoch: 1, batch: 685, loss: 0.14303189516067505\n",
      "epoch: 1, batch: 686, loss: 0.10446701943874359\n",
      "epoch: 1, batch: 687, loss: 0.0490870475769043\n",
      "epoch: 1, batch: 688, loss: 0.026366522535681725\n",
      "epoch: 1, batch: 689, loss: 0.07109469175338745\n",
      "epoch: 1, batch: 690, loss: 0.03063696064054966\n",
      "epoch: 1, batch: 691, loss: 0.09321480244398117\n",
      "epoch: 1, batch: 692, loss: 0.03785787150263786\n",
      "epoch: 1, batch: 693, loss: 0.039111584424972534\n",
      "epoch: 1, batch: 694, loss: 0.0861581340432167\n",
      "epoch: 1, batch: 695, loss: 0.06519275903701782\n",
      "epoch: 1, batch: 696, loss: 0.11680232733488083\n",
      "epoch: 1, batch: 697, loss: 0.0948920026421547\n",
      "epoch: 1, batch: 698, loss: 0.061536625027656555\n",
      "epoch: 1, batch: 699, loss: 0.05398907512426376\n",
      "epoch: 1, batch: 700, loss: 0.22959120571613312\n",
      "epoch: 1, batch: 701, loss: 0.10756850987672806\n",
      "epoch: 1, batch: 702, loss: 0.23396064341068268\n",
      "epoch: 1, batch: 703, loss: 0.13173025846481323\n",
      "epoch: 1, batch: 704, loss: 0.06026162952184677\n",
      "epoch: 1, batch: 705, loss: 0.03506343811750412\n",
      "epoch: 1, batch: 706, loss: 0.052702587097883224\n",
      "epoch: 1, batch: 707, loss: 0.09387379884719849\n",
      "epoch: 1, batch: 708, loss: 0.045057978481054306\n",
      "epoch: 1, batch: 709, loss: 0.16607967019081116\n",
      "epoch: 1, batch: 710, loss: 0.07972319424152374\n",
      "epoch: 1, batch: 711, loss: 0.02591589093208313\n",
      "epoch: 1, batch: 712, loss: 0.07382775098085403\n",
      "epoch: 1, batch: 713, loss: 0.08767402172088623\n",
      "epoch: 1, batch: 714, loss: 0.07744203507900238\n",
      "epoch: 1, batch: 715, loss: 0.04827503114938736\n",
      "epoch: 1, batch: 716, loss: 0.15450343489646912\n",
      "epoch: 1, batch: 717, loss: 0.14704297482967377\n",
      "epoch: 1, batch: 718, loss: 0.09864679723978043\n",
      "epoch: 1, batch: 719, loss: 0.11954399198293686\n",
      "epoch: 1, batch: 720, loss: 0.14822812378406525\n",
      "epoch: 1, batch: 721, loss: 0.060140714049339294\n",
      "epoch: 1, batch: 722, loss: 0.0644732266664505\n",
      "epoch: 1, batch: 723, loss: 0.10955885797739029\n",
      "epoch: 1, batch: 724, loss: 0.10520444810390472\n",
      "epoch: 1, batch: 725, loss: 0.09992914646863937\n",
      "epoch: 1, batch: 726, loss: 0.08753262460231781\n",
      "epoch: 1, batch: 727, loss: 0.06744086742401123\n",
      "epoch: 1, batch: 728, loss: 0.029668239876627922\n",
      "epoch: 1, batch: 729, loss: 0.024733100086450577\n",
      "epoch: 1, batch: 730, loss: 0.06957951933145523\n",
      "epoch: 1, batch: 731, loss: 0.033469267189502716\n",
      "epoch: 1, batch: 732, loss: 0.12708738446235657\n",
      "epoch: 1, batch: 733, loss: 0.03078487515449524\n",
      "epoch: 1, batch: 734, loss: 0.05460575968027115\n",
      "epoch: 1, batch: 735, loss: 0.10560888797044754\n",
      "epoch: 1, batch: 736, loss: 0.15365692973136902\n",
      "epoch: 1, batch: 737, loss: 0.24456292390823364\n",
      "epoch: 1, batch: 738, loss: 0.01664828322827816\n",
      "epoch: 1, batch: 739, loss: 0.1338740438222885\n",
      "epoch: 1, batch: 740, loss: 0.17690014839172363\n",
      "epoch: 1, batch: 741, loss: 0.0639495700597763\n",
      "epoch: 1, batch: 742, loss: 0.2440776377916336\n",
      "epoch: 1, batch: 743, loss: 0.09593991935253143\n",
      "epoch: 1, batch: 744, loss: 0.043078336864709854\n",
      "epoch: 1, batch: 745, loss: 0.10167022049427032\n",
      "epoch: 1, batch: 746, loss: 0.15651872754096985\n",
      "epoch: 1, batch: 747, loss: 0.12741458415985107\n",
      "epoch: 1, batch: 748, loss: 0.053043074905872345\n",
      "epoch: 1, batch: 749, loss: 0.04138065502047539\n",
      "epoch: 1, batch: 750, loss: 0.11033506691455841\n",
      "epoch: 1, batch: 751, loss: 0.030606992542743683\n",
      "epoch: 1, batch: 752, loss: 0.07496265321969986\n",
      "epoch: 1, batch: 753, loss: 0.04997595027089119\n",
      "epoch: 1, batch: 754, loss: 0.13026078045368195\n",
      "epoch: 1, batch: 755, loss: 0.024412108585238457\n",
      "epoch: 1, batch: 756, loss: 0.023561453446745872\n",
      "epoch: 1, batch: 757, loss: 0.017183886840939522\n",
      "epoch: 1, batch: 758, loss: 0.06480803340673447\n",
      "epoch: 1, batch: 759, loss: 0.07152596116065979\n",
      "epoch: 1, batch: 760, loss: 0.06626997143030167\n",
      "epoch: 1, batch: 761, loss: 0.12171002477407455\n",
      "epoch: 1, batch: 762, loss: 0.18165601789951324\n",
      "epoch: 1, batch: 763, loss: 0.03512866422533989\n",
      "epoch: 1, batch: 764, loss: 0.08685784041881561\n",
      "epoch: 1, batch: 765, loss: 0.033467914909124374\n",
      "epoch: 1, batch: 766, loss: 0.06378879398107529\n",
      "epoch: 1, batch: 767, loss: 0.0451912060379982\n",
      "epoch: 1, batch: 768, loss: 0.06685259938240051\n",
      "epoch: 1, batch: 769, loss: 0.10530335456132889\n",
      "epoch: 1, batch: 770, loss: 0.15009936690330505\n",
      "epoch: 1, batch: 771, loss: 0.04391198605298996\n",
      "epoch: 1, batch: 772, loss: 0.023486118763685226\n",
      "epoch: 1, batch: 773, loss: 0.06359047442674637\n",
      "epoch: 1, batch: 774, loss: 0.149720698595047\n",
      "epoch: 1, batch: 775, loss: 0.08615058660507202\n",
      "epoch: 1, batch: 776, loss: 0.2222152203321457\n",
      "epoch: 1, batch: 777, loss: 0.16385842859745026\n",
      "epoch: 1, batch: 778, loss: 0.0361168347299099\n",
      "epoch: 1, batch: 779, loss: 0.040736813098192215\n",
      "epoch: 1, batch: 780, loss: 0.06792465597391129\n",
      "epoch: 1, batch: 781, loss: 0.16426430642604828\n",
      "epoch: 1, batch: 782, loss: 0.03217221051454544\n",
      "epoch: 1, batch: 783, loss: 0.0794234573841095\n",
      "epoch: 1, batch: 784, loss: 0.04971697926521301\n",
      "epoch: 1, batch: 785, loss: 0.22355027496814728\n",
      "epoch: 1, batch: 786, loss: 0.07610993832349777\n",
      "epoch: 1, batch: 787, loss: 0.04656399041414261\n",
      "epoch: 1, batch: 788, loss: 0.021570835262537003\n",
      "epoch: 1, batch: 789, loss: 0.060530323535203934\n",
      "epoch: 1, batch: 790, loss: 0.0898660197854042\n",
      "epoch: 1, batch: 791, loss: 0.03329227864742279\n",
      "epoch: 1, batch: 792, loss: 0.11678117513656616\n",
      "epoch: 1, batch: 793, loss: 0.13851091265678406\n",
      "epoch: 1, batch: 794, loss: 0.031099697574973106\n",
      "epoch: 1, batch: 795, loss: 0.06813615560531616\n",
      "epoch: 1, batch: 796, loss: 0.056487075984478\n",
      "epoch: 1, batch: 797, loss: 0.0627504214644432\n",
      "epoch: 1, batch: 798, loss: 0.21251694858074188\n",
      "epoch: 1, batch: 799, loss: 0.03771413117647171\n",
      "epoch: 1, batch: 800, loss: 0.066931813955307\n",
      "epoch: 1, batch: 801, loss: 0.0733981728553772\n",
      "epoch: 1, batch: 802, loss: 0.11137464642524719\n",
      "epoch: 1, batch: 803, loss: 0.04223519191145897\n",
      "epoch: 1, batch: 804, loss: 0.04502672702074051\n",
      "epoch: 1, batch: 805, loss: 0.08432747423648834\n",
      "epoch: 1, batch: 806, loss: 0.03323381021618843\n",
      "epoch: 1, batch: 807, loss: 0.019907720386981964\n",
      "epoch: 1, batch: 808, loss: 0.0412462018430233\n",
      "epoch: 1, batch: 809, loss: 0.04315171018242836\n",
      "epoch: 1, batch: 810, loss: 0.05353903770446777\n",
      "epoch: 1, batch: 811, loss: 0.04036843404173851\n",
      "epoch: 1, batch: 812, loss: 0.07611270248889923\n",
      "epoch: 1, batch: 813, loss: 0.01920836977660656\n",
      "epoch: 1, batch: 814, loss: 0.06834384799003601\n",
      "epoch: 1, batch: 815, loss: 0.11902730911970139\n",
      "epoch: 1, batch: 816, loss: 0.10775523632764816\n",
      "epoch: 1, batch: 817, loss: 0.05662096291780472\n",
      "epoch: 1, batch: 818, loss: 0.06628204882144928\n",
      "epoch: 1, batch: 819, loss: 0.21401382982730865\n",
      "epoch: 1, batch: 820, loss: 0.19548259675502777\n",
      "epoch: 1, batch: 821, loss: 0.05678640678524971\n",
      "epoch: 1, batch: 822, loss: 0.013358945026993752\n",
      "epoch: 1, batch: 823, loss: 0.05762205645442009\n",
      "epoch: 1, batch: 824, loss: 0.05961885303258896\n",
      "epoch: 1, batch: 825, loss: 0.0509105920791626\n",
      "epoch: 1, batch: 826, loss: 0.15926486253738403\n",
      "epoch: 1, batch: 827, loss: 0.04635677859187126\n",
      "epoch: 1, batch: 828, loss: 0.045338355004787445\n",
      "epoch: 1, batch: 829, loss: 0.0400368832051754\n",
      "epoch: 1, batch: 830, loss: 0.06674344092607498\n",
      "epoch: 1, batch: 831, loss: 0.07109203934669495\n",
      "epoch: 1, batch: 832, loss: 0.04502687603235245\n",
      "epoch: 1, batch: 833, loss: 0.11324203759431839\n",
      "epoch: 1, batch: 834, loss: 0.09190602600574493\n",
      "epoch: 1, batch: 835, loss: 0.13896258175373077\n",
      "epoch: 1, batch: 836, loss: 0.018940355628728867\n",
      "epoch: 1, batch: 837, loss: 0.04007352143526077\n",
      "epoch: 1, batch: 838, loss: 0.03128490224480629\n",
      "epoch: 1, batch: 839, loss: 0.1286710947751999\n",
      "epoch: 1, batch: 840, loss: 0.16817376017570496\n",
      "epoch: 1, batch: 841, loss: 0.1548701524734497\n",
      "epoch: 1, batch: 842, loss: 0.08744125813245773\n",
      "epoch: 1, batch: 843, loss: 0.0672491267323494\n",
      "epoch: 1, batch: 844, loss: 0.047384679317474365\n",
      "epoch: 1, batch: 845, loss: 0.0991191565990448\n",
      "epoch: 1, batch: 846, loss: 0.1303778737783432\n",
      "epoch: 1, batch: 847, loss: 0.2703159749507904\n",
      "epoch: 1, batch: 848, loss: 0.021472163498401642\n",
      "epoch: 1, batch: 849, loss: 0.13249318301677704\n",
      "epoch: 1, batch: 850, loss: 0.10243053734302521\n",
      "epoch: 1, batch: 851, loss: 0.03893960267305374\n",
      "epoch: 1, batch: 852, loss: 0.10047624260187149\n",
      "epoch: 1, batch: 853, loss: 0.08368503302335739\n",
      "epoch: 1, batch: 854, loss: 0.051336247473955154\n",
      "epoch: 1, batch: 855, loss: 0.08101877570152283\n",
      "epoch: 1, batch: 856, loss: 0.061492670327425\n",
      "epoch: 1, batch: 857, loss: 0.08999711275100708\n",
      "epoch: 1, batch: 858, loss: 0.01000012457370758\n",
      "epoch: 1, batch: 859, loss: 0.040837451815605164\n",
      "epoch: 1, batch: 860, loss: 0.009777865372598171\n",
      "epoch: 1, batch: 861, loss: 0.07655207812786102\n",
      "epoch: 1, batch: 862, loss: 0.14430361986160278\n",
      "epoch: 1, batch: 863, loss: 0.05400271341204643\n",
      "epoch: 1, batch: 864, loss: 0.031693235039711\n",
      "epoch: 1, batch: 865, loss: 0.12479809671640396\n",
      "epoch: 1, batch: 866, loss: 0.10610932111740112\n",
      "epoch: 1, batch: 867, loss: 0.07446974515914917\n",
      "epoch: 1, batch: 868, loss: 0.2626034915447235\n",
      "epoch: 1, batch: 869, loss: 0.021953770890831947\n",
      "epoch: 1, batch: 870, loss: 0.0539221353828907\n",
      "epoch: 1, batch: 871, loss: 0.03470040485262871\n",
      "epoch: 1, batch: 872, loss: 0.19248950481414795\n",
      "epoch: 1, batch: 873, loss: 0.011981112882494926\n",
      "epoch: 1, batch: 874, loss: 0.013407361693680286\n",
      "epoch: 1, batch: 875, loss: 0.04071516543626785\n",
      "epoch: 1, batch: 876, loss: 0.022076597437262535\n",
      "epoch: 1, batch: 877, loss: 0.1932099163532257\n",
      "epoch: 1, batch: 878, loss: 0.04787473753094673\n",
      "epoch: 1, batch: 879, loss: 0.27281099557876587\n",
      "epoch: 1, batch: 880, loss: 0.18160322308540344\n",
      "epoch: 1, batch: 881, loss: 0.14396360516548157\n",
      "epoch: 1, batch: 882, loss: 0.13432760536670685\n",
      "epoch: 1, batch: 883, loss: 0.04911255091428757\n",
      "epoch: 1, batch: 884, loss: 0.18523813784122467\n",
      "epoch: 1, batch: 885, loss: 0.08463217318058014\n",
      "epoch: 1, batch: 886, loss: 0.016859332099556923\n",
      "epoch: 1, batch: 887, loss: 0.07309133559465408\n",
      "epoch: 1, batch: 888, loss: 0.022903859615325928\n",
      "epoch: 1, batch: 889, loss: 0.1066315621137619\n",
      "epoch: 1, batch: 890, loss: 0.15754258632659912\n",
      "epoch: 1, batch: 891, loss: 0.030318953096866608\n",
      "epoch: 1, batch: 892, loss: 0.16812793910503387\n",
      "epoch: 1, batch: 893, loss: 0.05979892611503601\n",
      "epoch: 1, batch: 894, loss: 0.15660302340984344\n",
      "epoch: 1, batch: 895, loss: 0.03531224653124809\n",
      "epoch: 1, batch: 896, loss: 0.07800479233264923\n",
      "epoch: 1, batch: 897, loss: 0.022145500406622887\n",
      "epoch: 1, batch: 898, loss: 0.07151716947555542\n",
      "epoch: 1, batch: 899, loss: 0.17301760613918304\n",
      "epoch: 1, batch: 900, loss: 0.0965406596660614\n",
      "epoch: 1, batch: 901, loss: 0.04208271950483322\n",
      "epoch: 1, batch: 902, loss: 0.06138720363378525\n",
      "epoch: 1, batch: 903, loss: 0.08765222877264023\n",
      "epoch: 1, batch: 904, loss: 0.01886850968003273\n",
      "epoch: 1, batch: 905, loss: 0.18963341414928436\n",
      "epoch: 1, batch: 906, loss: 0.03282151743769646\n",
      "epoch: 1, batch: 907, loss: 0.04594823718070984\n",
      "epoch: 1, batch: 908, loss: 0.13752268254756927\n",
      "epoch: 1, batch: 909, loss: 0.06549901515245438\n",
      "epoch: 1, batch: 910, loss: 0.03723191097378731\n",
      "epoch: 1, batch: 911, loss: 0.037344615906476974\n",
      "epoch: 1, batch: 912, loss: 0.07556240260601044\n",
      "epoch: 1, batch: 913, loss: 0.07236406207084656\n",
      "epoch: 1, batch: 914, loss: 0.037953801453113556\n",
      "epoch: 1, batch: 915, loss: 0.08858568221330643\n",
      "epoch: 1, batch: 916, loss: 0.03543238341808319\n",
      "epoch: 1, batch: 917, loss: 0.12022938579320908\n",
      "epoch: 1, batch: 918, loss: 0.03284568339586258\n",
      "epoch: 1, batch: 919, loss: 0.15429045259952545\n",
      "epoch: 1, batch: 920, loss: 0.13185684382915497\n",
      "epoch: 1, batch: 921, loss: 0.06705362349748611\n",
      "epoch: 1, batch: 922, loss: 0.07608823478221893\n",
      "epoch: 1, batch: 923, loss: 0.05006089806556702\n",
      "epoch: 1, batch: 924, loss: 0.09458395838737488\n",
      "epoch: 1, batch: 925, loss: 0.03869100287556648\n",
      "epoch: 1, batch: 926, loss: 0.06514298170804977\n",
      "epoch: 1, batch: 927, loss: 0.0391014888882637\n",
      "epoch: 1, batch: 928, loss: 0.05203140154480934\n",
      "epoch: 1, batch: 929, loss: 0.026103107258677483\n",
      "epoch: 1, batch: 930, loss: 0.07666009664535522\n",
      "epoch: 1, batch: 931, loss: 0.04914339259266853\n",
      "epoch: 1, batch: 932, loss: 0.037111081182956696\n",
      "epoch: 1, batch: 933, loss: 0.006666366942226887\n",
      "epoch: 1, batch: 934, loss: 0.18050265312194824\n",
      "epoch: 1, batch: 935, loss: 0.028396328911185265\n",
      "epoch: 1, batch: 936, loss: 0.032106898725032806\n",
      "epoch: 1, batch: 937, loss: 0.03420768305659294\n",
      "epoch: 2, batch: 0, loss: 0.050843365490436554\n",
      "epoch: 2, batch: 1, loss: 0.020689893513917923\n",
      "epoch: 2, batch: 2, loss: 0.019737545400857925\n",
      "epoch: 2, batch: 3, loss: 0.030771620571613312\n",
      "epoch: 2, batch: 4, loss: 0.10153565555810928\n",
      "epoch: 2, batch: 5, loss: 0.08671639859676361\n",
      "epoch: 2, batch: 6, loss: 0.030268318951129913\n",
      "epoch: 2, batch: 7, loss: 0.08982057124376297\n",
      "epoch: 2, batch: 8, loss: 0.015228811651468277\n",
      "epoch: 2, batch: 9, loss: 0.03175223618745804\n",
      "epoch: 2, batch: 10, loss: 0.15627384185791016\n",
      "epoch: 2, batch: 11, loss: 0.061633333563804626\n",
      "epoch: 2, batch: 12, loss: 0.05768897384405136\n",
      "epoch: 2, batch: 13, loss: 0.03275791183114052\n",
      "epoch: 2, batch: 14, loss: 0.05069221556186676\n",
      "epoch: 2, batch: 15, loss: 0.05372334271669388\n",
      "epoch: 2, batch: 16, loss: 0.025113189592957497\n",
      "epoch: 2, batch: 17, loss: 0.17616187036037445\n",
      "epoch: 2, batch: 18, loss: 0.0914926752448082\n",
      "epoch: 2, batch: 19, loss: 0.0437748022377491\n",
      "epoch: 2, batch: 20, loss: 0.058678872883319855\n",
      "epoch: 2, batch: 21, loss: 0.16752128303050995\n",
      "epoch: 2, batch: 22, loss: 0.02088637836277485\n",
      "epoch: 2, batch: 23, loss: 0.029276585206389427\n",
      "epoch: 2, batch: 24, loss: 0.03945537284016609\n",
      "epoch: 2, batch: 25, loss: 0.07662559300661087\n",
      "epoch: 2, batch: 26, loss: 0.04740272834897041\n",
      "epoch: 2, batch: 27, loss: 0.08768898248672485\n",
      "epoch: 2, batch: 28, loss: 0.14684747159481049\n",
      "epoch: 2, batch: 29, loss: 0.04076673090457916\n",
      "epoch: 2, batch: 30, loss: 0.11894809454679489\n",
      "epoch: 2, batch: 31, loss: 0.0717669278383255\n",
      "epoch: 2, batch: 32, loss: 0.02554493211209774\n",
      "epoch: 2, batch: 33, loss: 0.024553781375288963\n",
      "epoch: 2, batch: 34, loss: 0.027074458077549934\n",
      "epoch: 2, batch: 35, loss: 0.0585586279630661\n",
      "epoch: 2, batch: 36, loss: 0.03818519413471222\n",
      "epoch: 2, batch: 37, loss: 0.1578587293624878\n",
      "epoch: 2, batch: 38, loss: 0.13224755227565765\n",
      "epoch: 2, batch: 39, loss: 0.03355211392045021\n",
      "epoch: 2, batch: 40, loss: 0.09533626586198807\n",
      "epoch: 2, batch: 41, loss: 0.04669807478785515\n",
      "epoch: 2, batch: 42, loss: 0.10084878653287888\n",
      "epoch: 2, batch: 43, loss: 0.022221243008971214\n",
      "epoch: 2, batch: 44, loss: 0.039314012974500656\n",
      "epoch: 2, batch: 45, loss: 0.021727273240685463\n",
      "epoch: 2, batch: 46, loss: 0.07615245878696442\n",
      "epoch: 2, batch: 47, loss: 0.048410799354314804\n",
      "epoch: 2, batch: 48, loss: 0.14822465181350708\n",
      "epoch: 2, batch: 49, loss: 0.10948417335748672\n",
      "epoch: 2, batch: 50, loss: 0.017264921218156815\n",
      "epoch: 2, batch: 51, loss: 0.06826765090227127\n",
      "epoch: 2, batch: 52, loss: 0.10425148159265518\n",
      "epoch: 2, batch: 53, loss: 0.10997530072927475\n",
      "epoch: 2, batch: 54, loss: 0.06777206063270569\n",
      "epoch: 2, batch: 55, loss: 0.01710440032184124\n",
      "epoch: 2, batch: 56, loss: 0.023488523438572884\n",
      "epoch: 2, batch: 57, loss: 0.03246328607201576\n",
      "epoch: 2, batch: 58, loss: 0.07486187666654587\n",
      "epoch: 2, batch: 59, loss: 0.09758507460355759\n",
      "epoch: 2, batch: 60, loss: 0.0442219041287899\n",
      "epoch: 2, batch: 61, loss: 0.06253720819950104\n",
      "epoch: 2, batch: 62, loss: 0.040838856250047684\n",
      "epoch: 2, batch: 63, loss: 0.025507979094982147\n",
      "epoch: 2, batch: 64, loss: 0.031602147966623306\n",
      "epoch: 2, batch: 65, loss: 0.02931837923824787\n",
      "epoch: 2, batch: 66, loss: 0.10124064236879349\n",
      "epoch: 2, batch: 67, loss: 0.025619950145483017\n",
      "epoch: 2, batch: 68, loss: 0.036380644887685776\n",
      "epoch: 2, batch: 69, loss: 0.0302113126963377\n",
      "epoch: 2, batch: 70, loss: 0.027460716664791107\n",
      "epoch: 2, batch: 71, loss: 0.15177282691001892\n",
      "epoch: 2, batch: 72, loss: 0.028203021734952927\n",
      "epoch: 2, batch: 73, loss: 0.04975595697760582\n",
      "epoch: 2, batch: 74, loss: 0.01338384859263897\n",
      "epoch: 2, batch: 75, loss: 0.14050525426864624\n",
      "epoch: 2, batch: 76, loss: 0.05369646102190018\n",
      "epoch: 2, batch: 77, loss: 0.09026950597763062\n",
      "epoch: 2, batch: 78, loss: 0.02877657301723957\n",
      "epoch: 2, batch: 79, loss: 0.07206317037343979\n",
      "epoch: 2, batch: 80, loss: 0.10535459220409393\n",
      "epoch: 2, batch: 81, loss: 0.040084511041641235\n",
      "epoch: 2, batch: 82, loss: 0.05761830508708954\n",
      "epoch: 2, batch: 83, loss: 0.10077730566263199\n",
      "epoch: 2, batch: 84, loss: 0.028152326121926308\n",
      "epoch: 2, batch: 85, loss: 0.0828842893242836\n",
      "epoch: 2, batch: 86, loss: 0.016268612816929817\n",
      "epoch: 2, batch: 87, loss: 0.012885377742350101\n",
      "epoch: 2, batch: 88, loss: 0.009743955917656422\n",
      "epoch: 2, batch: 89, loss: 0.20521460473537445\n",
      "epoch: 2, batch: 90, loss: 0.09410539269447327\n",
      "epoch: 2, batch: 91, loss: 0.11719179898500443\n",
      "epoch: 2, batch: 92, loss: 0.04865037277340889\n",
      "epoch: 2, batch: 93, loss: 0.0793425664305687\n",
      "epoch: 2, batch: 94, loss: 0.06135599687695503\n",
      "epoch: 2, batch: 95, loss: 0.0506298802793026\n",
      "epoch: 2, batch: 96, loss: 0.0318797267973423\n",
      "epoch: 2, batch: 97, loss: 0.040785182267427444\n",
      "epoch: 2, batch: 98, loss: 0.03269508481025696\n",
      "epoch: 2, batch: 99, loss: 0.03486837446689606\n",
      "epoch: 2, batch: 100, loss: 0.01614728569984436\n",
      "epoch: 2, batch: 101, loss: 0.03848842903971672\n",
      "epoch: 2, batch: 102, loss: 0.06051662936806679\n",
      "epoch: 2, batch: 103, loss: 0.11647313833236694\n",
      "epoch: 2, batch: 104, loss: 0.04991794005036354\n",
      "epoch: 2, batch: 105, loss: 0.06838560849428177\n",
      "epoch: 2, batch: 106, loss: 0.04209894314408302\n",
      "epoch: 2, batch: 107, loss: 0.1966705173254013\n",
      "epoch: 2, batch: 108, loss: 0.17602303624153137\n",
      "epoch: 2, batch: 109, loss: 0.053633227944374084\n",
      "epoch: 2, batch: 110, loss: 0.011799706146121025\n",
      "epoch: 2, batch: 111, loss: 0.1565103530883789\n",
      "epoch: 2, batch: 112, loss: 0.029611313715577126\n",
      "epoch: 2, batch: 113, loss: 0.08101028949022293\n",
      "epoch: 2, batch: 114, loss: 0.14463621377944946\n",
      "epoch: 2, batch: 115, loss: 0.06540025770664215\n",
      "epoch: 2, batch: 116, loss: 0.07240328937768936\n",
      "epoch: 2, batch: 117, loss: 0.11077466607093811\n",
      "epoch: 2, batch: 118, loss: 0.01746745966374874\n",
      "epoch: 2, batch: 119, loss: 0.07154715806245804\n",
      "epoch: 2, batch: 120, loss: 0.1078677847981453\n",
      "epoch: 2, batch: 121, loss: 0.048331890255212784\n",
      "epoch: 2, batch: 122, loss: 0.0248543843626976\n",
      "epoch: 2, batch: 123, loss: 0.0863410234451294\n",
      "epoch: 2, batch: 124, loss: 0.019465986639261246\n",
      "epoch: 2, batch: 125, loss: 0.038229696452617645\n",
      "epoch: 2, batch: 126, loss: 0.046143729239702225\n",
      "epoch: 2, batch: 127, loss: 0.02888339012861252\n",
      "epoch: 2, batch: 128, loss: 0.0237883310765028\n",
      "epoch: 2, batch: 129, loss: 0.07903307676315308\n",
      "epoch: 2, batch: 130, loss: 0.10113225877285004\n",
      "epoch: 2, batch: 131, loss: 0.014426440931856632\n",
      "epoch: 2, batch: 132, loss: 0.06703347712755203\n",
      "epoch: 2, batch: 133, loss: 0.05564579367637634\n",
      "epoch: 2, batch: 134, loss: 0.1755758374929428\n",
      "epoch: 2, batch: 135, loss: 0.21978677809238434\n",
      "epoch: 2, batch: 136, loss: 0.07568623125553131\n",
      "epoch: 2, batch: 137, loss: 0.036803651601076126\n",
      "epoch: 2, batch: 138, loss: 0.06419327110052109\n",
      "epoch: 2, batch: 139, loss: 0.05472879484295845\n",
      "epoch: 2, batch: 140, loss: 0.108521968126297\n",
      "epoch: 2, batch: 141, loss: 0.034314047545194626\n",
      "epoch: 2, batch: 142, loss: 0.019703229889273643\n",
      "epoch: 2, batch: 143, loss: 0.02694903500378132\n",
      "epoch: 2, batch: 144, loss: 0.06923150271177292\n",
      "epoch: 2, batch: 145, loss: 0.15563750267028809\n",
      "epoch: 2, batch: 146, loss: 0.015458316542208195\n",
      "epoch: 2, batch: 147, loss: 0.032214317470788956\n",
      "epoch: 2, batch: 148, loss: 0.044302187860012054\n",
      "epoch: 2, batch: 149, loss: 0.03317796066403389\n",
      "epoch: 2, batch: 150, loss: 0.05537667125463486\n",
      "epoch: 2, batch: 151, loss: 0.018004504963755608\n",
      "epoch: 2, batch: 152, loss: 0.07434046268463135\n",
      "epoch: 2, batch: 153, loss: 0.1344449371099472\n",
      "epoch: 2, batch: 154, loss: 0.013210139237344265\n",
      "epoch: 2, batch: 155, loss: 0.026986567303538322\n",
      "epoch: 2, batch: 156, loss: 0.048836179077625275\n",
      "epoch: 2, batch: 157, loss: 0.032465312629938126\n",
      "epoch: 2, batch: 158, loss: 0.014342899434268475\n",
      "epoch: 2, batch: 159, loss: 0.1883278787136078\n",
      "epoch: 2, batch: 160, loss: 0.026012780144810677\n",
      "epoch: 2, batch: 161, loss: 0.031077608466148376\n",
      "epoch: 2, batch: 162, loss: 0.14367705583572388\n",
      "epoch: 2, batch: 163, loss: 0.0471525639295578\n",
      "epoch: 2, batch: 164, loss: 0.10927562415599823\n",
      "epoch: 2, batch: 165, loss: 0.022539488971233368\n",
      "epoch: 2, batch: 166, loss: 0.04296011105179787\n",
      "epoch: 2, batch: 167, loss: 0.12982679903507233\n",
      "epoch: 2, batch: 168, loss: 0.021925851702690125\n",
      "epoch: 2, batch: 169, loss: 0.06969718635082245\n",
      "epoch: 2, batch: 170, loss: 0.04508262127637863\n",
      "epoch: 2, batch: 171, loss: 0.07646238058805466\n",
      "epoch: 2, batch: 172, loss: 0.07308778166770935\n",
      "epoch: 2, batch: 173, loss: 0.13148735463619232\n",
      "epoch: 2, batch: 174, loss: 0.044416479766368866\n",
      "epoch: 2, batch: 175, loss: 0.030175214633345604\n",
      "epoch: 2, batch: 176, loss: 0.020130952820181847\n",
      "epoch: 2, batch: 177, loss: 0.04187038540840149\n",
      "epoch: 2, batch: 178, loss: 0.06398139894008636\n",
      "epoch: 2, batch: 179, loss: 0.027262911200523376\n",
      "epoch: 2, batch: 180, loss: 0.061445437371730804\n",
      "epoch: 2, batch: 181, loss: 0.018978171050548553\n",
      "epoch: 2, batch: 182, loss: 0.08357961475849152\n",
      "epoch: 2, batch: 183, loss: 0.03308245167136192\n",
      "epoch: 2, batch: 184, loss: 0.024130025878548622\n",
      "epoch: 2, batch: 185, loss: 0.055002037435770035\n",
      "epoch: 2, batch: 186, loss: 0.03670204058289528\n",
      "epoch: 2, batch: 187, loss: 0.048580147325992584\n",
      "epoch: 2, batch: 188, loss: 0.15774765610694885\n",
      "epoch: 2, batch: 189, loss: 0.04249824211001396\n",
      "epoch: 2, batch: 190, loss: 0.06505459547042847\n",
      "epoch: 2, batch: 191, loss: 0.012904864735901356\n",
      "epoch: 2, batch: 192, loss: 0.03702261671423912\n",
      "epoch: 2, batch: 193, loss: 0.015064682811498642\n",
      "epoch: 2, batch: 194, loss: 0.07536420971155167\n",
      "epoch: 2, batch: 195, loss: 0.04027526080608368\n",
      "epoch: 2, batch: 196, loss: 0.034791283309459686\n",
      "epoch: 2, batch: 197, loss: 0.03420587256550789\n",
      "epoch: 2, batch: 198, loss: 0.056078068912029266\n",
      "epoch: 2, batch: 199, loss: 0.05276080593466759\n",
      "epoch: 2, batch: 200, loss: 0.1486467570066452\n",
      "epoch: 2, batch: 201, loss: 0.1456197202205658\n",
      "epoch: 2, batch: 202, loss: 0.10066214203834534\n",
      "epoch: 2, batch: 203, loss: 0.09769173711538315\n",
      "epoch: 2, batch: 204, loss: 0.03482721745967865\n",
      "epoch: 2, batch: 205, loss: 0.16862709820270538\n",
      "epoch: 2, batch: 206, loss: 0.05192950367927551\n",
      "epoch: 2, batch: 207, loss: 0.021068738773465157\n",
      "epoch: 2, batch: 208, loss: 0.14507147669792175\n",
      "epoch: 2, batch: 209, loss: 0.08980581909418106\n",
      "epoch: 2, batch: 210, loss: 0.005937510170042515\n",
      "epoch: 2, batch: 211, loss: 0.21794353425502777\n",
      "epoch: 2, batch: 212, loss: 0.08275448530912399\n",
      "epoch: 2, batch: 213, loss: 0.028362853452563286\n",
      "epoch: 2, batch: 214, loss: 0.07079070061445236\n",
      "epoch: 2, batch: 215, loss: 0.09840401262044907\n",
      "epoch: 2, batch: 216, loss: 0.06167782470583916\n",
      "epoch: 2, batch: 217, loss: 0.049825891852378845\n",
      "epoch: 2, batch: 218, loss: 0.12565399706363678\n",
      "epoch: 2, batch: 219, loss: 0.0909595936536789\n",
      "epoch: 2, batch: 220, loss: 0.04493538290262222\n",
      "epoch: 2, batch: 221, loss: 0.10197846591472626\n",
      "epoch: 2, batch: 222, loss: 0.14027513563632965\n",
      "epoch: 2, batch: 223, loss: 0.059650592505931854\n",
      "epoch: 2, batch: 224, loss: 0.026860635727643967\n",
      "epoch: 2, batch: 225, loss: 0.013177848421037197\n",
      "epoch: 2, batch: 226, loss: 0.04915006086230278\n",
      "epoch: 2, batch: 227, loss: 0.08696451783180237\n",
      "epoch: 2, batch: 228, loss: 0.029692791402339935\n",
      "epoch: 2, batch: 229, loss: 0.03162454068660736\n",
      "epoch: 2, batch: 230, loss: 0.03752027079463005\n",
      "epoch: 2, batch: 231, loss: 0.01777786575257778\n",
      "epoch: 2, batch: 232, loss: 0.18766459822654724\n",
      "epoch: 2, batch: 233, loss: 0.14398342370986938\n",
      "epoch: 2, batch: 234, loss: 0.05019861459732056\n",
      "epoch: 2, batch: 235, loss: 0.1831742823123932\n",
      "epoch: 2, batch: 236, loss: 0.05637890473008156\n",
      "epoch: 2, batch: 237, loss: 0.014215919189155102\n",
      "epoch: 2, batch: 238, loss: 0.09888703376054764\n",
      "epoch: 2, batch: 239, loss: 0.01354752667248249\n",
      "epoch: 2, batch: 240, loss: 0.05237545073032379\n",
      "epoch: 2, batch: 241, loss: 0.052693527191877365\n",
      "epoch: 2, batch: 242, loss: 0.027006272226572037\n",
      "epoch: 2, batch: 243, loss: 0.04286706820130348\n",
      "epoch: 2, batch: 244, loss: 0.09496238827705383\n",
      "epoch: 2, batch: 245, loss: 0.05633651465177536\n",
      "epoch: 2, batch: 246, loss: 0.1649688184261322\n",
      "epoch: 2, batch: 247, loss: 0.012610874138772488\n",
      "epoch: 2, batch: 248, loss: 0.038766246289014816\n",
      "epoch: 2, batch: 249, loss: 0.07082419842481613\n",
      "epoch: 2, batch: 250, loss: 0.12111733853816986\n",
      "epoch: 2, batch: 251, loss: 0.030914142727851868\n",
      "epoch: 2, batch: 252, loss: 0.18496927618980408\n",
      "epoch: 2, batch: 253, loss: 0.11371925473213196\n",
      "epoch: 2, batch: 254, loss: 0.13069351017475128\n",
      "epoch: 2, batch: 255, loss: 0.08857620507478714\n",
      "epoch: 2, batch: 256, loss: 0.04367460682988167\n",
      "epoch: 2, batch: 257, loss: 0.25450047850608826\n",
      "epoch: 2, batch: 258, loss: 0.013353114947676659\n",
      "epoch: 2, batch: 259, loss: 0.0729190781712532\n",
      "epoch: 2, batch: 260, loss: 0.09980286657810211\n",
      "epoch: 2, batch: 261, loss: 0.09606654942035675\n",
      "epoch: 2, batch: 262, loss: 0.09322544932365417\n",
      "epoch: 2, batch: 263, loss: 0.09183903783559799\n",
      "epoch: 2, batch: 264, loss: 0.04448451101779938\n",
      "epoch: 2, batch: 265, loss: 0.07213683426380157\n",
      "epoch: 2, batch: 266, loss: 0.010247396305203438\n",
      "epoch: 2, batch: 267, loss: 0.00781784113496542\n",
      "epoch: 2, batch: 268, loss: 0.07705117762088776\n",
      "epoch: 2, batch: 269, loss: 0.07115215808153152\n",
      "epoch: 2, batch: 270, loss: 0.028208963572978973\n",
      "epoch: 2, batch: 271, loss: 0.07340975105762482\n",
      "epoch: 2, batch: 272, loss: 0.09593231230974197\n",
      "epoch: 2, batch: 273, loss: 0.0745779424905777\n",
      "epoch: 2, batch: 274, loss: 0.022546928375959396\n",
      "epoch: 2, batch: 275, loss: 0.0705052986741066\n",
      "epoch: 2, batch: 276, loss: 0.04380034655332565\n",
      "epoch: 2, batch: 277, loss: 0.055631741881370544\n",
      "epoch: 2, batch: 278, loss: 0.15527747571468353\n",
      "epoch: 2, batch: 279, loss: 0.08050338923931122\n",
      "epoch: 2, batch: 280, loss: 0.04297861084342003\n",
      "epoch: 2, batch: 281, loss: 0.06483523547649384\n",
      "epoch: 2, batch: 282, loss: 0.048507511615753174\n",
      "epoch: 2, batch: 283, loss: 0.19595037400722504\n",
      "epoch: 2, batch: 284, loss: 0.25693416595458984\n",
      "epoch: 2, batch: 285, loss: 0.08232566714286804\n",
      "epoch: 2, batch: 286, loss: 0.037721436470746994\n",
      "epoch: 2, batch: 287, loss: 0.18107357621192932\n",
      "epoch: 2, batch: 288, loss: 0.10573527216911316\n",
      "epoch: 2, batch: 289, loss: 0.030553633347153664\n",
      "epoch: 2, batch: 290, loss: 0.08722766488790512\n",
      "epoch: 2, batch: 291, loss: 0.07006193697452545\n",
      "epoch: 2, batch: 292, loss: 0.028313741087913513\n",
      "epoch: 2, batch: 293, loss: 0.07330383360385895\n",
      "epoch: 2, batch: 294, loss: 0.09646130353212357\n",
      "epoch: 2, batch: 295, loss: 0.04342931881546974\n",
      "epoch: 2, batch: 296, loss: 0.07668060809373856\n",
      "epoch: 2, batch: 297, loss: 0.19950048625469208\n",
      "epoch: 2, batch: 298, loss: 0.040120042860507965\n",
      "epoch: 2, batch: 299, loss: 0.388772189617157\n",
      "epoch: 2, batch: 300, loss: 0.025644157081842422\n",
      "epoch: 2, batch: 301, loss: 0.05927969887852669\n",
      "epoch: 2, batch: 302, loss: 0.05509120225906372\n",
      "epoch: 2, batch: 303, loss: 0.007363869342952967\n",
      "epoch: 2, batch: 304, loss: 0.02823788858950138\n",
      "epoch: 2, batch: 305, loss: 0.04201743006706238\n",
      "epoch: 2, batch: 306, loss: 0.09169035404920578\n",
      "epoch: 2, batch: 307, loss: 0.0848761796951294\n",
      "epoch: 2, batch: 308, loss: 0.05370824411511421\n",
      "epoch: 2, batch: 309, loss: 0.014883562922477722\n",
      "epoch: 2, batch: 310, loss: 0.12402364611625671\n",
      "epoch: 2, batch: 311, loss: 0.0762888565659523\n",
      "epoch: 2, batch: 312, loss: 0.41238394379615784\n",
      "epoch: 2, batch: 313, loss: 0.08442439883947372\n",
      "epoch: 2, batch: 314, loss: 0.038783226162195206\n",
      "epoch: 2, batch: 315, loss: 0.028492258861660957\n",
      "epoch: 2, batch: 316, loss: 0.04664598032832146\n",
      "epoch: 2, batch: 317, loss: 0.06155622750520706\n",
      "epoch: 2, batch: 318, loss: 0.024019647389650345\n",
      "epoch: 2, batch: 319, loss: 0.032540999352931976\n",
      "epoch: 2, batch: 320, loss: 0.09365594387054443\n",
      "epoch: 2, batch: 321, loss: 0.01148007158190012\n",
      "epoch: 2, batch: 322, loss: 0.1769859343767166\n",
      "epoch: 2, batch: 323, loss: 0.2190762460231781\n",
      "epoch: 2, batch: 324, loss: 0.04754946753382683\n",
      "epoch: 2, batch: 325, loss: 0.05893654376268387\n",
      "epoch: 2, batch: 326, loss: 0.055346135050058365\n",
      "epoch: 2, batch: 327, loss: 0.08706837147474289\n",
      "epoch: 2, batch: 328, loss: 0.03580702468752861\n",
      "epoch: 2, batch: 329, loss: 0.010708077810704708\n",
      "epoch: 2, batch: 330, loss: 0.037590812891721725\n",
      "epoch: 2, batch: 331, loss: 0.13591741025447845\n",
      "epoch: 2, batch: 332, loss: 0.07269695401191711\n",
      "epoch: 2, batch: 333, loss: 0.05524107441306114\n",
      "epoch: 2, batch: 334, loss: 0.15656936168670654\n",
      "epoch: 2, batch: 335, loss: 0.04869048297405243\n",
      "epoch: 2, batch: 336, loss: 0.029914280399680138\n",
      "epoch: 2, batch: 337, loss: 0.0836658775806427\n",
      "epoch: 2, batch: 338, loss: 0.12576068937778473\n",
      "epoch: 2, batch: 339, loss: 0.045615822076797485\n",
      "epoch: 2, batch: 340, loss: 0.1887131780385971\n",
      "epoch: 2, batch: 341, loss: 0.08057404309511185\n",
      "epoch: 2, batch: 342, loss: 0.06760871410369873\n",
      "epoch: 2, batch: 343, loss: 0.026269733905792236\n",
      "epoch: 2, batch: 344, loss: 0.11471108347177505\n",
      "epoch: 2, batch: 345, loss: 0.13771550357341766\n",
      "epoch: 2, batch: 346, loss: 0.14355885982513428\n",
      "epoch: 2, batch: 347, loss: 0.010234395042061806\n",
      "epoch: 2, batch: 348, loss: 0.05775456130504608\n",
      "epoch: 2, batch: 349, loss: 0.03532206267118454\n",
      "epoch: 2, batch: 350, loss: 0.03832738474011421\n",
      "epoch: 2, batch: 351, loss: 0.1795855164527893\n",
      "epoch: 2, batch: 352, loss: 0.07970214635133743\n",
      "epoch: 2, batch: 353, loss: 0.04782656580209732\n",
      "epoch: 2, batch: 354, loss: 0.024112647399306297\n",
      "epoch: 2, batch: 355, loss: 0.07277543097734451\n",
      "epoch: 2, batch: 356, loss: 0.1309266984462738\n",
      "epoch: 2, batch: 357, loss: 0.06820998340845108\n",
      "epoch: 2, batch: 358, loss: 0.036981336772441864\n",
      "epoch: 2, batch: 359, loss: 0.01701383665204048\n",
      "epoch: 2, batch: 360, loss: 0.11722266674041748\n",
      "epoch: 2, batch: 361, loss: 0.09934154897928238\n",
      "epoch: 2, batch: 362, loss: 0.07084054499864578\n",
      "epoch: 2, batch: 363, loss: 0.05285472050309181\n",
      "epoch: 2, batch: 364, loss: 0.08235462009906769\n",
      "epoch: 2, batch: 365, loss: 0.020475901663303375\n",
      "epoch: 2, batch: 366, loss: 0.1200123131275177\n",
      "epoch: 2, batch: 367, loss: 0.08018240332603455\n",
      "epoch: 2, batch: 368, loss: 0.015424976125359535\n",
      "epoch: 2, batch: 369, loss: 0.04091787710785866\n",
      "epoch: 2, batch: 370, loss: 0.06728844344615936\n",
      "epoch: 2, batch: 371, loss: 0.014058418571949005\n",
      "epoch: 2, batch: 372, loss: 0.08746296912431717\n",
      "epoch: 2, batch: 373, loss: 0.0353471003472805\n",
      "epoch: 2, batch: 374, loss: 0.036173779517412186\n",
      "epoch: 2, batch: 375, loss: 0.09062036871910095\n",
      "epoch: 2, batch: 376, loss: 0.028959602117538452\n",
      "epoch: 2, batch: 377, loss: 0.1569453477859497\n",
      "epoch: 2, batch: 378, loss: 0.02434614673256874\n",
      "epoch: 2, batch: 379, loss: 0.03740249574184418\n",
      "epoch: 2, batch: 380, loss: 0.017320482060313225\n",
      "epoch: 2, batch: 381, loss: 0.0514526329934597\n",
      "epoch: 2, batch: 382, loss: 0.04158029332756996\n",
      "epoch: 2, batch: 383, loss: 0.013939562253654003\n",
      "epoch: 2, batch: 384, loss: 0.04211759939789772\n",
      "epoch: 2, batch: 385, loss: 0.032577890902757645\n",
      "epoch: 2, batch: 386, loss: 0.13646511733531952\n",
      "epoch: 2, batch: 387, loss: 0.08036883175373077\n",
      "epoch: 2, batch: 388, loss: 0.1064140573143959\n",
      "epoch: 2, batch: 389, loss: 0.025978412479162216\n",
      "epoch: 2, batch: 390, loss: 0.012552962638437748\n",
      "epoch: 2, batch: 391, loss: 0.010835775174200535\n",
      "epoch: 2, batch: 392, loss: 0.16063953936100006\n",
      "epoch: 2, batch: 393, loss: 0.06582619994878769\n",
      "epoch: 2, batch: 394, loss: 0.010742804035544395\n",
      "epoch: 2, batch: 395, loss: 0.05418039485812187\n",
      "epoch: 2, batch: 396, loss: 0.08146174252033234\n",
      "epoch: 2, batch: 397, loss: 0.024716777727007866\n",
      "epoch: 2, batch: 398, loss: 0.0754372701048851\n",
      "epoch: 2, batch: 399, loss: 0.04906884580850601\n",
      "epoch: 2, batch: 400, loss: 0.036990758031606674\n",
      "epoch: 2, batch: 401, loss: 0.1391717493534088\n",
      "epoch: 2, batch: 402, loss: 0.056873202323913574\n",
      "epoch: 2, batch: 403, loss: 0.13185684382915497\n",
      "epoch: 2, batch: 404, loss: 0.039980072528123856\n",
      "epoch: 2, batch: 405, loss: 0.1281140148639679\n",
      "epoch: 2, batch: 406, loss: 0.034778885543346405\n",
      "epoch: 2, batch: 407, loss: 0.01306062936782837\n",
      "epoch: 2, batch: 408, loss: 0.030779974535107613\n",
      "epoch: 2, batch: 409, loss: 0.08627007901668549\n",
      "epoch: 2, batch: 410, loss: 0.04659004509449005\n",
      "epoch: 2, batch: 411, loss: 0.0723724365234375\n",
      "epoch: 2, batch: 412, loss: 0.04830941557884216\n",
      "epoch: 2, batch: 413, loss: 0.022645147517323494\n",
      "epoch: 2, batch: 414, loss: 0.09256544709205627\n",
      "epoch: 2, batch: 415, loss: 0.22927944362163544\n",
      "epoch: 2, batch: 416, loss: 0.025716982781887054\n",
      "epoch: 2, batch: 417, loss: 0.15821310877799988\n",
      "epoch: 2, batch: 418, loss: 0.016403064131736755\n",
      "epoch: 2, batch: 419, loss: 0.030758488923311234\n",
      "epoch: 2, batch: 420, loss: 0.020119402557611465\n",
      "epoch: 2, batch: 421, loss: 0.05107531324028969\n",
      "epoch: 2, batch: 422, loss: 0.025697823613882065\n",
      "epoch: 2, batch: 423, loss: 0.08007295429706573\n",
      "epoch: 2, batch: 424, loss: 0.026149030774831772\n",
      "epoch: 2, batch: 425, loss: 0.14952240884304047\n",
      "epoch: 2, batch: 426, loss: 0.053063392639160156\n",
      "epoch: 2, batch: 427, loss: 0.06378516554832458\n",
      "epoch: 2, batch: 428, loss: 0.03568116948008537\n",
      "epoch: 2, batch: 429, loss: 0.16393311321735382\n",
      "epoch: 2, batch: 430, loss: 0.039850201457738876\n",
      "epoch: 2, batch: 431, loss: 0.04655395448207855\n",
      "epoch: 2, batch: 432, loss: 0.08219955116510391\n",
      "epoch: 2, batch: 433, loss: 0.041813381016254425\n",
      "epoch: 2, batch: 434, loss: 0.02490348555147648\n",
      "epoch: 2, batch: 435, loss: 0.04985920339822769\n",
      "epoch: 2, batch: 436, loss: 0.057211630046367645\n",
      "epoch: 2, batch: 437, loss: 0.08218236267566681\n",
      "epoch: 2, batch: 438, loss: 0.14227759838104248\n",
      "epoch: 2, batch: 439, loss: 0.02103860303759575\n",
      "epoch: 2, batch: 440, loss: 0.10552826523780823\n",
      "epoch: 2, batch: 441, loss: 0.15296487510204315\n",
      "epoch: 2, batch: 442, loss: 0.14751502871513367\n",
      "epoch: 2, batch: 443, loss: 0.03727525845170021\n",
      "epoch: 2, batch: 444, loss: 0.15059742331504822\n",
      "epoch: 2, batch: 445, loss: 0.0503556914627552\n",
      "epoch: 2, batch: 446, loss: 0.030504699796438217\n",
      "epoch: 2, batch: 447, loss: 0.0938892588019371\n",
      "epoch: 2, batch: 448, loss: 0.06415262073278427\n",
      "epoch: 2, batch: 449, loss: 0.024161091074347496\n",
      "epoch: 2, batch: 450, loss: 0.0337396077811718\n",
      "epoch: 2, batch: 451, loss: 0.013367163948714733\n",
      "epoch: 2, batch: 452, loss: 0.1668749302625656\n",
      "epoch: 2, batch: 453, loss: 0.032666999846696854\n",
      "epoch: 2, batch: 454, loss: 0.02285796031355858\n",
      "epoch: 2, batch: 455, loss: 0.11660557985305786\n",
      "epoch: 2, batch: 456, loss: 0.035016413778066635\n",
      "epoch: 2, batch: 457, loss: 0.16391527652740479\n",
      "epoch: 2, batch: 458, loss: 0.020154548808932304\n",
      "epoch: 2, batch: 459, loss: 0.011623905971646309\n",
      "epoch: 2, batch: 460, loss: 0.027225907891988754\n",
      "epoch: 2, batch: 461, loss: 0.032290130853652954\n",
      "epoch: 2, batch: 462, loss: 0.08659373223781586\n",
      "epoch: 2, batch: 463, loss: 0.028364792466163635\n",
      "epoch: 2, batch: 464, loss: 0.01987987942993641\n",
      "epoch: 2, batch: 465, loss: 0.26072266697883606\n",
      "epoch: 2, batch: 466, loss: 0.11842881143093109\n",
      "epoch: 2, batch: 467, loss: 0.013803141191601753\n",
      "epoch: 2, batch: 468, loss: 0.030743110924959183\n",
      "epoch: 2, batch: 469, loss: 0.01643531396985054\n",
      "epoch: 2, batch: 470, loss: 0.05627452954649925\n",
      "epoch: 2, batch: 471, loss: 0.02067568711936474\n",
      "epoch: 2, batch: 472, loss: 0.08243381977081299\n",
      "epoch: 2, batch: 473, loss: 0.0627395287156105\n",
      "epoch: 2, batch: 474, loss: 0.03653906285762787\n",
      "epoch: 2, batch: 475, loss: 0.04170500859618187\n",
      "epoch: 2, batch: 476, loss: 0.03637456148862839\n",
      "epoch: 2, batch: 477, loss: 0.06318200379610062\n",
      "epoch: 2, batch: 478, loss: 0.0601300373673439\n",
      "epoch: 2, batch: 479, loss: 0.017954912036657333\n",
      "epoch: 2, batch: 480, loss: 0.027282360941171646\n",
      "epoch: 2, batch: 481, loss: 0.013368645682930946\n",
      "epoch: 2, batch: 482, loss: 0.0369727723300457\n",
      "epoch: 2, batch: 483, loss: 0.10539444535970688\n",
      "epoch: 2, batch: 484, loss: 0.02165522426366806\n",
      "epoch: 2, batch: 485, loss: 0.016795331612229347\n",
      "epoch: 2, batch: 486, loss: 0.09619306027889252\n",
      "epoch: 2, batch: 487, loss: 0.05475098639726639\n",
      "epoch: 2, batch: 488, loss: 0.3408392071723938\n",
      "epoch: 2, batch: 489, loss: 0.0699169710278511\n",
      "epoch: 2, batch: 490, loss: 0.07293107360601425\n",
      "epoch: 2, batch: 491, loss: 0.04659391567111015\n",
      "epoch: 2, batch: 492, loss: 0.05381421744823456\n",
      "epoch: 2, batch: 493, loss: 0.06197614595293999\n",
      "epoch: 2, batch: 494, loss: 0.011121133342385292\n",
      "epoch: 2, batch: 495, loss: 0.0628715381026268\n",
      "epoch: 2, batch: 496, loss: 0.1361268162727356\n",
      "epoch: 2, batch: 497, loss: 0.09064077585935593\n",
      "epoch: 2, batch: 498, loss: 0.2578057050704956\n",
      "epoch: 2, batch: 499, loss: 0.1063278540968895\n",
      "epoch: 2, batch: 500, loss: 0.22787822782993317\n",
      "epoch: 2, batch: 501, loss: 0.05667268484830856\n",
      "epoch: 2, batch: 502, loss: 0.021337244659662247\n",
      "epoch: 2, batch: 503, loss: 0.05859016254544258\n",
      "epoch: 2, batch: 504, loss: 0.10359758138656616\n",
      "epoch: 2, batch: 505, loss: 0.03544451296329498\n",
      "epoch: 2, batch: 506, loss: 0.0909837931394577\n",
      "epoch: 2, batch: 507, loss: 0.028666624799370766\n",
      "epoch: 2, batch: 508, loss: 0.03936062380671501\n",
      "epoch: 2, batch: 509, loss: 0.09640631824731827\n",
      "epoch: 2, batch: 510, loss: 0.07153408974409103\n",
      "epoch: 2, batch: 511, loss: 0.07899493724107742\n",
      "epoch: 2, batch: 512, loss: 0.011329030618071556\n",
      "epoch: 2, batch: 513, loss: 0.027669033035635948\n",
      "epoch: 2, batch: 514, loss: 0.056849539279937744\n",
      "epoch: 2, batch: 515, loss: 0.06586771458387375\n",
      "epoch: 2, batch: 516, loss: 0.06212061643600464\n",
      "epoch: 2, batch: 517, loss: 0.03311369568109512\n",
      "epoch: 2, batch: 518, loss: 0.12747015058994293\n",
      "epoch: 2, batch: 519, loss: 0.01932482421398163\n",
      "epoch: 2, batch: 520, loss: 0.03216278553009033\n",
      "epoch: 2, batch: 521, loss: 0.02230527624487877\n",
      "epoch: 2, batch: 522, loss: 0.044402483850717545\n",
      "epoch: 2, batch: 523, loss: 0.03080783411860466\n",
      "epoch: 2, batch: 524, loss: 0.05023428797721863\n",
      "epoch: 2, batch: 525, loss: 0.13526780903339386\n",
      "epoch: 2, batch: 526, loss: 0.15897127985954285\n",
      "epoch: 2, batch: 527, loss: 0.09992948174476624\n",
      "epoch: 2, batch: 528, loss: 0.06469579041004181\n",
      "epoch: 2, batch: 529, loss: 0.008910796605050564\n",
      "epoch: 2, batch: 530, loss: 0.07051549106836319\n",
      "epoch: 2, batch: 531, loss: 0.05473047122359276\n",
      "epoch: 2, batch: 532, loss: 0.012124608270823956\n",
      "epoch: 2, batch: 533, loss: 0.10510707646608353\n",
      "epoch: 2, batch: 534, loss: 0.007084221113473177\n",
      "epoch: 2, batch: 535, loss: 0.015875212848186493\n",
      "epoch: 2, batch: 536, loss: 0.015037810429930687\n",
      "epoch: 2, batch: 537, loss: 0.012407694011926651\n",
      "epoch: 2, batch: 538, loss: 0.08722896873950958\n",
      "epoch: 2, batch: 539, loss: 0.028569595888257027\n",
      "epoch: 2, batch: 540, loss: 0.11106698215007782\n",
      "epoch: 2, batch: 541, loss: 0.18139897286891937\n",
      "epoch: 2, batch: 542, loss: 0.035421889275312424\n",
      "epoch: 2, batch: 543, loss: 0.019557379186153412\n",
      "epoch: 2, batch: 544, loss: 0.12149515748023987\n",
      "epoch: 2, batch: 545, loss: 0.0736849382519722\n",
      "epoch: 2, batch: 546, loss: 0.03604642674326897\n",
      "epoch: 2, batch: 547, loss: 0.08283378928899765\n",
      "epoch: 2, batch: 548, loss: 0.04934438690543175\n",
      "epoch: 2, batch: 549, loss: 0.029627641662955284\n",
      "epoch: 2, batch: 550, loss: 0.14750893414020538\n",
      "epoch: 2, batch: 551, loss: 0.028782173991203308\n",
      "epoch: 2, batch: 552, loss: 0.06625590473413467\n",
      "epoch: 2, batch: 553, loss: 0.037373196333646774\n",
      "epoch: 2, batch: 554, loss: 0.058599501848220825\n",
      "epoch: 2, batch: 555, loss: 0.16809070110321045\n",
      "epoch: 2, batch: 556, loss: 0.016437916085124016\n",
      "epoch: 2, batch: 557, loss: 0.042320337146520615\n",
      "epoch: 2, batch: 558, loss: 0.1267356276512146\n",
      "epoch: 2, batch: 559, loss: 0.2574346363544464\n",
      "epoch: 2, batch: 560, loss: 0.0587613545358181\n",
      "epoch: 2, batch: 561, loss: 0.026159774512052536\n",
      "epoch: 2, batch: 562, loss: 0.10547658056020737\n",
      "epoch: 2, batch: 563, loss: 0.052379243075847626\n",
      "epoch: 2, batch: 564, loss: 0.0634598657488823\n",
      "epoch: 2, batch: 565, loss: 0.022866331040859222\n",
      "epoch: 2, batch: 566, loss: 0.03093530796468258\n",
      "epoch: 2, batch: 567, loss: 0.03753436729311943\n",
      "epoch: 2, batch: 568, loss: 0.06488705426454544\n",
      "epoch: 2, batch: 569, loss: 0.06877682358026505\n",
      "epoch: 2, batch: 570, loss: 0.04784376546740532\n",
      "epoch: 2, batch: 571, loss: 0.11652415245771408\n",
      "epoch: 2, batch: 572, loss: 0.015438313595950603\n",
      "epoch: 2, batch: 573, loss: 0.005136899650096893\n",
      "epoch: 2, batch: 574, loss: 0.025448404252529144\n",
      "epoch: 2, batch: 575, loss: 0.014479459263384342\n",
      "epoch: 2, batch: 576, loss: 0.10854404419660568\n",
      "epoch: 2, batch: 577, loss: 0.03779362142086029\n",
      "epoch: 2, batch: 578, loss: 0.05738334730267525\n",
      "epoch: 2, batch: 579, loss: 0.0848165899515152\n",
      "epoch: 2, batch: 580, loss: 0.024244399741292\n",
      "epoch: 2, batch: 581, loss: 0.022593064233660698\n",
      "epoch: 2, batch: 582, loss: 0.07436782866716385\n",
      "epoch: 2, batch: 583, loss: 0.030102191492915154\n",
      "epoch: 2, batch: 584, loss: 0.06493184715509415\n",
      "epoch: 2, batch: 585, loss: 0.06683055311441422\n",
      "epoch: 2, batch: 586, loss: 0.24108293652534485\n",
      "epoch: 2, batch: 587, loss: 0.08131170272827148\n",
      "epoch: 2, batch: 588, loss: 0.06967860460281372\n",
      "epoch: 2, batch: 589, loss: 0.08885470777750015\n",
      "epoch: 2, batch: 590, loss: 0.0992228165268898\n",
      "epoch: 2, batch: 591, loss: 0.11615876853466034\n",
      "epoch: 2, batch: 592, loss: 0.035676877945661545\n",
      "epoch: 2, batch: 593, loss: 0.12182021886110306\n",
      "epoch: 2, batch: 594, loss: 0.1412965953350067\n",
      "epoch: 2, batch: 595, loss: 0.0647086650133133\n",
      "epoch: 2, batch: 596, loss: 0.07267201691865921\n",
      "epoch: 2, batch: 597, loss: 0.034235429018735886\n",
      "epoch: 2, batch: 598, loss: 0.0516839474439621\n",
      "epoch: 2, batch: 599, loss: 0.0818900540471077\n",
      "epoch: 2, batch: 600, loss: 0.05972444638609886\n",
      "epoch: 2, batch: 601, loss: 0.11751154810190201\n",
      "epoch: 2, batch: 602, loss: 0.08772318810224533\n",
      "epoch: 2, batch: 603, loss: 0.05075876787304878\n",
      "epoch: 2, batch: 604, loss: 0.1347695142030716\n",
      "epoch: 2, batch: 605, loss: 0.08199247717857361\n",
      "epoch: 2, batch: 606, loss: 0.12958359718322754\n",
      "epoch: 2, batch: 607, loss: 0.12074922025203705\n",
      "epoch: 2, batch: 608, loss: 0.012145121581852436\n",
      "epoch: 2, batch: 609, loss: 0.06799201667308807\n",
      "epoch: 2, batch: 610, loss: 0.02784563973546028\n",
      "epoch: 2, batch: 611, loss: 0.13470421731472015\n",
      "epoch: 2, batch: 612, loss: 0.03174015134572983\n",
      "epoch: 2, batch: 613, loss: 0.0686522126197815\n",
      "epoch: 2, batch: 614, loss: 0.06311376392841339\n",
      "epoch: 2, batch: 615, loss: 0.08908696472644806\n",
      "epoch: 2, batch: 616, loss: 0.019073499366641045\n",
      "epoch: 2, batch: 617, loss: 0.06956706196069717\n",
      "epoch: 2, batch: 618, loss: 0.09478728473186493\n",
      "epoch: 2, batch: 619, loss: 0.14287075400352478\n",
      "epoch: 2, batch: 620, loss: 0.04503346607089043\n",
      "epoch: 2, batch: 621, loss: 0.061873190104961395\n",
      "epoch: 2, batch: 622, loss: 0.023016298189759254\n",
      "epoch: 2, batch: 623, loss: 0.04587697237730026\n",
      "epoch: 2, batch: 624, loss: 0.06891465187072754\n",
      "epoch: 2, batch: 625, loss: 0.12498282641172409\n",
      "epoch: 2, batch: 626, loss: 0.04638085141777992\n",
      "epoch: 2, batch: 627, loss: 0.04668634384870529\n",
      "epoch: 2, batch: 628, loss: 0.052559465169906616\n",
      "epoch: 2, batch: 629, loss: 0.050487104803323746\n",
      "epoch: 2, batch: 630, loss: 0.023830436170101166\n",
      "epoch: 2, batch: 631, loss: 0.03155497834086418\n",
      "epoch: 2, batch: 632, loss: 0.13278363645076752\n",
      "epoch: 2, batch: 633, loss: 0.04535937309265137\n",
      "epoch: 2, batch: 634, loss: 0.10884709656238556\n",
      "epoch: 2, batch: 635, loss: 0.13974538445472717\n",
      "epoch: 2, batch: 636, loss: 0.19777031242847443\n",
      "epoch: 2, batch: 637, loss: 0.014203083701431751\n",
      "epoch: 2, batch: 638, loss: 0.05999334156513214\n",
      "epoch: 2, batch: 639, loss: 0.04843752086162567\n",
      "epoch: 2, batch: 640, loss: 0.10110990703105927\n",
      "epoch: 2, batch: 641, loss: 0.12785151600837708\n",
      "epoch: 2, batch: 642, loss: 0.06979520618915558\n",
      "epoch: 2, batch: 643, loss: 0.16366297006607056\n",
      "epoch: 2, batch: 644, loss: 0.12434142827987671\n",
      "epoch: 2, batch: 645, loss: 0.08430250734090805\n",
      "epoch: 2, batch: 646, loss: 0.06008269265294075\n",
      "epoch: 2, batch: 647, loss: 0.06850224733352661\n",
      "epoch: 2, batch: 648, loss: 0.03163504973053932\n",
      "epoch: 2, batch: 649, loss: 0.04668017104268074\n",
      "epoch: 2, batch: 650, loss: 0.016991376876831055\n",
      "epoch: 2, batch: 651, loss: 0.06623230129480362\n",
      "epoch: 2, batch: 652, loss: 0.11213700473308563\n",
      "epoch: 2, batch: 653, loss: 0.0638442263007164\n",
      "epoch: 2, batch: 654, loss: 0.08021311461925507\n",
      "epoch: 2, batch: 655, loss: 0.011159569956362247\n",
      "epoch: 2, batch: 656, loss: 0.049210816621780396\n",
      "epoch: 2, batch: 657, loss: 0.0591186061501503\n",
      "epoch: 2, batch: 658, loss: 0.04869905486702919\n",
      "epoch: 2, batch: 659, loss: 0.042922601103782654\n",
      "epoch: 2, batch: 660, loss: 0.09964302182197571\n",
      "epoch: 2, batch: 661, loss: 0.03966265171766281\n",
      "epoch: 2, batch: 662, loss: 0.14164327085018158\n",
      "epoch: 2, batch: 663, loss: 0.0633305236697197\n",
      "epoch: 2, batch: 664, loss: 0.014304938726127148\n",
      "epoch: 2, batch: 665, loss: 0.16894762217998505\n",
      "epoch: 2, batch: 666, loss: 0.05226677656173706\n",
      "epoch: 2, batch: 667, loss: 0.06480865925550461\n",
      "epoch: 2, batch: 668, loss: 0.04006729647517204\n",
      "epoch: 2, batch: 669, loss: 0.015602749772369862\n",
      "epoch: 2, batch: 670, loss: 0.05554315820336342\n",
      "epoch: 2, batch: 671, loss: 0.07807022333145142\n",
      "epoch: 2, batch: 672, loss: 0.009792830795049667\n",
      "epoch: 2, batch: 673, loss: 0.061135586351156235\n",
      "epoch: 2, batch: 674, loss: 0.05547170713543892\n",
      "epoch: 2, batch: 675, loss: 0.0916535034775734\n",
      "epoch: 2, batch: 676, loss: 0.1719060093164444\n",
      "epoch: 2, batch: 677, loss: 0.1597292572259903\n",
      "epoch: 2, batch: 678, loss: 0.1062389388680458\n",
      "epoch: 2, batch: 679, loss: 0.04548048600554466\n",
      "epoch: 2, batch: 680, loss: 0.09163831919431686\n",
      "epoch: 2, batch: 681, loss: 0.026225371286273003\n",
      "epoch: 2, batch: 682, loss: 0.06490105390548706\n",
      "epoch: 2, batch: 683, loss: 0.1135043352842331\n",
      "epoch: 2, batch: 684, loss: 0.011691732332110405\n",
      "epoch: 2, batch: 685, loss: 0.019897855818271637\n",
      "epoch: 2, batch: 686, loss: 0.05915866047143936\n",
      "epoch: 2, batch: 687, loss: 0.16911251842975616\n",
      "epoch: 2, batch: 688, loss: 0.05150870233774185\n",
      "epoch: 2, batch: 689, loss: 0.009926388040184975\n",
      "epoch: 2, batch: 690, loss: 0.06959057599306107\n",
      "epoch: 2, batch: 691, loss: 0.010653359815478325\n",
      "epoch: 2, batch: 692, loss: 0.025952881202101707\n",
      "epoch: 2, batch: 693, loss: 0.05136033892631531\n",
      "epoch: 2, batch: 694, loss: 0.0794924721121788\n",
      "epoch: 2, batch: 695, loss: 0.007260853424668312\n",
      "epoch: 2, batch: 696, loss: 0.23220662772655487\n",
      "epoch: 2, batch: 697, loss: 0.057693298906087875\n",
      "epoch: 2, batch: 698, loss: 0.1299905627965927\n",
      "epoch: 2, batch: 699, loss: 0.14018982648849487\n",
      "epoch: 2, batch: 700, loss: 0.04531081020832062\n",
      "epoch: 2, batch: 701, loss: 0.04759126901626587\n",
      "epoch: 2, batch: 702, loss: 0.026470353826880455\n",
      "epoch: 2, batch: 703, loss: 0.017015758901834488\n",
      "epoch: 2, batch: 704, loss: 0.05817970260977745\n",
      "epoch: 2, batch: 705, loss: 0.05518380552530289\n",
      "epoch: 2, batch: 706, loss: 0.05508987605571747\n",
      "epoch: 2, batch: 707, loss: 0.04624365270137787\n",
      "epoch: 2, batch: 708, loss: 0.02250118926167488\n",
      "epoch: 2, batch: 709, loss: 0.05298192799091339\n",
      "epoch: 2, batch: 710, loss: 0.07489341497421265\n",
      "epoch: 2, batch: 711, loss: 0.0949598178267479\n",
      "epoch: 2, batch: 712, loss: 0.10808727145195007\n",
      "epoch: 2, batch: 713, loss: 0.015627099201083183\n",
      "epoch: 2, batch: 714, loss: 0.16588640213012695\n",
      "epoch: 2, batch: 715, loss: 0.022687580436468124\n",
      "epoch: 2, batch: 716, loss: 0.00946270115673542\n",
      "epoch: 2, batch: 717, loss: 0.09942109137773514\n",
      "epoch: 2, batch: 718, loss: 0.013789382763206959\n",
      "epoch: 2, batch: 719, loss: 0.04155309125781059\n",
      "epoch: 2, batch: 720, loss: 0.052954547107219696\n",
      "epoch: 2, batch: 721, loss: 0.08367592096328735\n",
      "epoch: 2, batch: 722, loss: 0.02695290744304657\n",
      "epoch: 2, batch: 723, loss: 0.10539468377828598\n",
      "epoch: 2, batch: 724, loss: 0.03296831622719765\n",
      "epoch: 2, batch: 725, loss: 0.04269612580537796\n",
      "epoch: 2, batch: 726, loss: 0.020701756700873375\n",
      "epoch: 2, batch: 727, loss: 0.04710213467478752\n",
      "epoch: 2, batch: 728, loss: 0.010547440499067307\n",
      "epoch: 2, batch: 729, loss: 0.0937865823507309\n",
      "epoch: 2, batch: 730, loss: 0.03476977348327637\n",
      "epoch: 2, batch: 731, loss: 0.2348301112651825\n",
      "epoch: 2, batch: 732, loss: 0.007732868194580078\n",
      "epoch: 2, batch: 733, loss: 0.022293953225016594\n",
      "epoch: 2, batch: 734, loss: 0.047320976853370667\n",
      "epoch: 2, batch: 735, loss: 0.05218317359685898\n",
      "epoch: 2, batch: 736, loss: 0.031945906579494476\n",
      "epoch: 2, batch: 737, loss: 0.03253252059221268\n",
      "epoch: 2, batch: 738, loss: 0.1071399673819542\n",
      "epoch: 2, batch: 739, loss: 0.09256785362958908\n",
      "epoch: 2, batch: 740, loss: 0.11419899761676788\n",
      "epoch: 2, batch: 741, loss: 0.005240569356828928\n",
      "epoch: 2, batch: 742, loss: 0.009302513673901558\n",
      "epoch: 2, batch: 743, loss: 0.021504471078515053\n",
      "epoch: 2, batch: 744, loss: 0.04775848984718323\n",
      "epoch: 2, batch: 745, loss: 0.006935331504791975\n",
      "epoch: 2, batch: 746, loss: 0.10751773416996002\n",
      "epoch: 2, batch: 747, loss: 0.18572930991649628\n",
      "epoch: 2, batch: 748, loss: 0.08753426373004913\n",
      "epoch: 2, batch: 749, loss: 0.012504387646913528\n",
      "epoch: 2, batch: 750, loss: 0.09547697752714157\n",
      "epoch: 2, batch: 751, loss: 0.03378205746412277\n",
      "epoch: 2, batch: 752, loss: 0.05577960982918739\n",
      "epoch: 2, batch: 753, loss: 0.045131657272577286\n",
      "epoch: 2, batch: 754, loss: 0.09903702884912491\n",
      "epoch: 2, batch: 755, loss: 0.02248200960457325\n",
      "epoch: 2, batch: 756, loss: 0.03807302936911583\n",
      "epoch: 2, batch: 757, loss: 0.03554515913128853\n",
      "epoch: 2, batch: 758, loss: 0.04517967998981476\n",
      "epoch: 2, batch: 759, loss: 0.02689746953547001\n",
      "epoch: 2, batch: 760, loss: 0.01792553812265396\n",
      "epoch: 2, batch: 761, loss: 0.062007758766412735\n",
      "epoch: 2, batch: 762, loss: 0.14380131661891937\n",
      "epoch: 2, batch: 763, loss: 0.03280242905020714\n",
      "epoch: 2, batch: 764, loss: 0.1929062306880951\n",
      "epoch: 2, batch: 765, loss: 0.04290806129574776\n",
      "epoch: 2, batch: 766, loss: 0.04197750985622406\n",
      "epoch: 2, batch: 767, loss: 0.07088040560483932\n",
      "epoch: 2, batch: 768, loss: 0.09906718134880066\n",
      "epoch: 2, batch: 769, loss: 0.1319749355316162\n",
      "epoch: 2, batch: 770, loss: 0.03277172893285751\n",
      "epoch: 2, batch: 771, loss: 0.05034910514950752\n",
      "epoch: 2, batch: 772, loss: 0.03709781914949417\n",
      "epoch: 2, batch: 773, loss: 0.047209180891513824\n",
      "epoch: 2, batch: 774, loss: 0.10050307214260101\n",
      "epoch: 2, batch: 775, loss: 0.034333404153585434\n",
      "epoch: 2, batch: 776, loss: 0.01754888892173767\n",
      "epoch: 2, batch: 777, loss: 0.04637616127729416\n",
      "epoch: 2, batch: 778, loss: 0.039873987436294556\n",
      "epoch: 2, batch: 779, loss: 0.01673175022006035\n",
      "epoch: 2, batch: 780, loss: 0.018197257071733475\n",
      "epoch: 2, batch: 781, loss: 0.01561423297971487\n",
      "epoch: 2, batch: 782, loss: 0.07708988338708878\n",
      "epoch: 2, batch: 783, loss: 0.024239543825387955\n",
      "epoch: 2, batch: 784, loss: 0.04301411658525467\n",
      "epoch: 2, batch: 785, loss: 0.01990600675344467\n",
      "epoch: 2, batch: 786, loss: 0.06215914338827133\n",
      "epoch: 2, batch: 787, loss: 0.038058340549468994\n",
      "epoch: 2, batch: 788, loss: 0.06685449182987213\n",
      "epoch: 2, batch: 789, loss: 0.08921755850315094\n",
      "epoch: 2, batch: 790, loss: 0.07916667312383652\n",
      "epoch: 2, batch: 791, loss: 0.04394461214542389\n",
      "epoch: 2, batch: 792, loss: 0.21765360236167908\n",
      "epoch: 2, batch: 793, loss: 0.016971180215477943\n",
      "epoch: 2, batch: 794, loss: 0.021216999739408493\n",
      "epoch: 2, batch: 795, loss: 0.025362567976117134\n",
      "epoch: 2, batch: 796, loss: 0.052611105144023895\n",
      "epoch: 2, batch: 797, loss: 0.0550529770553112\n",
      "epoch: 2, batch: 798, loss: 0.04188266023993492\n",
      "epoch: 2, batch: 799, loss: 0.02363237366080284\n",
      "epoch: 2, batch: 800, loss: 0.061813611537218094\n",
      "epoch: 2, batch: 801, loss: 0.06148919090628624\n",
      "epoch: 2, batch: 802, loss: 0.025086568668484688\n",
      "epoch: 2, batch: 803, loss: 0.09009227156639099\n",
      "epoch: 2, batch: 804, loss: 0.0533442422747612\n",
      "epoch: 2, batch: 805, loss: 0.06349162012338638\n",
      "epoch: 2, batch: 806, loss: 0.06383198499679565\n",
      "epoch: 2, batch: 807, loss: 0.03338758647441864\n",
      "epoch: 2, batch: 808, loss: 0.05968640744686127\n",
      "epoch: 2, batch: 809, loss: 0.0032859842758625746\n",
      "epoch: 2, batch: 810, loss: 0.03769955784082413\n",
      "epoch: 2, batch: 811, loss: 0.030888507142663002\n",
      "epoch: 2, batch: 812, loss: 0.10055738687515259\n",
      "epoch: 2, batch: 813, loss: 0.07133881002664566\n",
      "epoch: 2, batch: 814, loss: 0.13291165232658386\n",
      "epoch: 2, batch: 815, loss: 0.15230479836463928\n",
      "epoch: 2, batch: 816, loss: 0.06240631267428398\n",
      "epoch: 2, batch: 817, loss: 0.12313830107450485\n",
      "epoch: 2, batch: 818, loss: 0.1426912546157837\n",
      "epoch: 2, batch: 819, loss: 0.024776408448815346\n",
      "epoch: 2, batch: 820, loss: 0.06677114963531494\n",
      "epoch: 2, batch: 821, loss: 0.09776395559310913\n",
      "epoch: 2, batch: 822, loss: 0.15925392508506775\n",
      "epoch: 2, batch: 823, loss: 0.11473768949508667\n",
      "epoch: 2, batch: 824, loss: 0.045116353780031204\n",
      "epoch: 2, batch: 825, loss: 0.02262289822101593\n",
      "epoch: 2, batch: 826, loss: 0.0883663073182106\n",
      "epoch: 2, batch: 827, loss: 0.10640988498926163\n",
      "epoch: 2, batch: 828, loss: 0.12746216356754303\n",
      "epoch: 2, batch: 829, loss: 0.017151735723018646\n",
      "epoch: 2, batch: 830, loss: 0.049809519201517105\n",
      "epoch: 2, batch: 831, loss: 0.021928509697318077\n",
      "epoch: 2, batch: 832, loss: 0.03214061260223389\n",
      "epoch: 2, batch: 833, loss: 0.09175676852464676\n",
      "epoch: 2, batch: 834, loss: 0.15221090614795685\n",
      "epoch: 2, batch: 835, loss: 0.06040501967072487\n",
      "epoch: 2, batch: 836, loss: 0.07101737707853317\n",
      "epoch: 2, batch: 837, loss: 0.016648508608341217\n",
      "epoch: 2, batch: 838, loss: 0.08670100569725037\n",
      "epoch: 2, batch: 839, loss: 0.008023361675441265\n",
      "epoch: 2, batch: 840, loss: 0.09672170132398605\n",
      "epoch: 2, batch: 841, loss: 0.03793807700276375\n",
      "epoch: 2, batch: 842, loss: 0.044879209250211716\n",
      "epoch: 2, batch: 843, loss: 0.11561622470617294\n",
      "epoch: 2, batch: 844, loss: 0.044355280697345734\n",
      "epoch: 2, batch: 845, loss: 0.03650221228599548\n",
      "epoch: 2, batch: 846, loss: 0.03306012973189354\n",
      "epoch: 2, batch: 847, loss: 0.13545291125774384\n",
      "epoch: 2, batch: 848, loss: 0.03318104147911072\n",
      "epoch: 2, batch: 849, loss: 0.06276965141296387\n",
      "epoch: 2, batch: 850, loss: 0.09903347492218018\n",
      "epoch: 2, batch: 851, loss: 0.15706110000610352\n",
      "epoch: 2, batch: 852, loss: 0.007768676616251469\n",
      "epoch: 2, batch: 853, loss: 0.11603914946317673\n",
      "epoch: 2, batch: 854, loss: 0.068879634141922\n",
      "epoch: 2, batch: 855, loss: 0.027522344142198563\n",
      "epoch: 2, batch: 856, loss: 0.23892074823379517\n",
      "epoch: 2, batch: 857, loss: 0.10579901933670044\n",
      "epoch: 2, batch: 858, loss: 0.04323537275195122\n",
      "epoch: 2, batch: 859, loss: 0.08212866634130478\n",
      "epoch: 2, batch: 860, loss: 0.25549694895744324\n",
      "epoch: 2, batch: 861, loss: 0.1258004605770111\n",
      "epoch: 2, batch: 862, loss: 0.052381549030542374\n",
      "epoch: 2, batch: 863, loss: 0.083849236369133\n",
      "epoch: 2, batch: 864, loss: 0.04701771214604378\n",
      "epoch: 2, batch: 865, loss: 0.14795513451099396\n",
      "epoch: 2, batch: 866, loss: 0.0725853443145752\n",
      "epoch: 2, batch: 867, loss: 0.0478912889957428\n",
      "epoch: 2, batch: 868, loss: 0.08281729370355606\n",
      "epoch: 2, batch: 869, loss: 0.10999185591936111\n",
      "epoch: 2, batch: 870, loss: 0.05172433331608772\n",
      "epoch: 2, batch: 871, loss: 0.015373815782368183\n",
      "epoch: 2, batch: 872, loss: 0.0718778595328331\n",
      "epoch: 2, batch: 873, loss: 0.028882643207907677\n",
      "epoch: 2, batch: 874, loss: 0.05222708359360695\n",
      "epoch: 2, batch: 875, loss: 0.13035240769386292\n",
      "epoch: 2, batch: 876, loss: 0.04492389038205147\n",
      "epoch: 2, batch: 877, loss: 0.12288182973861694\n",
      "epoch: 2, batch: 878, loss: 0.07101065665483475\n",
      "epoch: 2, batch: 879, loss: 0.02106330916285515\n",
      "epoch: 2, batch: 880, loss: 0.07520399242639542\n",
      "epoch: 2, batch: 881, loss: 0.10618376731872559\n",
      "epoch: 2, batch: 882, loss: 0.04199075698852539\n",
      "epoch: 2, batch: 883, loss: 0.013284011743962765\n",
      "epoch: 2, batch: 884, loss: 0.15393568575382233\n",
      "epoch: 2, batch: 885, loss: 0.08237537741661072\n",
      "epoch: 2, batch: 886, loss: 0.081944040954113\n",
      "epoch: 2, batch: 887, loss: 0.04507695510983467\n",
      "epoch: 2, batch: 888, loss: 0.009060889482498169\n",
      "epoch: 2, batch: 889, loss: 0.0164161566644907\n",
      "epoch: 2, batch: 890, loss: 0.0500895120203495\n",
      "epoch: 2, batch: 891, loss: 0.01924576424062252\n",
      "epoch: 2, batch: 892, loss: 0.03679633140563965\n",
      "epoch: 2, batch: 893, loss: 0.06518686562776566\n",
      "epoch: 2, batch: 894, loss: 0.01484541967511177\n",
      "epoch: 2, batch: 895, loss: 0.12683925032615662\n",
      "epoch: 2, batch: 896, loss: 0.013690770603716373\n",
      "epoch: 2, batch: 897, loss: 0.04798192158341408\n",
      "epoch: 2, batch: 898, loss: 0.07191380858421326\n",
      "epoch: 2, batch: 899, loss: 0.04134472459554672\n",
      "epoch: 2, batch: 900, loss: 0.07975113391876221\n",
      "epoch: 2, batch: 901, loss: 0.10061028599739075\n",
      "epoch: 2, batch: 902, loss: 0.03387623280286789\n",
      "epoch: 2, batch: 903, loss: 0.04976681247353554\n",
      "epoch: 2, batch: 904, loss: 0.04237603396177292\n",
      "epoch: 2, batch: 905, loss: 0.12805303931236267\n",
      "epoch: 2, batch: 906, loss: 0.01193168107420206\n",
      "epoch: 2, batch: 907, loss: 0.012412822805345058\n",
      "epoch: 2, batch: 908, loss: 0.010115057229995728\n",
      "epoch: 2, batch: 909, loss: 0.048535700887441635\n",
      "epoch: 2, batch: 910, loss: 0.0439591221511364\n",
      "epoch: 2, batch: 911, loss: 0.017727989703416824\n",
      "epoch: 2, batch: 912, loss: 0.05140048265457153\n",
      "epoch: 2, batch: 913, loss: 0.04302181303501129\n",
      "epoch: 2, batch: 914, loss: 0.15822438895702362\n",
      "epoch: 2, batch: 915, loss: 0.07757449150085449\n",
      "epoch: 2, batch: 916, loss: 0.034379374235868454\n",
      "epoch: 2, batch: 917, loss: 0.022201387211680412\n",
      "epoch: 2, batch: 918, loss: 0.02600168064236641\n",
      "epoch: 2, batch: 919, loss: 0.0511469691991806\n",
      "epoch: 2, batch: 920, loss: 0.06034449487924576\n",
      "epoch: 2, batch: 921, loss: 0.2534785270690918\n",
      "epoch: 2, batch: 922, loss: 0.048762671649456024\n",
      "epoch: 2, batch: 923, loss: 0.16360008716583252\n",
      "epoch: 2, batch: 924, loss: 0.04912012070417404\n",
      "epoch: 2, batch: 925, loss: 0.006483924575150013\n",
      "epoch: 2, batch: 926, loss: 0.10659921914339066\n",
      "epoch: 2, batch: 927, loss: 0.0180202629417181\n",
      "epoch: 2, batch: 928, loss: 0.039353176951408386\n",
      "epoch: 2, batch: 929, loss: 0.04454837366938591\n",
      "epoch: 2, batch: 930, loss: 0.11206788569688797\n",
      "epoch: 2, batch: 931, loss: 0.10523756593465805\n",
      "epoch: 2, batch: 932, loss: 0.0762220025062561\n",
      "epoch: 2, batch: 933, loss: 0.05208170413970947\n",
      "epoch: 2, batch: 934, loss: 0.05022121220827103\n",
      "epoch: 2, batch: 935, loss: 0.14710332453250885\n",
      "epoch: 2, batch: 936, loss: 0.043570809066295624\n",
      "epoch: 2, batch: 937, loss: 0.05597279593348503\n",
      "epoch: 3, batch: 0, loss: 0.019836699590086937\n",
      "epoch: 3, batch: 1, loss: 0.025991450995206833\n",
      "epoch: 3, batch: 2, loss: 0.11093812435865402\n",
      "epoch: 3, batch: 3, loss: 0.02361297979950905\n",
      "epoch: 3, batch: 4, loss: 0.048333048820495605\n",
      "epoch: 3, batch: 5, loss: 0.03638540208339691\n",
      "epoch: 3, batch: 6, loss: 0.011695457622408867\n",
      "epoch: 3, batch: 7, loss: 0.06831667572259903\n",
      "epoch: 3, batch: 8, loss: 0.011033637449145317\n",
      "epoch: 3, batch: 9, loss: 0.014331731013953686\n",
      "epoch: 3, batch: 10, loss: 0.029926378279924393\n",
      "epoch: 3, batch: 11, loss: 0.13412809371948242\n",
      "epoch: 3, batch: 12, loss: 0.04544917866587639\n",
      "epoch: 3, batch: 13, loss: 0.029273154214024544\n",
      "epoch: 3, batch: 14, loss: 0.01479058712720871\n",
      "epoch: 3, batch: 15, loss: 0.07392382621765137\n",
      "epoch: 3, batch: 16, loss: 0.01095542311668396\n",
      "epoch: 3, batch: 17, loss: 0.07461617887020111\n",
      "epoch: 3, batch: 18, loss: 0.037108346819877625\n",
      "epoch: 3, batch: 19, loss: 0.03337986022233963\n",
      "epoch: 3, batch: 20, loss: 0.019718989729881287\n",
      "epoch: 3, batch: 21, loss: 0.12945836782455444\n",
      "epoch: 3, batch: 22, loss: 0.11137980222702026\n",
      "epoch: 3, batch: 23, loss: 0.0721554085612297\n",
      "epoch: 3, batch: 24, loss: 0.09722939878702164\n",
      "epoch: 3, batch: 25, loss: 0.08367418497800827\n",
      "epoch: 3, batch: 26, loss: 0.03683982789516449\n",
      "epoch: 3, batch: 27, loss: 0.06689254939556122\n",
      "epoch: 3, batch: 28, loss: 0.020508969202637672\n",
      "epoch: 3, batch: 29, loss: 0.02992407977581024\n",
      "epoch: 3, batch: 30, loss: 0.17481987178325653\n",
      "epoch: 3, batch: 31, loss: 0.048655908554792404\n",
      "epoch: 3, batch: 32, loss: 0.01592136360704899\n",
      "epoch: 3, batch: 33, loss: 0.03331749513745308\n",
      "epoch: 3, batch: 34, loss: 0.06903477758169174\n",
      "epoch: 3, batch: 35, loss: 0.006075459532439709\n",
      "epoch: 3, batch: 36, loss: 0.11958696693181992\n",
      "epoch: 3, batch: 37, loss: 0.007681825663894415\n",
      "epoch: 3, batch: 38, loss: 0.024692846462130547\n",
      "epoch: 3, batch: 39, loss: 0.0193350650370121\n",
      "epoch: 3, batch: 40, loss: 0.0177815780043602\n",
      "epoch: 3, batch: 41, loss: 0.13444435596466064\n",
      "epoch: 3, batch: 42, loss: 0.02087671123445034\n",
      "epoch: 3, batch: 43, loss: 0.009514554403722286\n",
      "epoch: 3, batch: 44, loss: 0.03268079087138176\n",
      "epoch: 3, batch: 45, loss: 0.015270346775650978\n",
      "epoch: 3, batch: 46, loss: 0.06949930638074875\n",
      "epoch: 3, batch: 47, loss: 0.17584258317947388\n",
      "epoch: 3, batch: 48, loss: 0.14681093394756317\n",
      "epoch: 3, batch: 49, loss: 0.03247864916920662\n",
      "epoch: 3, batch: 50, loss: 0.0221853069961071\n",
      "epoch: 3, batch: 51, loss: 0.04239214211702347\n",
      "epoch: 3, batch: 52, loss: 0.03871089592576027\n",
      "epoch: 3, batch: 53, loss: 0.127675861120224\n",
      "epoch: 3, batch: 54, loss: 0.028784554451704025\n",
      "epoch: 3, batch: 55, loss: 0.07628794759511948\n",
      "epoch: 3, batch: 56, loss: 0.031946513801813126\n",
      "epoch: 3, batch: 57, loss: 0.06366328150033951\n",
      "epoch: 3, batch: 58, loss: 0.035486459732055664\n",
      "epoch: 3, batch: 59, loss: 0.011597183533012867\n",
      "epoch: 3, batch: 60, loss: 0.0136351827532053\n",
      "epoch: 3, batch: 61, loss: 0.012448208406567574\n",
      "epoch: 3, batch: 62, loss: 0.009939141571521759\n",
      "epoch: 3, batch: 63, loss: 0.04711645841598511\n",
      "epoch: 3, batch: 64, loss: 0.025691848248243332\n",
      "epoch: 3, batch: 65, loss: 0.07604768872261047\n",
      "epoch: 3, batch: 66, loss: 0.03165232762694359\n",
      "epoch: 3, batch: 67, loss: 0.11620712280273438\n",
      "epoch: 3, batch: 68, loss: 0.1630222052335739\n",
      "epoch: 3, batch: 69, loss: 0.05199717357754707\n",
      "epoch: 3, batch: 70, loss: 0.009723389521241188\n",
      "epoch: 3, batch: 71, loss: 0.04993591085076332\n",
      "epoch: 3, batch: 72, loss: 0.02333197556436062\n",
      "epoch: 3, batch: 73, loss: 0.07030075043439865\n",
      "epoch: 3, batch: 74, loss: 0.08139055967330933\n",
      "epoch: 3, batch: 75, loss: 0.10616156458854675\n",
      "epoch: 3, batch: 76, loss: 0.09520091861486435\n",
      "epoch: 3, batch: 77, loss: 0.1331721693277359\n",
      "epoch: 3, batch: 78, loss: 0.042165666818618774\n",
      "epoch: 3, batch: 79, loss: 0.005788774695247412\n",
      "epoch: 3, batch: 80, loss: 0.03605252131819725\n",
      "epoch: 3, batch: 81, loss: 0.054660335183143616\n",
      "epoch: 3, batch: 82, loss: 0.023682937026023865\n",
      "epoch: 3, batch: 83, loss: 0.13967150449752808\n",
      "epoch: 3, batch: 84, loss: 0.0135539211332798\n",
      "epoch: 3, batch: 85, loss: 0.006780211813747883\n",
      "epoch: 3, batch: 86, loss: 0.09841055423021317\n",
      "epoch: 3, batch: 87, loss: 0.04461177811026573\n",
      "epoch: 3, batch: 88, loss: 0.07231221348047256\n",
      "epoch: 3, batch: 89, loss: 0.028837677091360092\n",
      "epoch: 3, batch: 90, loss: 0.06776563823223114\n",
      "epoch: 3, batch: 91, loss: 0.16304470598697662\n",
      "epoch: 3, batch: 92, loss: 0.01717596873641014\n",
      "epoch: 3, batch: 93, loss: 0.01433531567454338\n",
      "epoch: 3, batch: 94, loss: 0.03722487390041351\n",
      "epoch: 3, batch: 95, loss: 0.05208204686641693\n",
      "epoch: 3, batch: 96, loss: 0.038332004100084305\n",
      "epoch: 3, batch: 97, loss: 0.08627966046333313\n",
      "epoch: 3, batch: 98, loss: 0.019611649215221405\n",
      "epoch: 3, batch: 99, loss: 0.06716379523277283\n",
      "epoch: 3, batch: 100, loss: 0.20378804206848145\n",
      "epoch: 3, batch: 101, loss: 0.05306226760149002\n",
      "epoch: 3, batch: 102, loss: 0.009911584667861462\n",
      "epoch: 3, batch: 103, loss: 0.06705085933208466\n",
      "epoch: 3, batch: 104, loss: 0.01650373637676239\n",
      "epoch: 3, batch: 105, loss: 0.018819909542798996\n",
      "epoch: 3, batch: 106, loss: 0.07889462262392044\n",
      "epoch: 3, batch: 107, loss: 0.03284810110926628\n",
      "epoch: 3, batch: 108, loss: 0.038883939385414124\n",
      "epoch: 3, batch: 109, loss: 0.06357985734939575\n",
      "epoch: 3, batch: 110, loss: 0.13295462727546692\n",
      "epoch: 3, batch: 111, loss: 0.0425996370613575\n",
      "epoch: 3, batch: 112, loss: 0.06194308400154114\n",
      "epoch: 3, batch: 113, loss: 0.13279227912425995\n",
      "epoch: 3, batch: 114, loss: 0.05014646053314209\n",
      "epoch: 3, batch: 115, loss: 0.012937644496560097\n",
      "epoch: 3, batch: 116, loss: 0.16005268692970276\n",
      "epoch: 3, batch: 117, loss: 0.03401867300271988\n",
      "epoch: 3, batch: 118, loss: 0.06645488739013672\n",
      "epoch: 3, batch: 119, loss: 0.022654322907328606\n",
      "epoch: 3, batch: 120, loss: 0.01077030785381794\n",
      "epoch: 3, batch: 121, loss: 0.02945557050406933\n",
      "epoch: 3, batch: 122, loss: 0.019168023020029068\n",
      "epoch: 3, batch: 123, loss: 0.02884014882147312\n",
      "epoch: 3, batch: 124, loss: 0.07098673284053802\n",
      "epoch: 3, batch: 125, loss: 0.05099761113524437\n",
      "epoch: 3, batch: 126, loss: 0.021761810407042503\n",
      "epoch: 3, batch: 127, loss: 0.03790806606411934\n",
      "epoch: 3, batch: 128, loss: 0.05909954383969307\n",
      "epoch: 3, batch: 129, loss: 0.020907485857605934\n",
      "epoch: 3, batch: 130, loss: 0.01933961734175682\n",
      "epoch: 3, batch: 131, loss: 0.040473535656929016\n",
      "epoch: 3, batch: 132, loss: 0.007396329659968615\n",
      "epoch: 3, batch: 133, loss: 0.048264529556035995\n",
      "epoch: 3, batch: 134, loss: 0.08786817640066147\n",
      "epoch: 3, batch: 135, loss: 0.00598420575261116\n",
      "epoch: 3, batch: 136, loss: 0.13186615705490112\n",
      "epoch: 3, batch: 137, loss: 0.1721307337284088\n",
      "epoch: 3, batch: 138, loss: 0.03474400192499161\n",
      "epoch: 3, batch: 139, loss: 0.043860115110874176\n",
      "epoch: 3, batch: 140, loss: 0.1508927345275879\n",
      "epoch: 3, batch: 141, loss: 0.006441799458116293\n",
      "epoch: 3, batch: 142, loss: 0.02424536645412445\n",
      "epoch: 3, batch: 143, loss: 0.005698502063751221\n",
      "epoch: 3, batch: 144, loss: 0.06443498283624649\n",
      "epoch: 3, batch: 145, loss: 0.042635973542928696\n",
      "epoch: 3, batch: 146, loss: 0.09526926279067993\n",
      "epoch: 3, batch: 147, loss: 0.03345511853694916\n",
      "epoch: 3, batch: 148, loss: 0.024012330919504166\n",
      "epoch: 3, batch: 149, loss: 0.030685529112815857\n",
      "epoch: 3, batch: 150, loss: 0.006535600405186415\n",
      "epoch: 3, batch: 151, loss: 0.008028706535696983\n",
      "epoch: 3, batch: 152, loss: 0.01699891872704029\n",
      "epoch: 3, batch: 153, loss: 0.0518917553126812\n",
      "epoch: 3, batch: 154, loss: 0.043637532740831375\n",
      "epoch: 3, batch: 155, loss: 0.016238056123256683\n",
      "epoch: 3, batch: 156, loss: 0.05318503454327583\n",
      "epoch: 3, batch: 157, loss: 0.05177346244454384\n",
      "epoch: 3, batch: 158, loss: 0.08203289657831192\n",
      "epoch: 3, batch: 159, loss: 0.04949057847261429\n",
      "epoch: 3, batch: 160, loss: 0.006234936881810427\n",
      "epoch: 3, batch: 161, loss: 0.0918530821800232\n",
      "epoch: 3, batch: 162, loss: 0.0665452852845192\n",
      "epoch: 3, batch: 163, loss: 0.015065438114106655\n",
      "epoch: 3, batch: 164, loss: 0.05719324201345444\n",
      "epoch: 3, batch: 165, loss: 0.10123413801193237\n",
      "epoch: 3, batch: 166, loss: 0.1044565960764885\n",
      "epoch: 3, batch: 167, loss: 0.055036939680576324\n",
      "epoch: 3, batch: 168, loss: 0.07752536237239838\n",
      "epoch: 3, batch: 169, loss: 0.07074326276779175\n",
      "epoch: 3, batch: 170, loss: 0.08129214495420456\n",
      "epoch: 3, batch: 171, loss: 0.024531042203307152\n",
      "epoch: 3, batch: 172, loss: 0.045858509838581085\n",
      "epoch: 3, batch: 173, loss: 0.09100166708230972\n",
      "epoch: 3, batch: 174, loss: 0.041941359639167786\n",
      "epoch: 3, batch: 175, loss: 0.018382232636213303\n",
      "epoch: 3, batch: 176, loss: 0.010097225196659565\n",
      "epoch: 3, batch: 177, loss: 0.0470549538731575\n",
      "epoch: 3, batch: 178, loss: 0.042942486703395844\n",
      "epoch: 3, batch: 179, loss: 0.021458471193909645\n",
      "epoch: 3, batch: 180, loss: 0.017305372282862663\n",
      "epoch: 3, batch: 181, loss: 0.08291991800069809\n",
      "epoch: 3, batch: 182, loss: 0.020180057734251022\n",
      "epoch: 3, batch: 183, loss: 0.016725771129131317\n",
      "epoch: 3, batch: 184, loss: 0.02417830564081669\n",
      "epoch: 3, batch: 185, loss: 0.018094955012202263\n",
      "epoch: 3, batch: 186, loss: 0.0075443340465426445\n",
      "epoch: 3, batch: 187, loss: 0.08469794690608978\n",
      "epoch: 3, batch: 188, loss: 0.03164096921682358\n",
      "epoch: 3, batch: 189, loss: 0.08862930536270142\n",
      "epoch: 3, batch: 190, loss: 0.0082281194627285\n",
      "epoch: 3, batch: 191, loss: 0.01829075813293457\n",
      "epoch: 3, batch: 192, loss: 0.032914791256189346\n",
      "epoch: 3, batch: 193, loss: 0.09652053564786911\n",
      "epoch: 3, batch: 194, loss: 0.02603968232870102\n",
      "epoch: 3, batch: 195, loss: 0.019939858466386795\n",
      "epoch: 3, batch: 196, loss: 0.04874671995639801\n",
      "epoch: 3, batch: 197, loss: 0.05064155533909798\n",
      "epoch: 3, batch: 198, loss: 0.15175238251686096\n",
      "epoch: 3, batch: 199, loss: 0.16750207543373108\n",
      "epoch: 3, batch: 200, loss: 0.04285039007663727\n",
      "epoch: 3, batch: 201, loss: 0.03590409830212593\n",
      "epoch: 3, batch: 202, loss: 0.055069971829652786\n",
      "epoch: 3, batch: 203, loss: 0.005509359296411276\n",
      "epoch: 3, batch: 204, loss: 0.014473761431872845\n",
      "epoch: 3, batch: 205, loss: 0.024417906999588013\n",
      "epoch: 3, batch: 206, loss: 0.03672843053936958\n",
      "epoch: 3, batch: 207, loss: 0.027232110500335693\n",
      "epoch: 3, batch: 208, loss: 0.028963696211576462\n",
      "epoch: 3, batch: 209, loss: 0.045728519558906555\n",
      "epoch: 3, batch: 210, loss: 0.2305627018213272\n",
      "epoch: 3, batch: 211, loss: 0.04942219704389572\n",
      "epoch: 3, batch: 212, loss: 0.11101667582988739\n",
      "epoch: 3, batch: 213, loss: 0.054768744856119156\n",
      "epoch: 3, batch: 214, loss: 0.03788629174232483\n",
      "epoch: 3, batch: 215, loss: 0.012772761285305023\n",
      "epoch: 3, batch: 216, loss: 0.10289377719163895\n",
      "epoch: 3, batch: 217, loss: 0.15439805388450623\n",
      "epoch: 3, batch: 218, loss: 0.0453195720911026\n",
      "epoch: 3, batch: 219, loss: 0.01429726742208004\n",
      "epoch: 3, batch: 220, loss: 0.05065896362066269\n",
      "epoch: 3, batch: 221, loss: 0.02685900591313839\n",
      "epoch: 3, batch: 222, loss: 0.13875725865364075\n",
      "epoch: 3, batch: 223, loss: 0.08845895528793335\n",
      "epoch: 3, batch: 224, loss: 0.027679411694407463\n",
      "epoch: 3, batch: 225, loss: 0.025107625871896744\n",
      "epoch: 3, batch: 226, loss: 0.026670468971133232\n",
      "epoch: 3, batch: 227, loss: 0.03479675203561783\n",
      "epoch: 3, batch: 228, loss: 0.07350107282400131\n",
      "epoch: 3, batch: 229, loss: 0.027527717873454094\n",
      "epoch: 3, batch: 230, loss: 0.0773773267865181\n",
      "epoch: 3, batch: 231, loss: 0.05425421521067619\n",
      "epoch: 3, batch: 232, loss: 0.11389429122209549\n",
      "epoch: 3, batch: 233, loss: 0.024058301001787186\n",
      "epoch: 3, batch: 234, loss: 0.08475968986749649\n",
      "epoch: 3, batch: 235, loss: 0.038450486958026886\n",
      "epoch: 3, batch: 236, loss: 0.023977257311344147\n",
      "epoch: 3, batch: 237, loss: 0.0757920891046524\n",
      "epoch: 3, batch: 238, loss: 0.039841145277023315\n",
      "epoch: 3, batch: 239, loss: 0.04490448534488678\n",
      "epoch: 3, batch: 240, loss: 0.005496692378073931\n",
      "epoch: 3, batch: 241, loss: 0.0434519462287426\n",
      "epoch: 3, batch: 242, loss: 0.06286771595478058\n",
      "epoch: 3, batch: 243, loss: 0.10685527324676514\n",
      "epoch: 3, batch: 244, loss: 0.028994185850024223\n",
      "epoch: 3, batch: 245, loss: 0.03127440810203552\n",
      "epoch: 3, batch: 246, loss: 0.034612987190485\n",
      "epoch: 3, batch: 247, loss: 0.0644175186753273\n",
      "epoch: 3, batch: 248, loss: 0.07478360086679459\n",
      "epoch: 3, batch: 249, loss: 0.0488562136888504\n",
      "epoch: 3, batch: 250, loss: 0.01048372033983469\n",
      "epoch: 3, batch: 251, loss: 0.056925754994153976\n",
      "epoch: 3, batch: 252, loss: 0.04949180409312248\n",
      "epoch: 3, batch: 253, loss: 0.07415542006492615\n",
      "epoch: 3, batch: 254, loss: 0.06168249621987343\n",
      "epoch: 3, batch: 255, loss: 0.012184654362499714\n",
      "epoch: 3, batch: 256, loss: 0.011079232208430767\n",
      "epoch: 3, batch: 257, loss: 0.018961211666464806\n",
      "epoch: 3, batch: 258, loss: 0.01411897037178278\n",
      "epoch: 3, batch: 259, loss: 0.08203382790088654\n",
      "epoch: 3, batch: 260, loss: 0.09064775705337524\n",
      "epoch: 3, batch: 261, loss: 0.015357354655861855\n",
      "epoch: 3, batch: 262, loss: 0.10336635261774063\n",
      "epoch: 3, batch: 263, loss: 0.05871235951781273\n",
      "epoch: 3, batch: 264, loss: 0.13409104943275452\n",
      "epoch: 3, batch: 265, loss: 0.04812011867761612\n",
      "epoch: 3, batch: 266, loss: 0.22043640911579132\n",
      "epoch: 3, batch: 267, loss: 0.030895506963133812\n",
      "epoch: 3, batch: 268, loss: 0.10214786976575851\n",
      "epoch: 3, batch: 269, loss: 0.03525592386722565\n",
      "epoch: 3, batch: 270, loss: 0.04085199534893036\n",
      "epoch: 3, batch: 271, loss: 0.04335964843630791\n",
      "epoch: 3, batch: 272, loss: 0.03423845022916794\n",
      "epoch: 3, batch: 273, loss: 0.1508704274892807\n",
      "epoch: 3, batch: 274, loss: 0.08177069574594498\n",
      "epoch: 3, batch: 275, loss: 0.05160461738705635\n",
      "epoch: 3, batch: 276, loss: 0.13302844762802124\n",
      "epoch: 3, batch: 277, loss: 0.008217058144509792\n",
      "epoch: 3, batch: 278, loss: 0.09540378302335739\n",
      "epoch: 3, batch: 279, loss: 0.046275511384010315\n",
      "epoch: 3, batch: 280, loss: 0.011197793297469616\n",
      "epoch: 3, batch: 281, loss: 0.018719090148806572\n",
      "epoch: 3, batch: 282, loss: 0.0232089776545763\n",
      "epoch: 3, batch: 283, loss: 0.009695161134004593\n",
      "epoch: 3, batch: 284, loss: 0.034822724759578705\n",
      "epoch: 3, batch: 285, loss: 0.030645905062556267\n",
      "epoch: 3, batch: 286, loss: 0.1024661660194397\n",
      "epoch: 3, batch: 287, loss: 0.04622418060898781\n",
      "epoch: 3, batch: 288, loss: 0.021729905158281326\n",
      "epoch: 3, batch: 289, loss: 0.08191291987895966\n",
      "epoch: 3, batch: 290, loss: 0.013346903957426548\n",
      "epoch: 3, batch: 291, loss: 0.0546070896089077\n",
      "epoch: 3, batch: 292, loss: 0.025942213833332062\n",
      "epoch: 3, batch: 293, loss: 0.04593617096543312\n",
      "epoch: 3, batch: 294, loss: 0.05213218927383423\n",
      "epoch: 3, batch: 295, loss: 0.08175922185182571\n",
      "epoch: 3, batch: 296, loss: 0.1673102080821991\n",
      "epoch: 3, batch: 297, loss: 0.02257825806736946\n",
      "epoch: 3, batch: 298, loss: 0.028091352432966232\n",
      "epoch: 3, batch: 299, loss: 0.03562995791435242\n",
      "epoch: 3, batch: 300, loss: 0.027686968445777893\n",
      "epoch: 3, batch: 301, loss: 0.027384232729673386\n",
      "epoch: 3, batch: 302, loss: 0.015267438255250454\n",
      "epoch: 3, batch: 303, loss: 0.05690181627869606\n",
      "epoch: 3, batch: 304, loss: 0.05729589983820915\n",
      "epoch: 3, batch: 305, loss: 0.018674105405807495\n",
      "epoch: 3, batch: 306, loss: 0.0598168820142746\n",
      "epoch: 3, batch: 307, loss: 0.04271100461483002\n",
      "epoch: 3, batch: 308, loss: 0.01756564900279045\n",
      "epoch: 3, batch: 309, loss: 0.05045842379331589\n",
      "epoch: 3, batch: 310, loss: 0.03925997391343117\n",
      "epoch: 3, batch: 311, loss: 0.16364198923110962\n",
      "epoch: 3, batch: 312, loss: 0.05048121511936188\n",
      "epoch: 3, batch: 313, loss: 0.0557856410741806\n",
      "epoch: 3, batch: 314, loss: 0.1322038173675537\n",
      "epoch: 3, batch: 315, loss: 0.03178387135267258\n",
      "epoch: 3, batch: 316, loss: 0.06158999353647232\n",
      "epoch: 3, batch: 317, loss: 0.011074034497141838\n",
      "epoch: 3, batch: 318, loss: 0.04421108961105347\n",
      "epoch: 3, batch: 319, loss: 0.06755328178405762\n",
      "epoch: 3, batch: 320, loss: 0.00956549309194088\n",
      "epoch: 3, batch: 321, loss: 0.05356176570057869\n",
      "epoch: 3, batch: 322, loss: 0.15101727843284607\n",
      "epoch: 3, batch: 323, loss: 0.02208521217107773\n",
      "epoch: 3, batch: 324, loss: 0.10083002597093582\n",
      "epoch: 3, batch: 325, loss: 0.18481233716011047\n",
      "epoch: 3, batch: 326, loss: 0.016525782644748688\n",
      "epoch: 3, batch: 327, loss: 0.14663229882717133\n",
      "epoch: 3, batch: 328, loss: 0.08275496959686279\n",
      "epoch: 3, batch: 329, loss: 0.028907591477036476\n",
      "epoch: 3, batch: 330, loss: 0.05950792878866196\n",
      "epoch: 3, batch: 331, loss: 0.07904829829931259\n",
      "epoch: 3, batch: 332, loss: 0.055827051401138306\n",
      "epoch: 3, batch: 333, loss: 0.049553364515304565\n",
      "epoch: 3, batch: 334, loss: 0.1153535544872284\n",
      "epoch: 3, batch: 335, loss: 0.006886578164994717\n",
      "epoch: 3, batch: 336, loss: 0.09489735960960388\n",
      "epoch: 3, batch: 337, loss: 0.02358883060514927\n",
      "epoch: 3, batch: 338, loss: 0.013954824768006802\n",
      "epoch: 3, batch: 339, loss: 0.0450383797287941\n",
      "epoch: 3, batch: 340, loss: 0.036032747477293015\n",
      "epoch: 3, batch: 341, loss: 0.04556533321738243\n",
      "epoch: 3, batch: 342, loss: 0.030512744560837746\n",
      "epoch: 3, batch: 343, loss: 0.0647764652967453\n",
      "epoch: 3, batch: 344, loss: 0.07221139967441559\n",
      "epoch: 3, batch: 345, loss: 0.06717930734157562\n",
      "epoch: 3, batch: 346, loss: 0.019100625067949295\n",
      "epoch: 3, batch: 347, loss: 0.024579769000411034\n",
      "epoch: 3, batch: 348, loss: 0.07345715165138245\n",
      "epoch: 3, batch: 349, loss: 0.10727697610855103\n",
      "epoch: 3, batch: 350, loss: 0.07034342736005783\n",
      "epoch: 3, batch: 351, loss: 0.04118943214416504\n",
      "epoch: 3, batch: 352, loss: 0.03683176636695862\n",
      "epoch: 3, batch: 353, loss: 0.0330781415104866\n",
      "epoch: 3, batch: 354, loss: 0.007178854197263718\n",
      "epoch: 3, batch: 355, loss: 0.022442184388637543\n",
      "epoch: 3, batch: 356, loss: 0.10337670147418976\n",
      "epoch: 3, batch: 357, loss: 0.08894436061382294\n",
      "epoch: 3, batch: 358, loss: 0.15673696994781494\n",
      "epoch: 3, batch: 359, loss: 0.011269520036876202\n",
      "epoch: 3, batch: 360, loss: 0.08987189829349518\n",
      "epoch: 3, batch: 361, loss: 0.03130990266799927\n",
      "epoch: 3, batch: 362, loss: 0.03278275951743126\n",
      "epoch: 3, batch: 363, loss: 0.052110910415649414\n",
      "epoch: 3, batch: 364, loss: 0.0454096719622612\n",
      "epoch: 3, batch: 365, loss: 0.14209677278995514\n",
      "epoch: 3, batch: 366, loss: 0.012401805259287357\n",
      "epoch: 3, batch: 367, loss: 0.046555280685424805\n",
      "epoch: 3, batch: 368, loss: 0.008000030182301998\n",
      "epoch: 3, batch: 369, loss: 0.027488762512803078\n",
      "epoch: 3, batch: 370, loss: 0.013460428453981876\n",
      "epoch: 3, batch: 371, loss: 0.07281442731618881\n",
      "epoch: 3, batch: 372, loss: 0.08502184599637985\n",
      "epoch: 3, batch: 373, loss: 0.1034688651561737\n",
      "epoch: 3, batch: 374, loss: 0.023611104115843773\n",
      "epoch: 3, batch: 375, loss: 0.0246169101446867\n",
      "epoch: 3, batch: 376, loss: 0.007910327054560184\n",
      "epoch: 3, batch: 377, loss: 0.19867244362831116\n",
      "epoch: 3, batch: 378, loss: 0.1054946780204773\n",
      "epoch: 3, batch: 379, loss: 0.08144383877515793\n",
      "epoch: 3, batch: 380, loss: 0.05934979394078255\n",
      "epoch: 3, batch: 381, loss: 0.0928434357047081\n",
      "epoch: 3, batch: 382, loss: 0.05304551497101784\n",
      "epoch: 3, batch: 383, loss: 0.051852088421583176\n",
      "epoch: 3, batch: 384, loss: 0.08900798112154007\n",
      "epoch: 3, batch: 385, loss: 0.02169189229607582\n",
      "epoch: 3, batch: 386, loss: 0.02282349206507206\n",
      "epoch: 3, batch: 387, loss: 0.06060260534286499\n",
      "epoch: 3, batch: 388, loss: 0.06853524595499039\n",
      "epoch: 3, batch: 389, loss: 0.20033328235149384\n",
      "epoch: 3, batch: 390, loss: 0.04711337760090828\n",
      "epoch: 3, batch: 391, loss: 0.02202879637479782\n",
      "epoch: 3, batch: 392, loss: 0.04456557705998421\n",
      "epoch: 3, batch: 393, loss: 0.023401131853461266\n",
      "epoch: 3, batch: 394, loss: 0.02959873341023922\n",
      "epoch: 3, batch: 395, loss: 0.01560522336512804\n",
      "epoch: 3, batch: 396, loss: 0.02091239020228386\n",
      "epoch: 3, batch: 397, loss: 0.06877900660037994\n",
      "epoch: 3, batch: 398, loss: 0.11434253305196762\n",
      "epoch: 3, batch: 399, loss: 0.05282285809516907\n",
      "epoch: 3, batch: 400, loss: 0.0240410715341568\n",
      "epoch: 3, batch: 401, loss: 0.14477945864200592\n",
      "epoch: 3, batch: 402, loss: 0.04245295375585556\n",
      "epoch: 3, batch: 403, loss: 0.14740592241287231\n",
      "epoch: 3, batch: 404, loss: 0.041997943073511124\n",
      "epoch: 3, batch: 405, loss: 0.014632600359618664\n",
      "epoch: 3, batch: 406, loss: 0.009343612007796764\n",
      "epoch: 3, batch: 407, loss: 0.11787033081054688\n",
      "epoch: 3, batch: 408, loss: 0.0587104931473732\n",
      "epoch: 3, batch: 409, loss: 0.01048610731959343\n",
      "epoch: 3, batch: 410, loss: 0.08021552115678787\n",
      "epoch: 3, batch: 411, loss: 0.04858796298503876\n",
      "epoch: 3, batch: 412, loss: 0.07137241959571838\n",
      "epoch: 3, batch: 413, loss: 0.04710577055811882\n",
      "epoch: 3, batch: 414, loss: 0.05638761818408966\n",
      "epoch: 3, batch: 415, loss: 0.01610647514462471\n",
      "epoch: 3, batch: 416, loss: 0.019424743950366974\n",
      "epoch: 3, batch: 417, loss: 0.046537816524505615\n",
      "epoch: 3, batch: 418, loss: 0.06404764205217361\n",
      "epoch: 3, batch: 419, loss: 0.04712371900677681\n",
      "epoch: 3, batch: 420, loss: 0.01339116320014\n",
      "epoch: 3, batch: 421, loss: 0.04313359409570694\n",
      "epoch: 3, batch: 422, loss: 0.03650974482297897\n",
      "epoch: 3, batch: 423, loss: 0.022264514118433\n",
      "epoch: 3, batch: 424, loss: 0.01465437188744545\n",
      "epoch: 3, batch: 425, loss: 0.05813857540488243\n",
      "epoch: 3, batch: 426, loss: 0.004761673975735903\n",
      "epoch: 3, batch: 427, loss: 0.31214532256126404\n",
      "epoch: 3, batch: 428, loss: 0.06328058987855911\n",
      "epoch: 3, batch: 429, loss: 0.03632837161421776\n",
      "epoch: 3, batch: 430, loss: 0.031113794073462486\n",
      "epoch: 3, batch: 431, loss: 0.14520306885242462\n",
      "epoch: 3, batch: 432, loss: 0.10830527544021606\n",
      "epoch: 3, batch: 433, loss: 0.016771314665675163\n",
      "epoch: 3, batch: 434, loss: 0.1028258353471756\n",
      "epoch: 3, batch: 435, loss: 0.006058381870388985\n",
      "epoch: 3, batch: 436, loss: 0.06420734524726868\n",
      "epoch: 3, batch: 437, loss: 0.030300553888082504\n",
      "epoch: 3, batch: 438, loss: 0.07953464984893799\n",
      "epoch: 3, batch: 439, loss: 0.07506711781024933\n",
      "epoch: 3, batch: 440, loss: 0.06790945678949356\n",
      "epoch: 3, batch: 441, loss: 0.01474705245345831\n",
      "epoch: 3, batch: 442, loss: 0.009869570843875408\n",
      "epoch: 3, batch: 443, loss: 0.05761946365237236\n",
      "epoch: 3, batch: 444, loss: 0.11098309606313705\n",
      "epoch: 3, batch: 445, loss: 0.023864256218075752\n",
      "epoch: 3, batch: 446, loss: 0.07634839415550232\n",
      "epoch: 3, batch: 447, loss: 0.007193375378847122\n",
      "epoch: 3, batch: 448, loss: 0.02431143820285797\n",
      "epoch: 3, batch: 449, loss: 0.07624886184930801\n",
      "epoch: 3, batch: 450, loss: 0.0419018380343914\n",
      "epoch: 3, batch: 451, loss: 0.043292611837387085\n",
      "epoch: 3, batch: 452, loss: 0.2332211285829544\n",
      "epoch: 3, batch: 453, loss: 0.025433598086237907\n",
      "epoch: 3, batch: 454, loss: 0.10890518873929977\n",
      "epoch: 3, batch: 455, loss: 0.4314488470554352\n",
      "epoch: 3, batch: 456, loss: 0.07878735661506653\n",
      "epoch: 3, batch: 457, loss: 0.11500359326601028\n",
      "epoch: 3, batch: 458, loss: 0.015040670521557331\n",
      "epoch: 3, batch: 459, loss: 0.01226920634508133\n",
      "epoch: 3, batch: 460, loss: 0.0482279509305954\n",
      "epoch: 3, batch: 461, loss: 0.01720072329044342\n",
      "epoch: 3, batch: 462, loss: 0.08073259145021439\n",
      "epoch: 3, batch: 463, loss: 0.061261437833309174\n",
      "epoch: 3, batch: 464, loss: 0.06480097025632858\n",
      "epoch: 3, batch: 465, loss: 0.011495078913867474\n",
      "epoch: 3, batch: 466, loss: 0.0810953825712204\n",
      "epoch: 3, batch: 467, loss: 0.1494697779417038\n",
      "epoch: 3, batch: 468, loss: 0.02096191793680191\n",
      "epoch: 3, batch: 469, loss: 0.13722895085811615\n",
      "epoch: 3, batch: 470, loss: 0.016979699954390526\n",
      "epoch: 3, batch: 471, loss: 0.054156649857759476\n",
      "epoch: 3, batch: 472, loss: 0.028908390551805496\n",
      "epoch: 3, batch: 473, loss: 0.04496435075998306\n",
      "epoch: 3, batch: 474, loss: 0.007757175713777542\n",
      "epoch: 3, batch: 475, loss: 0.09876880794763565\n",
      "epoch: 3, batch: 476, loss: 0.13597814738750458\n",
      "epoch: 3, batch: 477, loss: 0.01465165801346302\n",
      "epoch: 3, batch: 478, loss: 0.06545962393283844\n",
      "epoch: 3, batch: 479, loss: 0.02518661879003048\n",
      "epoch: 3, batch: 480, loss: 0.10359887778759003\n",
      "epoch: 3, batch: 481, loss: 0.17267827689647675\n",
      "epoch: 3, batch: 482, loss: 0.005673447623848915\n",
      "epoch: 3, batch: 483, loss: 0.024328462779521942\n",
      "epoch: 3, batch: 484, loss: 0.1466815322637558\n",
      "epoch: 3, batch: 485, loss: 0.042930614203214645\n",
      "epoch: 3, batch: 486, loss: 0.032109037041664124\n",
      "epoch: 3, batch: 487, loss: 0.05203215032815933\n",
      "epoch: 3, batch: 488, loss: 0.021374836564064026\n",
      "epoch: 3, batch: 489, loss: 0.12670348584651947\n",
      "epoch: 3, batch: 490, loss: 0.032329607754945755\n",
      "epoch: 3, batch: 491, loss: 0.14003221690654755\n",
      "epoch: 3, batch: 492, loss: 0.10840155184268951\n",
      "epoch: 3, batch: 493, loss: 0.12659306824207306\n",
      "epoch: 3, batch: 494, loss: 0.04633470997214317\n",
      "epoch: 3, batch: 495, loss: 0.005600809119641781\n",
      "epoch: 3, batch: 496, loss: 0.033452488481998444\n",
      "epoch: 3, batch: 497, loss: 0.03864225000143051\n",
      "epoch: 3, batch: 498, loss: 0.06463390588760376\n",
      "epoch: 3, batch: 499, loss: 0.04921192303299904\n",
      "epoch: 3, batch: 500, loss: 0.04528777301311493\n",
      "epoch: 3, batch: 501, loss: 0.09335529059171677\n",
      "epoch: 3, batch: 502, loss: 0.08446669578552246\n",
      "epoch: 3, batch: 503, loss: 0.059775542467832565\n",
      "epoch: 3, batch: 504, loss: 0.022155174985527992\n",
      "epoch: 3, batch: 505, loss: 0.02260773442685604\n",
      "epoch: 3, batch: 506, loss: 0.06749073415994644\n",
      "epoch: 3, batch: 507, loss: 0.090547114610672\n",
      "epoch: 3, batch: 508, loss: 0.07156575471162796\n",
      "epoch: 3, batch: 509, loss: 0.08885911107063293\n",
      "epoch: 3, batch: 510, loss: 0.09380628913640976\n",
      "epoch: 3, batch: 511, loss: 0.034480076283216476\n",
      "epoch: 3, batch: 512, loss: 0.03241311386227608\n",
      "epoch: 3, batch: 513, loss: 0.13210123777389526\n",
      "epoch: 3, batch: 514, loss: 0.040469661355018616\n",
      "epoch: 3, batch: 515, loss: 0.03481157496571541\n",
      "epoch: 3, batch: 516, loss: 0.004976864438503981\n",
      "epoch: 3, batch: 517, loss: 0.04676879569888115\n",
      "epoch: 3, batch: 518, loss: 0.015952983871102333\n",
      "epoch: 3, batch: 519, loss: 0.05237068608403206\n",
      "epoch: 3, batch: 520, loss: 0.012845901772379875\n",
      "epoch: 3, batch: 521, loss: 0.04073645919561386\n",
      "epoch: 3, batch: 522, loss: 0.09604832530021667\n",
      "epoch: 3, batch: 523, loss: 0.07075899094343185\n",
      "epoch: 3, batch: 524, loss: 0.11572589725255966\n",
      "epoch: 3, batch: 525, loss: 0.009081119671463966\n",
      "epoch: 3, batch: 526, loss: 0.021800968796014786\n",
      "epoch: 3, batch: 527, loss: 0.031455498188734055\n",
      "epoch: 3, batch: 528, loss: 0.10077786445617676\n",
      "epoch: 3, batch: 529, loss: 0.04964296519756317\n",
      "epoch: 3, batch: 530, loss: 0.019059475511312485\n",
      "epoch: 3, batch: 531, loss: 0.07756383717060089\n",
      "epoch: 3, batch: 532, loss: 0.03252892568707466\n",
      "epoch: 3, batch: 533, loss: 0.25867295265197754\n",
      "epoch: 3, batch: 534, loss: 0.03015814907848835\n",
      "epoch: 3, batch: 535, loss: 0.06098354980349541\n",
      "epoch: 3, batch: 536, loss: 0.020318927243351936\n",
      "epoch: 3, batch: 537, loss: 0.1476821005344391\n",
      "epoch: 3, batch: 538, loss: 0.019770173355937004\n",
      "epoch: 3, batch: 539, loss: 0.019717665389180183\n",
      "epoch: 3, batch: 540, loss: 0.11959084868431091\n",
      "epoch: 3, batch: 541, loss: 0.018185710534453392\n",
      "epoch: 3, batch: 542, loss: 0.12594415247440338\n",
      "epoch: 3, batch: 543, loss: 0.056322209537029266\n",
      "epoch: 3, batch: 544, loss: 0.14112229645252228\n",
      "epoch: 3, batch: 545, loss: 0.029083305969834328\n",
      "epoch: 3, batch: 546, loss: 0.037496086210012436\n",
      "epoch: 3, batch: 547, loss: 0.1561363935470581\n",
      "epoch: 3, batch: 548, loss: 0.14596791565418243\n",
      "epoch: 3, batch: 549, loss: 0.026586376130580902\n",
      "epoch: 3, batch: 550, loss: 0.03414961323142052\n",
      "epoch: 3, batch: 551, loss: 0.021019846200942993\n",
      "epoch: 3, batch: 552, loss: 0.06437983363866806\n",
      "epoch: 3, batch: 553, loss: 0.0778101235628128\n",
      "epoch: 3, batch: 554, loss: 0.0423172228038311\n",
      "epoch: 3, batch: 555, loss: 0.041899170726537704\n",
      "epoch: 3, batch: 556, loss: 0.03212445229291916\n",
      "epoch: 3, batch: 557, loss: 0.10802680999040604\n",
      "epoch: 3, batch: 558, loss: 0.12995696067810059\n",
      "epoch: 3, batch: 559, loss: 0.10635685920715332\n",
      "epoch: 3, batch: 560, loss: 0.03742092475295067\n",
      "epoch: 3, batch: 561, loss: 0.016607843339443207\n",
      "epoch: 3, batch: 562, loss: 0.04952605441212654\n",
      "epoch: 3, batch: 563, loss: 0.04051654413342476\n",
      "epoch: 3, batch: 564, loss: 0.10508506000041962\n",
      "epoch: 3, batch: 565, loss: 0.02715388685464859\n",
      "epoch: 3, batch: 566, loss: 0.06352479010820389\n",
      "epoch: 3, batch: 567, loss: 0.016682907938957214\n",
      "epoch: 3, batch: 568, loss: 0.16912581026554108\n",
      "epoch: 3, batch: 569, loss: 0.02513524703681469\n",
      "epoch: 3, batch: 570, loss: 0.04442894458770752\n",
      "epoch: 3, batch: 571, loss: 0.0705326572060585\n",
      "epoch: 3, batch: 572, loss: 0.11098423600196838\n",
      "epoch: 3, batch: 573, loss: 0.05381105840206146\n",
      "epoch: 3, batch: 574, loss: 0.1262785643339157\n",
      "epoch: 3, batch: 575, loss: 0.021116292104125023\n",
      "epoch: 3, batch: 576, loss: 0.030821071937680244\n",
      "epoch: 3, batch: 577, loss: 0.031903769820928574\n",
      "epoch: 3, batch: 578, loss: 0.06255783140659332\n",
      "epoch: 3, batch: 579, loss: 0.0840579941868782\n",
      "epoch: 3, batch: 580, loss: 0.011692465282976627\n",
      "epoch: 3, batch: 581, loss: 0.10336263477802277\n",
      "epoch: 3, batch: 582, loss: 0.031252410262823105\n",
      "epoch: 3, batch: 583, loss: 0.04387430101633072\n",
      "epoch: 3, batch: 584, loss: 0.11108425259590149\n",
      "epoch: 3, batch: 585, loss: 0.05008798465132713\n",
      "epoch: 3, batch: 586, loss: 0.055211205035448074\n",
      "epoch: 3, batch: 587, loss: 0.05000200495123863\n",
      "epoch: 3, batch: 588, loss: 0.10591328144073486\n",
      "epoch: 3, batch: 589, loss: 0.05642995610833168\n",
      "epoch: 3, batch: 590, loss: 0.14881442487239838\n",
      "epoch: 3, batch: 591, loss: 0.019485535100102425\n",
      "epoch: 3, batch: 592, loss: 0.08519326150417328\n",
      "epoch: 3, batch: 593, loss: 0.07898710668087006\n",
      "epoch: 3, batch: 594, loss: 0.012510037049651146\n",
      "epoch: 3, batch: 595, loss: 0.04587211459875107\n",
      "epoch: 3, batch: 596, loss: 0.025402789935469627\n",
      "epoch: 3, batch: 597, loss: 0.0883553996682167\n",
      "epoch: 3, batch: 598, loss: 0.0557563379406929\n",
      "epoch: 3, batch: 599, loss: 0.06567753106355667\n",
      "epoch: 3, batch: 600, loss: 0.04049549251794815\n",
      "epoch: 3, batch: 601, loss: 0.10079575330018997\n",
      "epoch: 3, batch: 602, loss: 0.09058801084756851\n",
      "epoch: 3, batch: 603, loss: 0.006945211440324783\n",
      "epoch: 3, batch: 604, loss: 0.29797837138175964\n",
      "epoch: 3, batch: 605, loss: 0.03457464650273323\n",
      "epoch: 3, batch: 606, loss: 0.050867002457380295\n",
      "epoch: 3, batch: 607, loss: 0.01616601087152958\n",
      "epoch: 3, batch: 608, loss: 0.08735109865665436\n",
      "epoch: 3, batch: 609, loss: 0.012751988135278225\n",
      "epoch: 3, batch: 610, loss: 0.03544986993074417\n",
      "epoch: 3, batch: 611, loss: 0.06927048414945602\n",
      "epoch: 3, batch: 612, loss: 0.1328069269657135\n",
      "epoch: 3, batch: 613, loss: 0.038842394948005676\n",
      "epoch: 3, batch: 614, loss: 0.024027811363339424\n",
      "epoch: 3, batch: 615, loss: 0.08767250180244446\n",
      "epoch: 3, batch: 616, loss: 0.14753122627735138\n",
      "epoch: 3, batch: 617, loss: 0.052137624472379684\n",
      "epoch: 3, batch: 618, loss: 0.20190644264221191\n",
      "epoch: 3, batch: 619, loss: 0.05035735294222832\n",
      "epoch: 3, batch: 620, loss: 0.020028578117489815\n",
      "epoch: 3, batch: 621, loss: 0.0646350234746933\n",
      "epoch: 3, batch: 622, loss: 0.02998005785048008\n",
      "epoch: 3, batch: 623, loss: 0.018865671008825302\n",
      "epoch: 3, batch: 624, loss: 0.008318071253597736\n",
      "epoch: 3, batch: 625, loss: 0.037032462656497955\n",
      "epoch: 3, batch: 626, loss: 0.049220435321331024\n",
      "epoch: 3, batch: 627, loss: 0.05664167180657387\n",
      "epoch: 3, batch: 628, loss: 0.040937021374702454\n",
      "epoch: 3, batch: 629, loss: 0.05635203421115875\n",
      "epoch: 3, batch: 630, loss: 0.010614299215376377\n",
      "epoch: 3, batch: 631, loss: 0.1006924957036972\n",
      "epoch: 3, batch: 632, loss: 0.015907030552625656\n",
      "epoch: 3, batch: 633, loss: 0.03601829335093498\n",
      "epoch: 3, batch: 634, loss: 0.02263985574245453\n",
      "epoch: 3, batch: 635, loss: 0.09676048904657364\n",
      "epoch: 3, batch: 636, loss: 0.03684794157743454\n",
      "epoch: 3, batch: 637, loss: 0.011359007097780704\n",
      "epoch: 3, batch: 638, loss: 0.017330044880509377\n",
      "epoch: 3, batch: 639, loss: 0.011059879325330257\n",
      "epoch: 3, batch: 640, loss: 0.16099461913108826\n",
      "epoch: 3, batch: 641, loss: 0.013797905296087265\n",
      "epoch: 3, batch: 642, loss: 0.013743428513407707\n",
      "epoch: 3, batch: 643, loss: 0.05404578521847725\n",
      "epoch: 3, batch: 644, loss: 0.023221539333462715\n",
      "epoch: 3, batch: 645, loss: 0.0763334184885025\n",
      "epoch: 3, batch: 646, loss: 0.00942191667854786\n",
      "epoch: 3, batch: 647, loss: 0.02620300091803074\n",
      "epoch: 3, batch: 648, loss: 0.08944408595561981\n",
      "epoch: 3, batch: 649, loss: 0.03301400691270828\n",
      "epoch: 3, batch: 650, loss: 0.030008455738425255\n",
      "epoch: 3, batch: 651, loss: 0.029359782114624977\n",
      "epoch: 3, batch: 652, loss: 0.07797694206237793\n",
      "epoch: 3, batch: 653, loss: 0.057155054062604904\n",
      "epoch: 3, batch: 654, loss: 0.0792195051908493\n",
      "epoch: 3, batch: 655, loss: 0.0335669182240963\n",
      "epoch: 3, batch: 656, loss: 0.00717275682836771\n",
      "epoch: 3, batch: 657, loss: 0.11236707121133804\n",
      "epoch: 3, batch: 658, loss: 0.09481484442949295\n",
      "epoch: 3, batch: 659, loss: 0.009680008515715599\n",
      "epoch: 3, batch: 660, loss: 0.024383336305618286\n",
      "epoch: 3, batch: 661, loss: 0.04225428029894829\n",
      "epoch: 3, batch: 662, loss: 0.010396696627140045\n",
      "epoch: 3, batch: 663, loss: 0.01900755614042282\n",
      "epoch: 3, batch: 664, loss: 0.0287021454423666\n",
      "epoch: 3, batch: 665, loss: 0.21447746455669403\n",
      "epoch: 3, batch: 666, loss: 0.1538514494895935\n",
      "epoch: 3, batch: 667, loss: 0.06247853487730026\n",
      "epoch: 3, batch: 668, loss: 0.061670150607824326\n",
      "epoch: 3, batch: 669, loss: 0.08821862936019897\n",
      "epoch: 3, batch: 670, loss: 0.012762228958308697\n",
      "epoch: 3, batch: 671, loss: 0.02121857926249504\n",
      "epoch: 3, batch: 672, loss: 0.02054925635457039\n",
      "epoch: 3, batch: 673, loss: 0.03281061723828316\n",
      "epoch: 3, batch: 674, loss: 0.10180673003196716\n",
      "epoch: 3, batch: 675, loss: 0.059517282992601395\n",
      "epoch: 3, batch: 676, loss: 0.06074506416916847\n",
      "epoch: 3, batch: 677, loss: 0.16398309171199799\n",
      "epoch: 3, batch: 678, loss: 0.03631354868412018\n",
      "epoch: 3, batch: 679, loss: 0.1396556943655014\n",
      "epoch: 3, batch: 680, loss: 0.034342117607593536\n",
      "epoch: 3, batch: 681, loss: 0.026465309783816338\n",
      "epoch: 3, batch: 682, loss: 0.03868101164698601\n",
      "epoch: 3, batch: 683, loss: 0.023284677416086197\n",
      "epoch: 3, batch: 684, loss: 0.04570399597287178\n",
      "epoch: 3, batch: 685, loss: 0.0076248482801020145\n",
      "epoch: 3, batch: 686, loss: 0.013378613628447056\n",
      "epoch: 3, batch: 687, loss: 0.008651534095406532\n",
      "epoch: 3, batch: 688, loss: 0.01941862516105175\n",
      "epoch: 3, batch: 689, loss: 0.12998418509960175\n",
      "epoch: 3, batch: 690, loss: 0.06598974019289017\n",
      "epoch: 3, batch: 691, loss: 0.05602547153830528\n",
      "epoch: 3, batch: 692, loss: 0.008708642795681953\n",
      "epoch: 3, batch: 693, loss: 0.045435063540935516\n",
      "epoch: 3, batch: 694, loss: 0.015267661772668362\n",
      "epoch: 3, batch: 695, loss: 0.052147027105093\n",
      "epoch: 3, batch: 696, loss: 0.046130139380693436\n",
      "epoch: 3, batch: 697, loss: 0.05372139811515808\n",
      "epoch: 3, batch: 698, loss: 0.028714695945382118\n",
      "epoch: 3, batch: 699, loss: 0.007757920306175947\n",
      "epoch: 3, batch: 700, loss: 0.026166731491684914\n",
      "epoch: 3, batch: 701, loss: 0.12923875451087952\n",
      "epoch: 3, batch: 702, loss: 0.03287700191140175\n",
      "epoch: 3, batch: 703, loss: 0.11491475999355316\n",
      "epoch: 3, batch: 704, loss: 0.04628961905837059\n",
      "epoch: 3, batch: 705, loss: 0.013209638185799122\n",
      "epoch: 3, batch: 706, loss: 0.02197374403476715\n",
      "epoch: 3, batch: 707, loss: 0.011372706852853298\n",
      "epoch: 3, batch: 708, loss: 0.07793131470680237\n",
      "epoch: 3, batch: 709, loss: 0.03248847275972366\n",
      "epoch: 3, batch: 710, loss: 0.14278116822242737\n",
      "epoch: 3, batch: 711, loss: 0.18831969797611237\n",
      "epoch: 3, batch: 712, loss: 0.10466935485601425\n",
      "epoch: 3, batch: 713, loss: 0.10512338578701019\n",
      "epoch: 3, batch: 714, loss: 0.057608526200056076\n",
      "epoch: 3, batch: 715, loss: 0.05810168758034706\n",
      "epoch: 3, batch: 716, loss: 0.01769469678401947\n",
      "epoch: 3, batch: 717, loss: 0.08663389086723328\n",
      "epoch: 3, batch: 718, loss: 0.06002027168869972\n",
      "epoch: 3, batch: 719, loss: 0.06580443680286407\n",
      "epoch: 3, batch: 720, loss: 0.023489272221922874\n",
      "epoch: 3, batch: 721, loss: 0.015016027726233006\n",
      "epoch: 3, batch: 722, loss: 0.179036483168602\n",
      "epoch: 3, batch: 723, loss: 0.01392038818448782\n",
      "epoch: 3, batch: 724, loss: 0.06696860492229462\n",
      "epoch: 3, batch: 725, loss: 0.025087682530283928\n",
      "epoch: 3, batch: 726, loss: 0.020824890583753586\n",
      "epoch: 3, batch: 727, loss: 0.04460831731557846\n",
      "epoch: 3, batch: 728, loss: 0.0715135857462883\n",
      "epoch: 3, batch: 729, loss: 0.009972786530852318\n",
      "epoch: 3, batch: 730, loss: 0.052285801619291306\n",
      "epoch: 3, batch: 731, loss: 0.010325418785214424\n",
      "epoch: 3, batch: 732, loss: 0.03435312211513519\n",
      "epoch: 3, batch: 733, loss: 0.04414417967200279\n",
      "epoch: 3, batch: 734, loss: 0.091822549700737\n",
      "epoch: 3, batch: 735, loss: 0.07133756577968597\n",
      "epoch: 3, batch: 736, loss: 0.03643479570746422\n",
      "epoch: 3, batch: 737, loss: 0.06524328142404556\n",
      "epoch: 3, batch: 738, loss: 0.04076647385954857\n",
      "epoch: 3, batch: 739, loss: 0.1437031626701355\n",
      "epoch: 3, batch: 740, loss: 0.05445539578795433\n",
      "epoch: 3, batch: 741, loss: 0.018002092838287354\n",
      "epoch: 3, batch: 742, loss: 0.013399354182183743\n",
      "epoch: 3, batch: 743, loss: 0.10120468586683273\n",
      "epoch: 3, batch: 744, loss: 0.006540034897625446\n",
      "epoch: 3, batch: 745, loss: 0.0713488981127739\n",
      "epoch: 3, batch: 746, loss: 0.023795699700713158\n",
      "epoch: 3, batch: 747, loss: 0.02898367866873741\n",
      "epoch: 3, batch: 748, loss: 0.030409391969442368\n",
      "epoch: 3, batch: 749, loss: 0.04016546159982681\n",
      "epoch: 3, batch: 750, loss: 0.03551897034049034\n",
      "epoch: 3, batch: 751, loss: 0.10371904820203781\n",
      "epoch: 3, batch: 752, loss: 0.13185973465442657\n",
      "epoch: 3, batch: 753, loss: 0.08575711399316788\n",
      "epoch: 3, batch: 754, loss: 0.004459942225366831\n",
      "epoch: 3, batch: 755, loss: 0.023457100614905357\n",
      "epoch: 3, batch: 756, loss: 0.007808668073266745\n",
      "epoch: 3, batch: 757, loss: 0.03012177161872387\n",
      "epoch: 3, batch: 758, loss: 0.014278896152973175\n",
      "epoch: 3, batch: 759, loss: 0.031986404210329056\n",
      "epoch: 3, batch: 760, loss: 0.0112909572198987\n",
      "epoch: 3, batch: 761, loss: 0.12279751896858215\n",
      "epoch: 3, batch: 762, loss: 0.0917404517531395\n",
      "epoch: 3, batch: 763, loss: 0.006838956847786903\n",
      "epoch: 3, batch: 764, loss: 0.09141673147678375\n",
      "epoch: 3, batch: 765, loss: 0.0505954809486866\n",
      "epoch: 3, batch: 766, loss: 0.0346963033080101\n",
      "epoch: 3, batch: 767, loss: 0.18607929348945618\n",
      "epoch: 3, batch: 768, loss: 0.13144177198410034\n",
      "epoch: 3, batch: 769, loss: 0.03488016501069069\n",
      "epoch: 3, batch: 770, loss: 0.17334185540676117\n",
      "epoch: 3, batch: 771, loss: 0.09682934731245041\n",
      "epoch: 3, batch: 772, loss: 0.15758325159549713\n",
      "epoch: 3, batch: 773, loss: 0.09553218632936478\n",
      "epoch: 3, batch: 774, loss: 0.048651762306690216\n",
      "epoch: 3, batch: 775, loss: 0.058759815990924835\n",
      "epoch: 3, batch: 776, loss: 0.03589795529842377\n",
      "epoch: 3, batch: 777, loss: 0.028055403381586075\n",
      "epoch: 3, batch: 778, loss: 0.04325513169169426\n",
      "epoch: 3, batch: 779, loss: 0.02794506400823593\n",
      "epoch: 3, batch: 780, loss: 0.12089868634939194\n",
      "epoch: 3, batch: 781, loss: 0.011755989864468575\n",
      "epoch: 3, batch: 782, loss: 0.029204217717051506\n",
      "epoch: 3, batch: 783, loss: 0.06569426506757736\n",
      "epoch: 3, batch: 784, loss: 0.04534900188446045\n",
      "epoch: 3, batch: 785, loss: 0.10539491474628448\n",
      "epoch: 3, batch: 786, loss: 0.04498681798577309\n",
      "epoch: 3, batch: 787, loss: 0.013901947997510433\n",
      "epoch: 3, batch: 788, loss: 0.00676889019086957\n",
      "epoch: 3, batch: 789, loss: 0.04312719777226448\n",
      "epoch: 3, batch: 790, loss: 0.07687151432037354\n",
      "epoch: 3, batch: 791, loss: 0.07047748565673828\n",
      "epoch: 3, batch: 792, loss: 0.05204702168703079\n",
      "epoch: 3, batch: 793, loss: 0.0436602458357811\n",
      "epoch: 3, batch: 794, loss: 0.05564434081315994\n",
      "epoch: 3, batch: 795, loss: 0.05227584019303322\n",
      "epoch: 3, batch: 796, loss: 0.025645514950156212\n",
      "epoch: 3, batch: 797, loss: 0.030781306326389313\n",
      "epoch: 3, batch: 798, loss: 0.030758671462535858\n",
      "epoch: 3, batch: 799, loss: 0.03389681130647659\n",
      "epoch: 3, batch: 800, loss: 0.016482306644320488\n",
      "epoch: 3, batch: 801, loss: 0.04645265266299248\n",
      "epoch: 3, batch: 802, loss: 0.025552405044436455\n",
      "epoch: 3, batch: 803, loss: 0.08802714943885803\n",
      "epoch: 3, batch: 804, loss: 0.04795345291495323\n",
      "epoch: 3, batch: 805, loss: 0.08685308694839478\n",
      "epoch: 3, batch: 806, loss: 0.1308576911687851\n",
      "epoch: 3, batch: 807, loss: 0.06481987237930298\n",
      "epoch: 3, batch: 808, loss: 0.07763130217790604\n",
      "epoch: 3, batch: 809, loss: 0.062263429164886475\n",
      "epoch: 3, batch: 810, loss: 0.07323293387889862\n",
      "epoch: 3, batch: 811, loss: 0.04572560638189316\n",
      "epoch: 3, batch: 812, loss: 0.08164695650339127\n",
      "epoch: 3, batch: 813, loss: 0.014025077223777771\n",
      "epoch: 3, batch: 814, loss: 0.14319054782390594\n",
      "epoch: 3, batch: 815, loss: 0.21415632963180542\n",
      "epoch: 3, batch: 816, loss: 0.012669447809457779\n",
      "epoch: 3, batch: 817, loss: 0.029805675148963928\n",
      "epoch: 3, batch: 818, loss: 0.04584518074989319\n",
      "epoch: 3, batch: 819, loss: 0.004155534319579601\n",
      "epoch: 3, batch: 820, loss: 0.11935590952634811\n",
      "epoch: 3, batch: 821, loss: 0.027370909228920937\n",
      "epoch: 3, batch: 822, loss: 0.016987089067697525\n",
      "epoch: 3, batch: 823, loss: 0.04440226033329964\n",
      "epoch: 3, batch: 824, loss: 0.0932387262582779\n",
      "epoch: 3, batch: 825, loss: 0.0994466096162796\n",
      "epoch: 3, batch: 826, loss: 0.015068620443344116\n",
      "epoch: 3, batch: 827, loss: 0.03611449897289276\n",
      "epoch: 3, batch: 828, loss: 0.02658896893262863\n",
      "epoch: 3, batch: 829, loss: 0.03547157719731331\n",
      "epoch: 3, batch: 830, loss: 0.021229220554232597\n",
      "epoch: 3, batch: 831, loss: 0.0971335768699646\n",
      "epoch: 3, batch: 832, loss: 0.041386984288692474\n",
      "epoch: 3, batch: 833, loss: 0.1328711360692978\n",
      "epoch: 3, batch: 834, loss: 0.0365229956805706\n",
      "epoch: 3, batch: 835, loss: 0.028595056384801865\n",
      "epoch: 3, batch: 836, loss: 0.1615767925977707\n",
      "epoch: 3, batch: 837, loss: 0.014751086942851543\n",
      "epoch: 3, batch: 838, loss: 0.008590063080191612\n",
      "epoch: 3, batch: 839, loss: 0.09712423384189606\n",
      "epoch: 3, batch: 840, loss: 0.04825114086270332\n",
      "epoch: 3, batch: 841, loss: 0.06930351257324219\n",
      "epoch: 3, batch: 842, loss: 0.06545912474393845\n",
      "epoch: 3, batch: 843, loss: 0.08861955255270004\n",
      "epoch: 3, batch: 844, loss: 0.07335691899061203\n",
      "epoch: 3, batch: 845, loss: 0.027166230604052544\n",
      "epoch: 3, batch: 846, loss: 0.030263081192970276\n",
      "epoch: 3, batch: 847, loss: 0.027260448783636093\n",
      "epoch: 3, batch: 848, loss: 0.10050145536661148\n",
      "epoch: 3, batch: 849, loss: 0.021786890923976898\n",
      "epoch: 3, batch: 850, loss: 0.06735377758741379\n",
      "epoch: 3, batch: 851, loss: 0.03247153386473656\n",
      "epoch: 3, batch: 852, loss: 0.029582636430859566\n",
      "epoch: 3, batch: 853, loss: 0.018005425110459328\n",
      "epoch: 3, batch: 854, loss: 0.030681535601615906\n",
      "epoch: 3, batch: 855, loss: 0.024668121710419655\n",
      "epoch: 3, batch: 856, loss: 0.01353242713958025\n",
      "epoch: 3, batch: 857, loss: 0.03620818629860878\n",
      "epoch: 3, batch: 858, loss: 0.09388081729412079\n",
      "epoch: 3, batch: 859, loss: 0.08056533336639404\n",
      "epoch: 3, batch: 860, loss: 0.03902556374669075\n",
      "epoch: 3, batch: 861, loss: 0.014784946106374264\n",
      "epoch: 3, batch: 862, loss: 0.06800180673599243\n",
      "epoch: 3, batch: 863, loss: 0.051535725593566895\n",
      "epoch: 3, batch: 864, loss: 0.12889565527439117\n",
      "epoch: 3, batch: 865, loss: 0.09074966609477997\n",
      "epoch: 3, batch: 866, loss: 0.023951834067702293\n",
      "epoch: 3, batch: 867, loss: 0.16116110980510712\n",
      "epoch: 3, batch: 868, loss: 0.046384889632463455\n",
      "epoch: 3, batch: 869, loss: 0.0833234041929245\n",
      "epoch: 3, batch: 870, loss: 0.037358857691287994\n",
      "epoch: 3, batch: 871, loss: 0.08006858080625534\n",
      "epoch: 3, batch: 872, loss: 0.0216774120926857\n",
      "epoch: 3, batch: 873, loss: 0.0877658873796463\n",
      "epoch: 3, batch: 874, loss: 0.012988395988941193\n",
      "epoch: 3, batch: 875, loss: 0.038775816559791565\n",
      "epoch: 3, batch: 876, loss: 0.035352516919374466\n",
      "epoch: 3, batch: 877, loss: 0.010325562208890915\n",
      "epoch: 3, batch: 878, loss: 0.0572463795542717\n",
      "epoch: 3, batch: 879, loss: 0.06198932230472565\n",
      "epoch: 3, batch: 880, loss: 0.17515741288661957\n",
      "epoch: 3, batch: 881, loss: 0.0028458565939217806\n",
      "epoch: 3, batch: 882, loss: 0.0713881179690361\n",
      "epoch: 3, batch: 883, loss: 0.06348176300525665\n",
      "epoch: 3, batch: 884, loss: 0.10804460942745209\n",
      "epoch: 3, batch: 885, loss: 0.03834310173988342\n",
      "epoch: 3, batch: 886, loss: 0.17172037065029144\n",
      "epoch: 3, batch: 887, loss: 0.017096377909183502\n",
      "epoch: 3, batch: 888, loss: 0.07508845627307892\n",
      "epoch: 3, batch: 889, loss: 0.04397963732481003\n",
      "epoch: 3, batch: 890, loss: 0.04020846262574196\n",
      "epoch: 3, batch: 891, loss: 0.0481245256960392\n",
      "epoch: 3, batch: 892, loss: 0.023785943165421486\n",
      "epoch: 3, batch: 893, loss: 0.012278946116566658\n",
      "epoch: 3, batch: 894, loss: 0.02820468507707119\n",
      "epoch: 3, batch: 895, loss: 0.044054627418518066\n",
      "epoch: 3, batch: 896, loss: 0.10837497562170029\n",
      "epoch: 3, batch: 897, loss: 0.08023981750011444\n",
      "epoch: 3, batch: 898, loss: 0.07676196098327637\n",
      "epoch: 3, batch: 899, loss: 0.04480382055044174\n",
      "epoch: 3, batch: 900, loss: 0.12185088545084\n",
      "epoch: 3, batch: 901, loss: 0.13400761783123016\n",
      "epoch: 3, batch: 902, loss: 0.061600808054208755\n",
      "epoch: 3, batch: 903, loss: 0.006813794374465942\n",
      "epoch: 3, batch: 904, loss: 0.19401928782463074\n",
      "epoch: 3, batch: 905, loss: 0.010034061037003994\n",
      "epoch: 3, batch: 906, loss: 0.11140080541372299\n",
      "epoch: 3, batch: 907, loss: 0.13713183999061584\n",
      "epoch: 3, batch: 908, loss: 0.025865642353892326\n",
      "epoch: 3, batch: 909, loss: 0.07901427149772644\n",
      "epoch: 3, batch: 910, loss: 0.03784823417663574\n",
      "epoch: 3, batch: 911, loss: 0.06462044268846512\n",
      "epoch: 3, batch: 912, loss: 0.03306066617369652\n",
      "epoch: 3, batch: 913, loss: 0.05437694489955902\n",
      "epoch: 3, batch: 914, loss: 0.15543609857559204\n",
      "epoch: 3, batch: 915, loss: 0.14985260367393494\n",
      "epoch: 3, batch: 916, loss: 0.12032477557659149\n",
      "epoch: 3, batch: 917, loss: 0.027116648852825165\n",
      "epoch: 3, batch: 918, loss: 0.10415072739124298\n",
      "epoch: 3, batch: 919, loss: 0.05042228847742081\n",
      "epoch: 3, batch: 920, loss: 0.007118274457752705\n",
      "epoch: 3, batch: 921, loss: 0.027838552370667458\n",
      "epoch: 3, batch: 922, loss: 0.09530475735664368\n",
      "epoch: 3, batch: 923, loss: 0.07647305727005005\n",
      "epoch: 3, batch: 924, loss: 0.061240311712026596\n",
      "epoch: 3, batch: 925, loss: 0.03515046834945679\n",
      "epoch: 3, batch: 926, loss: 0.061185628175735474\n",
      "epoch: 3, batch: 927, loss: 0.042421188205480576\n",
      "epoch: 3, batch: 928, loss: 0.004928389098495245\n",
      "epoch: 3, batch: 929, loss: 0.06027723476290703\n",
      "epoch: 3, batch: 930, loss: 0.19621708989143372\n",
      "epoch: 3, batch: 931, loss: 0.10059193521738052\n",
      "epoch: 3, batch: 932, loss: 0.03687324374914169\n",
      "epoch: 3, batch: 933, loss: 0.0185556560754776\n",
      "epoch: 3, batch: 934, loss: 0.014767555519938469\n",
      "epoch: 3, batch: 935, loss: 0.06460477411746979\n",
      "epoch: 3, batch: 936, loss: 0.10153090208768845\n",
      "epoch: 3, batch: 937, loss: 0.021108604967594147\n",
      "epoch: 4, batch: 0, loss: 0.04431069642305374\n",
      "epoch: 4, batch: 1, loss: 0.10322932153940201\n",
      "epoch: 4, batch: 2, loss: 0.005834763869643211\n",
      "epoch: 4, batch: 3, loss: 0.012328493408858776\n",
      "epoch: 4, batch: 4, loss: 0.020048122853040695\n",
      "epoch: 4, batch: 5, loss: 0.01693638600409031\n",
      "epoch: 4, batch: 6, loss: 0.01891089417040348\n",
      "epoch: 4, batch: 7, loss: 0.035167351365089417\n",
      "epoch: 4, batch: 8, loss: 0.010539009235799313\n",
      "epoch: 4, batch: 9, loss: 0.006806300021708012\n",
      "epoch: 4, batch: 10, loss: 0.04468085244297981\n",
      "epoch: 4, batch: 11, loss: 0.06429515033960342\n",
      "epoch: 4, batch: 12, loss: 0.024717940017580986\n",
      "epoch: 4, batch: 13, loss: 0.02152300626039505\n",
      "epoch: 4, batch: 14, loss: 0.1342182159423828\n",
      "epoch: 4, batch: 15, loss: 0.035224854946136475\n",
      "epoch: 4, batch: 16, loss: 0.03263584524393082\n",
      "epoch: 4, batch: 17, loss: 0.09094497561454773\n",
      "epoch: 4, batch: 18, loss: 0.07275545597076416\n",
      "epoch: 4, batch: 19, loss: 0.012113415636122227\n",
      "epoch: 4, batch: 20, loss: 0.008535436354577541\n",
      "epoch: 4, batch: 21, loss: 0.10336069017648697\n",
      "epoch: 4, batch: 22, loss: 0.027506506070494652\n",
      "epoch: 4, batch: 23, loss: 0.1104465052485466\n",
      "epoch: 4, batch: 24, loss: 0.04018154740333557\n",
      "epoch: 4, batch: 25, loss: 0.047395817935466766\n",
      "epoch: 4, batch: 26, loss: 0.009846875444054604\n",
      "epoch: 4, batch: 27, loss: 0.037345778197050095\n",
      "epoch: 4, batch: 28, loss: 0.026259388774633408\n",
      "epoch: 4, batch: 29, loss: 0.010581102222204208\n",
      "epoch: 4, batch: 30, loss: 0.08071107417345047\n",
      "epoch: 4, batch: 31, loss: 0.0065482850186526775\n",
      "epoch: 4, batch: 32, loss: 0.030827613547444344\n",
      "epoch: 4, batch: 33, loss: 0.036659326404333115\n",
      "epoch: 4, batch: 34, loss: 0.044150032103061676\n",
      "epoch: 4, batch: 35, loss: 0.012265758588910103\n",
      "epoch: 4, batch: 36, loss: 0.03265286609530449\n",
      "epoch: 4, batch: 37, loss: 0.030697975307703018\n",
      "epoch: 4, batch: 38, loss: 0.025787997990846634\n",
      "epoch: 4, batch: 39, loss: 0.08466076850891113\n",
      "epoch: 4, batch: 40, loss: 0.046923521906137466\n",
      "epoch: 4, batch: 41, loss: 0.03467130288481712\n",
      "epoch: 4, batch: 42, loss: 0.010679561644792557\n",
      "epoch: 4, batch: 43, loss: 0.041436441242694855\n",
      "epoch: 4, batch: 44, loss: 0.11945335566997528\n",
      "epoch: 4, batch: 45, loss: 0.12033843994140625\n",
      "epoch: 4, batch: 46, loss: 0.006190531421452761\n",
      "epoch: 4, batch: 47, loss: 0.07682700455188751\n",
      "epoch: 4, batch: 48, loss: 0.023723047226667404\n",
      "epoch: 4, batch: 49, loss: 0.0251778457313776\n",
      "epoch: 4, batch: 50, loss: 0.05393950268626213\n",
      "epoch: 4, batch: 51, loss: 0.1178789883852005\n",
      "epoch: 4, batch: 52, loss: 0.02987266331911087\n",
      "epoch: 4, batch: 53, loss: 0.023437773808836937\n",
      "epoch: 4, batch: 54, loss: 0.015660326927900314\n",
      "epoch: 4, batch: 55, loss: 0.05250845476984978\n",
      "epoch: 4, batch: 56, loss: 0.011829412542283535\n",
      "epoch: 4, batch: 57, loss: 0.015432783402502537\n",
      "epoch: 4, batch: 58, loss: 0.07658851891756058\n",
      "epoch: 4, batch: 59, loss: 0.025615794584155083\n",
      "epoch: 4, batch: 60, loss: 0.015926653519272804\n",
      "epoch: 4, batch: 61, loss: 0.024719571694731712\n",
      "epoch: 4, batch: 62, loss: 0.04497489333152771\n",
      "epoch: 4, batch: 63, loss: 0.05143117159605026\n",
      "epoch: 4, batch: 64, loss: 0.08175890147686005\n",
      "epoch: 4, batch: 65, loss: 0.11886380612850189\n",
      "epoch: 4, batch: 66, loss: 0.04295148327946663\n",
      "epoch: 4, batch: 67, loss: 0.014428523369133472\n",
      "epoch: 4, batch: 68, loss: 0.01036092545837164\n",
      "epoch: 4, batch: 69, loss: 0.022842703387141228\n",
      "epoch: 4, batch: 70, loss: 0.013820724561810493\n",
      "epoch: 4, batch: 71, loss: 0.028539571911096573\n",
      "epoch: 4, batch: 72, loss: 0.09985107183456421\n",
      "epoch: 4, batch: 73, loss: 0.014632786624133587\n",
      "epoch: 4, batch: 74, loss: 0.042615342885255814\n",
      "epoch: 4, batch: 75, loss: 0.01569868065416813\n",
      "epoch: 4, batch: 76, loss: 0.08811822533607483\n",
      "epoch: 4, batch: 77, loss: 0.07261161506175995\n",
      "epoch: 4, batch: 78, loss: 0.012105001136660576\n",
      "epoch: 4, batch: 79, loss: 0.01407794002443552\n",
      "epoch: 4, batch: 80, loss: 0.08263740688562393\n",
      "epoch: 4, batch: 81, loss: 0.06340614706277847\n",
      "epoch: 4, batch: 82, loss: 0.1304275095462799\n",
      "epoch: 4, batch: 83, loss: 0.04972446709871292\n",
      "epoch: 4, batch: 84, loss: 0.014983275905251503\n",
      "epoch: 4, batch: 85, loss: 0.029210427775979042\n",
      "epoch: 4, batch: 86, loss: 0.13515864312648773\n",
      "epoch: 4, batch: 87, loss: 0.03969351947307587\n",
      "epoch: 4, batch: 88, loss: 0.024558834731578827\n",
      "epoch: 4, batch: 89, loss: 0.03556340932846069\n",
      "epoch: 4, batch: 90, loss: 0.025659652426838875\n",
      "epoch: 4, batch: 91, loss: 0.02466961182653904\n",
      "epoch: 4, batch: 92, loss: 0.05278366804122925\n",
      "epoch: 4, batch: 93, loss: 0.03260495141148567\n",
      "epoch: 4, batch: 94, loss: 0.01629115827381611\n",
      "epoch: 4, batch: 95, loss: 0.017206402495503426\n",
      "epoch: 4, batch: 96, loss: 0.04056459665298462\n",
      "epoch: 4, batch: 97, loss: 0.03339079022407532\n",
      "epoch: 4, batch: 98, loss: 0.040138546377420425\n",
      "epoch: 4, batch: 99, loss: 0.0787845253944397\n",
      "epoch: 4, batch: 100, loss: 0.06069332733750343\n",
      "epoch: 4, batch: 101, loss: 0.027332574129104614\n",
      "epoch: 4, batch: 102, loss: 0.010587520897388458\n",
      "epoch: 4, batch: 103, loss: 0.03587690740823746\n",
      "epoch: 4, batch: 104, loss: 0.0872325450181961\n",
      "epoch: 4, batch: 105, loss: 0.2649034261703491\n",
      "epoch: 4, batch: 106, loss: 0.023121846839785576\n",
      "epoch: 4, batch: 107, loss: 0.03758620843291283\n",
      "epoch: 4, batch: 108, loss: 0.04703471437096596\n",
      "epoch: 4, batch: 109, loss: 0.012176483869552612\n",
      "epoch: 4, batch: 110, loss: 0.0054993415251374245\n",
      "epoch: 4, batch: 111, loss: 0.07451045513153076\n",
      "epoch: 4, batch: 112, loss: 0.04902302101254463\n",
      "epoch: 4, batch: 113, loss: 0.015193792060017586\n",
      "epoch: 4, batch: 114, loss: 0.11242974549531937\n",
      "epoch: 4, batch: 115, loss: 0.05081453174352646\n",
      "epoch: 4, batch: 116, loss: 0.05803748965263367\n",
      "epoch: 4, batch: 117, loss: 0.05177577584981918\n",
      "epoch: 4, batch: 118, loss: 0.010231293737888336\n",
      "epoch: 4, batch: 119, loss: 0.009939201176166534\n",
      "epoch: 4, batch: 120, loss: 0.0662723109126091\n",
      "epoch: 4, batch: 121, loss: 0.008723607286810875\n",
      "epoch: 4, batch: 122, loss: 0.13563796877861023\n",
      "epoch: 4, batch: 123, loss: 0.1486460119485855\n",
      "epoch: 4, batch: 124, loss: 0.07485804706811905\n",
      "epoch: 4, batch: 125, loss: 0.03395223245024681\n",
      "epoch: 4, batch: 126, loss: 0.0786176472902298\n",
      "epoch: 4, batch: 127, loss: 0.00829681009054184\n",
      "epoch: 4, batch: 128, loss: 0.07623932510614395\n",
      "epoch: 4, batch: 129, loss: 0.013787219300866127\n",
      "epoch: 4, batch: 130, loss: 0.005208698567003012\n",
      "epoch: 4, batch: 131, loss: 0.03471619263291359\n",
      "epoch: 4, batch: 132, loss: 0.07350403070449829\n",
      "epoch: 4, batch: 133, loss: 0.03218642622232437\n",
      "epoch: 4, batch: 134, loss: 0.02571256086230278\n",
      "epoch: 4, batch: 135, loss: 0.07757221907377243\n",
      "epoch: 4, batch: 136, loss: 0.10546736419200897\n",
      "epoch: 4, batch: 137, loss: 0.05340933799743652\n",
      "epoch: 4, batch: 138, loss: 0.02915111370384693\n",
      "epoch: 4, batch: 139, loss: 0.014429925009608269\n",
      "epoch: 4, batch: 140, loss: 0.06302101910114288\n",
      "epoch: 4, batch: 141, loss: 0.05500579997897148\n",
      "epoch: 4, batch: 142, loss: 0.028816480189561844\n",
      "epoch: 4, batch: 143, loss: 0.19193126261234283\n",
      "epoch: 4, batch: 144, loss: 0.11488140374422073\n",
      "epoch: 4, batch: 145, loss: 0.11895216256380081\n",
      "epoch: 4, batch: 146, loss: 0.04266424849629402\n",
      "epoch: 4, batch: 147, loss: 0.06586362421512604\n",
      "epoch: 4, batch: 148, loss: 0.06518363952636719\n",
      "epoch: 4, batch: 149, loss: 0.11456882953643799\n",
      "epoch: 4, batch: 150, loss: 0.024432066828012466\n",
      "epoch: 4, batch: 151, loss: 0.012182164937257767\n",
      "epoch: 4, batch: 152, loss: 0.016999419778585434\n",
      "epoch: 4, batch: 153, loss: 0.09755250811576843\n",
      "epoch: 4, batch: 154, loss: 0.09195971488952637\n",
      "epoch: 4, batch: 155, loss: 0.05064890906214714\n",
      "epoch: 4, batch: 156, loss: 0.10499995946884155\n",
      "epoch: 4, batch: 157, loss: 0.133382186293602\n",
      "epoch: 4, batch: 158, loss: 0.02598237618803978\n",
      "epoch: 4, batch: 159, loss: 0.11228056252002716\n",
      "epoch: 4, batch: 160, loss: 0.01761035993695259\n",
      "epoch: 4, batch: 161, loss: 0.057289958000183105\n",
      "epoch: 4, batch: 162, loss: 0.0744975358247757\n",
      "epoch: 4, batch: 163, loss: 0.0050763655453920364\n",
      "epoch: 4, batch: 164, loss: 0.1357218474149704\n",
      "epoch: 4, batch: 165, loss: 0.03928086906671524\n",
      "epoch: 4, batch: 166, loss: 0.013849440030753613\n",
      "epoch: 4, batch: 167, loss: 0.04395740479230881\n",
      "epoch: 4, batch: 168, loss: 0.019292425364255905\n",
      "epoch: 4, batch: 169, loss: 0.011400893330574036\n",
      "epoch: 4, batch: 170, loss: 0.05121656134724617\n",
      "epoch: 4, batch: 171, loss: 0.09811997413635254\n",
      "epoch: 4, batch: 172, loss: 0.028416765853762627\n",
      "epoch: 4, batch: 173, loss: 0.028262028470635414\n",
      "epoch: 4, batch: 174, loss: 0.04462151229381561\n",
      "epoch: 4, batch: 175, loss: 0.012704149819910526\n",
      "epoch: 4, batch: 176, loss: 0.05576743930578232\n",
      "epoch: 4, batch: 177, loss: 0.015276284888386726\n",
      "epoch: 4, batch: 178, loss: 0.028129592537879944\n",
      "epoch: 4, batch: 179, loss: 0.011246382258832455\n",
      "epoch: 4, batch: 180, loss: 0.07203264534473419\n",
      "epoch: 4, batch: 181, loss: 0.07383222877979279\n",
      "epoch: 4, batch: 182, loss: 0.03895145654678345\n",
      "epoch: 4, batch: 183, loss: 0.13335418701171875\n",
      "epoch: 4, batch: 184, loss: 0.06783096492290497\n",
      "epoch: 4, batch: 185, loss: 0.008347682654857635\n",
      "epoch: 4, batch: 186, loss: 0.03728306293487549\n",
      "epoch: 4, batch: 187, loss: 0.08539830893278122\n",
      "epoch: 4, batch: 188, loss: 0.0650119036436081\n",
      "epoch: 4, batch: 189, loss: 0.16001154482364655\n",
      "epoch: 4, batch: 190, loss: 0.1137133464217186\n",
      "epoch: 4, batch: 191, loss: 0.015796810388565063\n",
      "epoch: 4, batch: 192, loss: 0.04481964558362961\n",
      "epoch: 4, batch: 193, loss: 0.07970670610666275\n",
      "epoch: 4, batch: 194, loss: 0.08967337012290955\n",
      "epoch: 4, batch: 195, loss: 0.016080565750598907\n",
      "epoch: 4, batch: 196, loss: 0.023473257198929787\n",
      "epoch: 4, batch: 197, loss: 0.0054374998435378075\n",
      "epoch: 4, batch: 198, loss: 0.011089755222201347\n",
      "epoch: 4, batch: 199, loss: 0.01098745409399271\n",
      "epoch: 4, batch: 200, loss: 0.012877252884209156\n",
      "epoch: 4, batch: 201, loss: 0.02106166072189808\n",
      "epoch: 4, batch: 202, loss: 0.019691895693540573\n",
      "epoch: 4, batch: 203, loss: 0.006306276191025972\n",
      "epoch: 4, batch: 204, loss: 0.014335287734866142\n",
      "epoch: 4, batch: 205, loss: 0.029051681980490685\n",
      "epoch: 4, batch: 206, loss: 0.07045290619134903\n",
      "epoch: 4, batch: 207, loss: 0.03394220024347305\n",
      "epoch: 4, batch: 208, loss: 0.13789290189743042\n",
      "epoch: 4, batch: 209, loss: 0.07067739963531494\n",
      "epoch: 4, batch: 210, loss: 0.03720482438802719\n",
      "epoch: 4, batch: 211, loss: 0.09484972804784775\n",
      "epoch: 4, batch: 212, loss: 0.08291459828615189\n",
      "epoch: 4, batch: 213, loss: 0.0131659135222435\n",
      "epoch: 4, batch: 214, loss: 0.012491799890995026\n",
      "epoch: 4, batch: 215, loss: 0.16478341817855835\n",
      "epoch: 4, batch: 216, loss: 0.08959929645061493\n",
      "epoch: 4, batch: 217, loss: 0.003568383865058422\n",
      "epoch: 4, batch: 218, loss: 0.04854903370141983\n",
      "epoch: 4, batch: 219, loss: 0.21611271798610687\n",
      "epoch: 4, batch: 220, loss: 0.01924927346408367\n",
      "epoch: 4, batch: 221, loss: 0.010226492770016193\n",
      "epoch: 4, batch: 222, loss: 0.03501848131418228\n",
      "epoch: 4, batch: 223, loss: 0.033449720591306686\n",
      "epoch: 4, batch: 224, loss: 0.10245751589536667\n",
      "epoch: 4, batch: 225, loss: 0.037977736443281174\n",
      "epoch: 4, batch: 226, loss: 0.05266718566417694\n",
      "epoch: 4, batch: 227, loss: 0.07448945194482803\n",
      "epoch: 4, batch: 228, loss: 0.025813143700361252\n",
      "epoch: 4, batch: 229, loss: 0.08336485922336578\n",
      "epoch: 4, batch: 230, loss: 0.006389799527823925\n",
      "epoch: 4, batch: 231, loss: 0.10274273157119751\n",
      "epoch: 4, batch: 232, loss: 0.05357735976576805\n",
      "epoch: 4, batch: 233, loss: 0.016770245507359505\n",
      "epoch: 4, batch: 234, loss: 0.038032472133636475\n",
      "epoch: 4, batch: 235, loss: 0.070489801466465\n",
      "epoch: 4, batch: 236, loss: 0.09246370196342468\n",
      "epoch: 4, batch: 237, loss: 0.04761810600757599\n",
      "epoch: 4, batch: 238, loss: 0.041082195937633514\n",
      "epoch: 4, batch: 239, loss: 0.15378324687480927\n",
      "epoch: 4, batch: 240, loss: 0.04262123256921768\n",
      "epoch: 4, batch: 241, loss: 0.028403500095009804\n",
      "epoch: 4, batch: 242, loss: 0.1383237987756729\n",
      "epoch: 4, batch: 243, loss: 0.0645984411239624\n",
      "epoch: 4, batch: 244, loss: 0.14494206011295319\n",
      "epoch: 4, batch: 245, loss: 0.029582297429442406\n",
      "epoch: 4, batch: 246, loss: 0.006379758007824421\n",
      "epoch: 4, batch: 247, loss: 0.016184955835342407\n",
      "epoch: 4, batch: 248, loss: 0.0384821854531765\n",
      "epoch: 4, batch: 249, loss: 0.02056393213570118\n",
      "epoch: 4, batch: 250, loss: 0.04973471909761429\n",
      "epoch: 4, batch: 251, loss: 0.014927823096513748\n",
      "epoch: 4, batch: 252, loss: 0.035599492490291595\n",
      "epoch: 4, batch: 253, loss: 0.07836352288722992\n",
      "epoch: 4, batch: 254, loss: 0.006867256946861744\n",
      "epoch: 4, batch: 255, loss: 0.03129107505083084\n",
      "epoch: 4, batch: 256, loss: 0.06553281098604202\n",
      "epoch: 4, batch: 257, loss: 0.008237355388700962\n",
      "epoch: 4, batch: 258, loss: 0.09420280903577805\n",
      "epoch: 4, batch: 259, loss: 0.058725301176309586\n",
      "epoch: 4, batch: 260, loss: 0.008040533401072025\n",
      "epoch: 4, batch: 261, loss: 0.07428517937660217\n",
      "epoch: 4, batch: 262, loss: 0.0987464115023613\n",
      "epoch: 4, batch: 263, loss: 0.04150514304637909\n",
      "epoch: 4, batch: 264, loss: 0.02169833518564701\n",
      "epoch: 4, batch: 265, loss: 0.013482135720551014\n",
      "epoch: 4, batch: 266, loss: 0.0196637324988842\n",
      "epoch: 4, batch: 267, loss: 0.03002067469060421\n",
      "epoch: 4, batch: 268, loss: 0.012094732373952866\n",
      "epoch: 4, batch: 269, loss: 0.004795646294951439\n",
      "epoch: 4, batch: 270, loss: 0.013175109401345253\n",
      "epoch: 4, batch: 271, loss: 0.029651835560798645\n",
      "epoch: 4, batch: 272, loss: 0.045205313712358475\n",
      "epoch: 4, batch: 273, loss: 0.012306610122323036\n",
      "epoch: 4, batch: 274, loss: 0.0065661161206662655\n",
      "epoch: 4, batch: 275, loss: 0.08592627942562103\n",
      "epoch: 4, batch: 276, loss: 0.06259201467037201\n",
      "epoch: 4, batch: 277, loss: 0.11717952787876129\n",
      "epoch: 4, batch: 278, loss: 0.012565619312226772\n",
      "epoch: 4, batch: 279, loss: 0.025192981585860252\n",
      "epoch: 4, batch: 280, loss: 0.036770083010196686\n",
      "epoch: 4, batch: 281, loss: 0.03772943839430809\n",
      "epoch: 4, batch: 282, loss: 0.00913910660892725\n",
      "epoch: 4, batch: 283, loss: 0.007854186929762363\n",
      "epoch: 4, batch: 284, loss: 0.00397681025788188\n",
      "epoch: 4, batch: 285, loss: 0.02170008234679699\n",
      "epoch: 4, batch: 286, loss: 0.061032894998788834\n",
      "epoch: 4, batch: 287, loss: 0.08617610484361649\n",
      "epoch: 4, batch: 288, loss: 0.015753254294395447\n",
      "epoch: 4, batch: 289, loss: 0.01584448665380478\n",
      "epoch: 4, batch: 290, loss: 0.05138179659843445\n",
      "epoch: 4, batch: 291, loss: 0.038171134889125824\n",
      "epoch: 4, batch: 292, loss: 0.021747879683971405\n",
      "epoch: 4, batch: 293, loss: 0.03414006903767586\n",
      "epoch: 4, batch: 294, loss: 0.0236539077013731\n",
      "epoch: 4, batch: 295, loss: 0.004251405596733093\n",
      "epoch: 4, batch: 296, loss: 0.056674882769584656\n",
      "epoch: 4, batch: 297, loss: 0.02219511941075325\n",
      "epoch: 4, batch: 298, loss: 0.044397078454494476\n",
      "epoch: 4, batch: 299, loss: 0.035174909979104996\n",
      "epoch: 4, batch: 300, loss: 0.08318238705396652\n",
      "epoch: 4, batch: 301, loss: 0.13008277118206024\n",
      "epoch: 4, batch: 302, loss: 0.027859997004270554\n",
      "epoch: 4, batch: 303, loss: 0.043309569358825684\n",
      "epoch: 4, batch: 304, loss: 0.0037958642933517694\n",
      "epoch: 4, batch: 305, loss: 0.05729945749044418\n",
      "epoch: 4, batch: 306, loss: 0.019242681562900543\n",
      "epoch: 4, batch: 307, loss: 0.06805937737226486\n",
      "epoch: 4, batch: 308, loss: 0.01742830127477646\n",
      "epoch: 4, batch: 309, loss: 0.013436203822493553\n",
      "epoch: 4, batch: 310, loss: 0.042312655597925186\n",
      "epoch: 4, batch: 311, loss: 0.007810608949512243\n",
      "epoch: 4, batch: 312, loss: 0.017908623442053795\n",
      "epoch: 4, batch: 313, loss: 0.07234862446784973\n",
      "epoch: 4, batch: 314, loss: 0.06123922020196915\n",
      "epoch: 4, batch: 315, loss: 0.09314852207899094\n",
      "epoch: 4, batch: 316, loss: 0.03343777731060982\n",
      "epoch: 4, batch: 317, loss: 0.021943707019090652\n",
      "epoch: 4, batch: 318, loss: 0.017832865938544273\n",
      "epoch: 4, batch: 319, loss: 0.10959428548812866\n",
      "epoch: 4, batch: 320, loss: 0.008618978783488274\n",
      "epoch: 4, batch: 321, loss: 0.01540999487042427\n",
      "epoch: 4, batch: 322, loss: 0.09162356704473495\n",
      "epoch: 4, batch: 323, loss: 0.46451348066329956\n",
      "epoch: 4, batch: 324, loss: 0.009383486583828926\n",
      "epoch: 4, batch: 325, loss: 0.04014769196510315\n",
      "epoch: 4, batch: 326, loss: 0.06738070398569107\n",
      "epoch: 4, batch: 327, loss: 0.021137038245797157\n",
      "epoch: 4, batch: 328, loss: 0.18199962377548218\n",
      "epoch: 4, batch: 329, loss: 0.06148171052336693\n",
      "epoch: 4, batch: 330, loss: 0.027547184377908707\n",
      "epoch: 4, batch: 331, loss: 0.044435206800699234\n",
      "epoch: 4, batch: 332, loss: 0.007368015591055155\n",
      "epoch: 4, batch: 333, loss: 0.047233957797288895\n",
      "epoch: 4, batch: 334, loss: 0.04947235807776451\n",
      "epoch: 4, batch: 335, loss: 0.014967868104577065\n",
      "epoch: 4, batch: 336, loss: 0.0757802277803421\n",
      "epoch: 4, batch: 337, loss: 0.014107078313827515\n",
      "epoch: 4, batch: 338, loss: 0.02055223472416401\n",
      "epoch: 4, batch: 339, loss: 0.04738774523139\n",
      "epoch: 4, batch: 340, loss: 0.02947128564119339\n",
      "epoch: 4, batch: 341, loss: 0.1840594857931137\n",
      "epoch: 4, batch: 342, loss: 0.051326725631952286\n",
      "epoch: 4, batch: 343, loss: 0.11505404114723206\n",
      "epoch: 4, batch: 344, loss: 0.08618433028459549\n",
      "epoch: 4, batch: 345, loss: 0.05700875446200371\n",
      "epoch: 4, batch: 346, loss: 0.0599057637155056\n",
      "epoch: 4, batch: 347, loss: 0.017705798149108887\n",
      "epoch: 4, batch: 348, loss: 0.033950287848711014\n",
      "epoch: 4, batch: 349, loss: 0.06492024660110474\n",
      "epoch: 4, batch: 350, loss: 0.014673981815576553\n",
      "epoch: 4, batch: 351, loss: 0.07384113222360611\n",
      "epoch: 4, batch: 352, loss: 0.018457846716046333\n",
      "epoch: 4, batch: 353, loss: 0.059590235352516174\n",
      "epoch: 4, batch: 354, loss: 0.10096278786659241\n",
      "epoch: 4, batch: 355, loss: 0.008701387792825699\n",
      "epoch: 4, batch: 356, loss: 0.08601053804159164\n",
      "epoch: 4, batch: 357, loss: 0.04724254831671715\n",
      "epoch: 4, batch: 358, loss: 0.034035682678222656\n",
      "epoch: 4, batch: 359, loss: 0.012159875594079494\n",
      "epoch: 4, batch: 360, loss: 0.1289500594139099\n",
      "epoch: 4, batch: 361, loss: 0.02091408334672451\n",
      "epoch: 4, batch: 362, loss: 0.033641017973423004\n",
      "epoch: 4, batch: 363, loss: 0.1651858687400818\n",
      "epoch: 4, batch: 364, loss: 0.046085815876722336\n",
      "epoch: 4, batch: 365, loss: 0.04867836460471153\n",
      "epoch: 4, batch: 366, loss: 0.0600694976747036\n",
      "epoch: 4, batch: 367, loss: 0.034680597484111786\n",
      "epoch: 4, batch: 368, loss: 0.05403284355998039\n",
      "epoch: 4, batch: 369, loss: 0.11781249195337296\n",
      "epoch: 4, batch: 370, loss: 0.007323865313082933\n",
      "epoch: 4, batch: 371, loss: 0.006352127064019442\n",
      "epoch: 4, batch: 372, loss: 0.0063356077298521996\n",
      "epoch: 4, batch: 373, loss: 0.05448315665125847\n",
      "epoch: 4, batch: 374, loss: 0.02452903240919113\n",
      "epoch: 4, batch: 375, loss: 0.060216810554265976\n",
      "epoch: 4, batch: 376, loss: 0.04528125748038292\n",
      "epoch: 4, batch: 377, loss: 0.03060171566903591\n",
      "epoch: 4, batch: 378, loss: 0.01975492760539055\n",
      "epoch: 4, batch: 379, loss: 0.02484268695116043\n",
      "epoch: 4, batch: 380, loss: 0.058687642216682434\n",
      "epoch: 4, batch: 381, loss: 0.05238032713532448\n",
      "epoch: 4, batch: 382, loss: 0.0230857003480196\n",
      "epoch: 4, batch: 383, loss: 0.023595599457621574\n",
      "epoch: 4, batch: 384, loss: 0.036250241100788116\n",
      "epoch: 4, batch: 385, loss: 0.1821892112493515\n",
      "epoch: 4, batch: 386, loss: 0.03269574046134949\n",
      "epoch: 4, batch: 387, loss: 0.02900170162320137\n",
      "epoch: 4, batch: 388, loss: 0.0485551692545414\n",
      "epoch: 4, batch: 389, loss: 0.014568651095032692\n",
      "epoch: 4, batch: 390, loss: 0.01022349577397108\n",
      "epoch: 4, batch: 391, loss: 0.012440981343388557\n",
      "epoch: 4, batch: 392, loss: 0.13340544700622559\n",
      "epoch: 4, batch: 393, loss: 0.043704837560653687\n",
      "epoch: 4, batch: 394, loss: 0.13309870660305023\n",
      "epoch: 4, batch: 395, loss: 0.01818949542939663\n",
      "epoch: 4, batch: 396, loss: 0.1401652842760086\n",
      "epoch: 4, batch: 397, loss: 0.02348993346095085\n",
      "epoch: 4, batch: 398, loss: 0.020677808672189713\n",
      "epoch: 4, batch: 399, loss: 0.02041640318930149\n",
      "epoch: 4, batch: 400, loss: 0.03516790270805359\n",
      "epoch: 4, batch: 401, loss: 0.05562566593289375\n",
      "epoch: 4, batch: 402, loss: 0.09979776293039322\n",
      "epoch: 4, batch: 403, loss: 0.00886391382664442\n",
      "epoch: 4, batch: 404, loss: 0.0849234089255333\n",
      "epoch: 4, batch: 405, loss: 0.09070619940757751\n",
      "epoch: 4, batch: 406, loss: 0.08985092490911484\n",
      "epoch: 4, batch: 407, loss: 0.034949615597724915\n",
      "epoch: 4, batch: 408, loss: 0.025126608088612556\n",
      "epoch: 4, batch: 409, loss: 0.11742240935564041\n",
      "epoch: 4, batch: 410, loss: 0.06649240106344223\n",
      "epoch: 4, batch: 411, loss: 0.02089403010904789\n",
      "epoch: 4, batch: 412, loss: 0.015470299869775772\n",
      "epoch: 4, batch: 413, loss: 0.13761857151985168\n",
      "epoch: 4, batch: 414, loss: 0.14266285300254822\n",
      "epoch: 4, batch: 415, loss: 0.031563740223646164\n",
      "epoch: 4, batch: 416, loss: 0.02764264866709709\n",
      "epoch: 4, batch: 417, loss: 0.004074429627507925\n",
      "epoch: 4, batch: 418, loss: 0.030913840979337692\n",
      "epoch: 4, batch: 419, loss: 0.022203022614121437\n",
      "epoch: 4, batch: 420, loss: 0.008952795527875423\n",
      "epoch: 4, batch: 421, loss: 0.06138826161623001\n",
      "epoch: 4, batch: 422, loss: 0.1528400331735611\n",
      "epoch: 4, batch: 423, loss: 0.01850062794983387\n",
      "epoch: 4, batch: 424, loss: 0.01636446639895439\n",
      "epoch: 4, batch: 425, loss: 0.0093363793566823\n",
      "epoch: 4, batch: 426, loss: 0.043767619878053665\n",
      "epoch: 4, batch: 427, loss: 0.053199246525764465\n",
      "epoch: 4, batch: 428, loss: 0.01333385519683361\n",
      "epoch: 4, batch: 429, loss: 0.07515813410282135\n",
      "epoch: 4, batch: 430, loss: 0.0374295637011528\n",
      "epoch: 4, batch: 431, loss: 0.10241150856018066\n",
      "epoch: 4, batch: 432, loss: 0.027524301782250404\n",
      "epoch: 4, batch: 433, loss: 0.029128599911928177\n",
      "epoch: 4, batch: 434, loss: 0.09246028959751129\n",
      "epoch: 4, batch: 435, loss: 0.022037077695131302\n",
      "epoch: 4, batch: 436, loss: 0.014184308238327503\n",
      "epoch: 4, batch: 437, loss: 0.014655182138085365\n",
      "epoch: 4, batch: 438, loss: 0.0133439926430583\n",
      "epoch: 4, batch: 439, loss: 0.06340847164392471\n",
      "epoch: 4, batch: 440, loss: 0.0291470754891634\n",
      "epoch: 4, batch: 441, loss: 0.04090935364365578\n",
      "epoch: 4, batch: 442, loss: 0.09402737021446228\n",
      "epoch: 4, batch: 443, loss: 0.022870320826768875\n",
      "epoch: 4, batch: 444, loss: 0.030680572614073753\n",
      "epoch: 4, batch: 445, loss: 0.07844576239585876\n",
      "epoch: 4, batch: 446, loss: 0.12744879722595215\n",
      "epoch: 4, batch: 447, loss: 0.015588578768074512\n",
      "epoch: 4, batch: 448, loss: 0.06799542158842087\n",
      "epoch: 4, batch: 449, loss: 0.02418523281812668\n",
      "epoch: 4, batch: 450, loss: 0.00556481909006834\n",
      "epoch: 4, batch: 451, loss: 0.00958964228630066\n",
      "epoch: 4, batch: 452, loss: 0.07563979178667068\n",
      "epoch: 4, batch: 453, loss: 0.04958086833357811\n",
      "epoch: 4, batch: 454, loss: 0.059409234672784805\n",
      "epoch: 4, batch: 455, loss: 0.025467637926340103\n",
      "epoch: 4, batch: 456, loss: 0.044787466526031494\n",
      "epoch: 4, batch: 457, loss: 0.0096670463681221\n",
      "epoch: 4, batch: 458, loss: 0.05649826303124428\n",
      "epoch: 4, batch: 459, loss: 0.05390419065952301\n",
      "epoch: 4, batch: 460, loss: 0.2576117217540741\n",
      "epoch: 4, batch: 461, loss: 0.04100659489631653\n",
      "epoch: 4, batch: 462, loss: 0.012463346123695374\n",
      "epoch: 4, batch: 463, loss: 0.06393316388130188\n",
      "epoch: 4, batch: 464, loss: 0.007618062198162079\n",
      "epoch: 4, batch: 465, loss: 0.08794056624174118\n",
      "epoch: 4, batch: 466, loss: 0.031086813658475876\n",
      "epoch: 4, batch: 467, loss: 0.02937372960150242\n",
      "epoch: 4, batch: 468, loss: 0.04447357729077339\n",
      "epoch: 4, batch: 469, loss: 0.060967348515987396\n",
      "epoch: 4, batch: 470, loss: 0.05871537700295448\n",
      "epoch: 4, batch: 471, loss: 0.10007442533969879\n",
      "epoch: 4, batch: 472, loss: 0.07481041550636292\n",
      "epoch: 4, batch: 473, loss: 0.04709010571241379\n",
      "epoch: 4, batch: 474, loss: 0.0423453152179718\n",
      "epoch: 4, batch: 475, loss: 0.019510716199874878\n",
      "epoch: 4, batch: 476, loss: 0.0068152667954564095\n",
      "epoch: 4, batch: 477, loss: 0.1605178415775299\n",
      "epoch: 4, batch: 478, loss: 0.022381456568837166\n",
      "epoch: 4, batch: 479, loss: 0.06453470885753632\n",
      "epoch: 4, batch: 480, loss: 0.045067865401506424\n",
      "epoch: 4, batch: 481, loss: 0.03977768495678902\n",
      "epoch: 4, batch: 482, loss: 0.09952061623334885\n",
      "epoch: 4, batch: 483, loss: 0.01505851186811924\n",
      "epoch: 4, batch: 484, loss: 0.04254152998328209\n",
      "epoch: 4, batch: 485, loss: 0.08090724796056747\n",
      "epoch: 4, batch: 486, loss: 0.03114422596991062\n",
      "epoch: 4, batch: 487, loss: 0.052460312843322754\n",
      "epoch: 4, batch: 488, loss: 0.0073132989928126335\n",
      "epoch: 4, batch: 489, loss: 0.03291580453515053\n",
      "epoch: 4, batch: 490, loss: 0.04653916135430336\n",
      "epoch: 4, batch: 491, loss: 0.01108046155422926\n",
      "epoch: 4, batch: 492, loss: 0.022594895213842392\n",
      "epoch: 4, batch: 493, loss: 0.07257521897554398\n",
      "epoch: 4, batch: 494, loss: 0.022723345085978508\n",
      "epoch: 4, batch: 495, loss: 0.026181519031524658\n",
      "epoch: 4, batch: 496, loss: 0.09854468703269958\n",
      "epoch: 4, batch: 497, loss: 0.024345623329281807\n",
      "epoch: 4, batch: 498, loss: 0.08752474933862686\n",
      "epoch: 4, batch: 499, loss: 0.09683625400066376\n",
      "epoch: 4, batch: 500, loss: 0.06334048509597778\n",
      "epoch: 4, batch: 501, loss: 0.016312727704644203\n",
      "epoch: 4, batch: 502, loss: 0.07771062850952148\n",
      "epoch: 4, batch: 503, loss: 0.037966497242450714\n",
      "epoch: 4, batch: 504, loss: 0.015591093339025974\n",
      "epoch: 4, batch: 505, loss: 0.006683534011244774\n",
      "epoch: 4, batch: 506, loss: 0.007577447220683098\n",
      "epoch: 4, batch: 507, loss: 0.02723044902086258\n",
      "epoch: 4, batch: 508, loss: 0.014005560427904129\n",
      "epoch: 4, batch: 509, loss: 0.014115351252257824\n",
      "epoch: 4, batch: 510, loss: 0.017625750973820686\n",
      "epoch: 4, batch: 511, loss: 0.026484481990337372\n",
      "epoch: 4, batch: 512, loss: 0.011239415034651756\n",
      "epoch: 4, batch: 513, loss: 0.023210903629660606\n",
      "epoch: 4, batch: 514, loss: 0.0886821299791336\n",
      "epoch: 4, batch: 515, loss: 0.00973393302410841\n",
      "epoch: 4, batch: 516, loss: 0.082329161465168\n",
      "epoch: 4, batch: 517, loss: 0.013623841106891632\n",
      "epoch: 4, batch: 518, loss: 0.04199919104576111\n",
      "epoch: 4, batch: 519, loss: 0.028285525739192963\n",
      "epoch: 4, batch: 520, loss: 0.005641236435621977\n",
      "epoch: 4, batch: 521, loss: 0.020010121166706085\n",
      "epoch: 4, batch: 522, loss: 0.03043636679649353\n",
      "epoch: 4, batch: 523, loss: 0.08495855331420898\n",
      "epoch: 4, batch: 524, loss: 0.02403241954743862\n",
      "epoch: 4, batch: 525, loss: 0.09126490354537964\n",
      "epoch: 4, batch: 526, loss: 0.11913885921239853\n",
      "epoch: 4, batch: 527, loss: 0.17764605581760406\n",
      "epoch: 4, batch: 528, loss: 0.02539856731891632\n",
      "epoch: 4, batch: 529, loss: 0.05878263711929321\n",
      "epoch: 4, batch: 530, loss: 0.04000429809093475\n",
      "epoch: 4, batch: 531, loss: 0.005259845405817032\n",
      "epoch: 4, batch: 532, loss: 0.014179316349327564\n",
      "epoch: 4, batch: 533, loss: 0.00559259532019496\n",
      "epoch: 4, batch: 534, loss: 0.17972233891487122\n",
      "epoch: 4, batch: 535, loss: 0.08446797728538513\n",
      "epoch: 4, batch: 536, loss: 0.036717262119054794\n",
      "epoch: 4, batch: 537, loss: 0.007520533166825771\n",
      "epoch: 4, batch: 538, loss: 0.008759981952607632\n",
      "epoch: 4, batch: 539, loss: 0.157316192984581\n",
      "epoch: 4, batch: 540, loss: 0.004007939249277115\n",
      "epoch: 4, batch: 541, loss: 0.03802676871418953\n",
      "epoch: 4, batch: 542, loss: 0.05602775886654854\n",
      "epoch: 4, batch: 543, loss: 0.01797463744878769\n",
      "epoch: 4, batch: 544, loss: 0.10001387447118759\n",
      "epoch: 4, batch: 545, loss: 0.028982043266296387\n",
      "epoch: 4, batch: 546, loss: 0.0065415846183896065\n",
      "epoch: 4, batch: 547, loss: 0.03552670404314995\n",
      "epoch: 4, batch: 548, loss: 0.08971859514713287\n",
      "epoch: 4, batch: 549, loss: 0.020442795008420944\n",
      "epoch: 4, batch: 550, loss: 0.0655292421579361\n",
      "epoch: 4, batch: 551, loss: 0.1599845439195633\n",
      "epoch: 4, batch: 552, loss: 0.05227793753147125\n",
      "epoch: 4, batch: 553, loss: 0.05129781365394592\n",
      "epoch: 4, batch: 554, loss: 0.02553381212055683\n",
      "epoch: 4, batch: 555, loss: 0.013995274901390076\n",
      "epoch: 4, batch: 556, loss: 0.0135638527572155\n",
      "epoch: 4, batch: 557, loss: 0.016645899042487144\n",
      "epoch: 4, batch: 558, loss: 0.01421383861452341\n",
      "epoch: 4, batch: 559, loss: 0.026532648131251335\n",
      "epoch: 4, batch: 560, loss: 0.055613599717617035\n",
      "epoch: 4, batch: 561, loss: 0.06111661344766617\n",
      "epoch: 4, batch: 562, loss: 0.03157207369804382\n",
      "epoch: 4, batch: 563, loss: 0.006752526853233576\n",
      "epoch: 4, batch: 564, loss: 0.06606817990541458\n",
      "epoch: 4, batch: 565, loss: 0.0513201579451561\n",
      "epoch: 4, batch: 566, loss: 0.043842192739248276\n",
      "epoch: 4, batch: 567, loss: 0.1412743777036667\n",
      "epoch: 4, batch: 568, loss: 0.02677963860332966\n",
      "epoch: 4, batch: 569, loss: 0.08912608027458191\n",
      "epoch: 4, batch: 570, loss: 0.03122089058160782\n",
      "epoch: 4, batch: 571, loss: 0.042148806154727936\n",
      "epoch: 4, batch: 572, loss: 0.13236944377422333\n",
      "epoch: 4, batch: 573, loss: 0.00803367793560028\n",
      "epoch: 4, batch: 574, loss: 0.03961725905537605\n",
      "epoch: 4, batch: 575, loss: 0.035341743379831314\n",
      "epoch: 4, batch: 576, loss: 0.010387400165200233\n",
      "epoch: 4, batch: 577, loss: 0.01967635378241539\n",
      "epoch: 4, batch: 578, loss: 0.011328320018947124\n",
      "epoch: 4, batch: 579, loss: 0.030437897890806198\n",
      "epoch: 4, batch: 580, loss: 0.028325650840997696\n",
      "epoch: 4, batch: 581, loss: 0.09036365896463394\n",
      "epoch: 4, batch: 582, loss: 0.0484713576734066\n",
      "epoch: 4, batch: 583, loss: 0.05951511859893799\n",
      "epoch: 4, batch: 584, loss: 0.011343545280396938\n",
      "epoch: 4, batch: 585, loss: 0.029690178111195564\n",
      "epoch: 4, batch: 586, loss: 0.012993360869586468\n",
      "epoch: 4, batch: 587, loss: 0.013428661040961742\n",
      "epoch: 4, batch: 588, loss: 0.04393669217824936\n",
      "epoch: 4, batch: 589, loss: 0.008985781110823154\n",
      "epoch: 4, batch: 590, loss: 0.14017434418201447\n",
      "epoch: 4, batch: 591, loss: 0.02068314515054226\n",
      "epoch: 4, batch: 592, loss: 0.07298161089420319\n",
      "epoch: 4, batch: 593, loss: 0.03216177970170975\n",
      "epoch: 4, batch: 594, loss: 0.06354841589927673\n",
      "epoch: 4, batch: 595, loss: 0.09669412672519684\n",
      "epoch: 4, batch: 596, loss: 0.19303901493549347\n",
      "epoch: 4, batch: 597, loss: 0.016248635947704315\n",
      "epoch: 4, batch: 598, loss: 0.0940684825181961\n",
      "epoch: 4, batch: 599, loss: 0.1065194308757782\n",
      "epoch: 4, batch: 600, loss: 0.03921856731176376\n",
      "epoch: 4, batch: 601, loss: 0.03383423388004303\n",
      "epoch: 4, batch: 602, loss: 0.01907668076455593\n",
      "epoch: 4, batch: 603, loss: 0.015134694054722786\n",
      "epoch: 4, batch: 604, loss: 0.00842480082064867\n",
      "epoch: 4, batch: 605, loss: 0.03771815448999405\n",
      "epoch: 4, batch: 606, loss: 0.04115801677107811\n",
      "epoch: 4, batch: 607, loss: 0.019690973684191704\n",
      "epoch: 4, batch: 608, loss: 0.09574709087610245\n",
      "epoch: 4, batch: 609, loss: 0.045795898884534836\n",
      "epoch: 4, batch: 610, loss: 0.10058353841304779\n",
      "epoch: 4, batch: 611, loss: 0.08218397945165634\n",
      "epoch: 4, batch: 612, loss: 0.07401489466428757\n",
      "epoch: 4, batch: 613, loss: 0.022731581702828407\n",
      "epoch: 4, batch: 614, loss: 0.01780741848051548\n",
      "epoch: 4, batch: 615, loss: 0.14107592403888702\n",
      "epoch: 4, batch: 616, loss: 0.15801595151424408\n",
      "epoch: 4, batch: 617, loss: 0.011327492073178291\n",
      "epoch: 4, batch: 618, loss: 0.009165472351014614\n",
      "epoch: 4, batch: 619, loss: 0.03995608165860176\n",
      "epoch: 4, batch: 620, loss: 0.046657636761665344\n",
      "epoch: 4, batch: 621, loss: 0.16582991182804108\n",
      "epoch: 4, batch: 622, loss: 0.18610210716724396\n",
      "epoch: 4, batch: 623, loss: 0.06740045547485352\n",
      "epoch: 4, batch: 624, loss: 0.010331574827432632\n",
      "epoch: 4, batch: 625, loss: 0.047045864164829254\n",
      "epoch: 4, batch: 626, loss: 0.03621741384267807\n",
      "epoch: 4, batch: 627, loss: 0.02902497537434101\n",
      "epoch: 4, batch: 628, loss: 0.008333776146173477\n",
      "epoch: 4, batch: 629, loss: 0.06510186940431595\n",
      "epoch: 4, batch: 630, loss: 0.01059208158403635\n",
      "epoch: 4, batch: 631, loss: 0.08046702295541763\n",
      "epoch: 4, batch: 632, loss: 0.05216263607144356\n",
      "epoch: 4, batch: 633, loss: 0.00865738745778799\n",
      "epoch: 4, batch: 634, loss: 0.1055544838309288\n",
      "epoch: 4, batch: 635, loss: 0.005686867516487837\n",
      "epoch: 4, batch: 636, loss: 0.12643714249134064\n",
      "epoch: 4, batch: 637, loss: 0.02164916694164276\n",
      "epoch: 4, batch: 638, loss: 0.010357001796364784\n",
      "epoch: 4, batch: 639, loss: 0.07411342859268188\n",
      "epoch: 4, batch: 640, loss: 0.264690637588501\n",
      "epoch: 4, batch: 641, loss: 0.04248344898223877\n",
      "epoch: 4, batch: 642, loss: 0.010753943584859371\n",
      "epoch: 4, batch: 643, loss: 0.08548258244991302\n",
      "epoch: 4, batch: 644, loss: 0.068931445479393\n",
      "epoch: 4, batch: 645, loss: 0.08227293938398361\n",
      "epoch: 4, batch: 646, loss: 0.1661178022623062\n",
      "epoch: 4, batch: 647, loss: 0.10246466100215912\n",
      "epoch: 4, batch: 648, loss: 0.0784134715795517\n",
      "epoch: 4, batch: 649, loss: 0.20437650382518768\n",
      "epoch: 4, batch: 650, loss: 0.033681366592645645\n",
      "epoch: 4, batch: 651, loss: 0.20402970910072327\n",
      "epoch: 4, batch: 652, loss: 0.05378701537847519\n",
      "epoch: 4, batch: 653, loss: 0.010169642977416515\n",
      "epoch: 4, batch: 654, loss: 0.0031626152340322733\n",
      "epoch: 4, batch: 655, loss: 0.011730832047760487\n",
      "epoch: 4, batch: 656, loss: 0.030474206432700157\n",
      "epoch: 4, batch: 657, loss: 0.026000719517469406\n",
      "epoch: 4, batch: 658, loss: 0.008190040476620197\n",
      "epoch: 4, batch: 659, loss: 0.01958715356886387\n",
      "epoch: 4, batch: 660, loss: 0.07231388241052628\n",
      "epoch: 4, batch: 661, loss: 0.030347047373652458\n",
      "epoch: 4, batch: 662, loss: 0.05271990969777107\n",
      "epoch: 4, batch: 663, loss: 0.04598629102110863\n",
      "epoch: 4, batch: 664, loss: 0.005565954372286797\n",
      "epoch: 4, batch: 665, loss: 0.013374381698668003\n",
      "epoch: 4, batch: 666, loss: 0.14300264418125153\n",
      "epoch: 4, batch: 667, loss: 0.01564161852002144\n",
      "epoch: 4, batch: 668, loss: 0.08212106674909592\n",
      "epoch: 4, batch: 669, loss: 0.0173488836735487\n",
      "epoch: 4, batch: 670, loss: 0.012649022042751312\n",
      "epoch: 4, batch: 671, loss: 0.06582792848348618\n",
      "epoch: 4, batch: 672, loss: 0.1502666473388672\n",
      "epoch: 4, batch: 673, loss: 0.013683436438441277\n",
      "epoch: 4, batch: 674, loss: 0.0497182197868824\n",
      "epoch: 4, batch: 675, loss: 0.029906177893280983\n",
      "epoch: 4, batch: 676, loss: 0.11741863191127777\n",
      "epoch: 4, batch: 677, loss: 0.058011043816804886\n",
      "epoch: 4, batch: 678, loss: 0.09834093600511551\n",
      "epoch: 4, batch: 679, loss: 0.285590797662735\n",
      "epoch: 4, batch: 680, loss: 0.02033516764640808\n",
      "epoch: 4, batch: 681, loss: 0.005846004467457533\n",
      "epoch: 4, batch: 682, loss: 0.0773322805762291\n",
      "epoch: 4, batch: 683, loss: 0.041198376566171646\n",
      "epoch: 4, batch: 684, loss: 0.02571231499314308\n",
      "epoch: 4, batch: 685, loss: 0.025665931403636932\n",
      "epoch: 4, batch: 686, loss: 0.0705082044005394\n",
      "epoch: 4, batch: 687, loss: 0.030920755118131638\n",
      "epoch: 4, batch: 688, loss: 0.0120247108861804\n",
      "epoch: 4, batch: 689, loss: 0.05133071169257164\n",
      "epoch: 4, batch: 690, loss: 0.017114978283643723\n",
      "epoch: 4, batch: 691, loss: 0.004333280958235264\n",
      "epoch: 4, batch: 692, loss: 0.052488379180431366\n",
      "epoch: 4, batch: 693, loss: 0.1169772669672966\n",
      "epoch: 4, batch: 694, loss: 0.05067354440689087\n",
      "epoch: 4, batch: 695, loss: 0.21297244727611542\n",
      "epoch: 4, batch: 696, loss: 0.04239071160554886\n",
      "epoch: 4, batch: 697, loss: 0.05051407963037491\n",
      "epoch: 4, batch: 698, loss: 0.23236162960529327\n",
      "epoch: 4, batch: 699, loss: 0.03404605761170387\n",
      "epoch: 4, batch: 700, loss: 0.05474754422903061\n",
      "epoch: 4, batch: 701, loss: 0.008806942962110043\n",
      "epoch: 4, batch: 702, loss: 0.02700362540781498\n",
      "epoch: 4, batch: 703, loss: 0.010417324490845203\n",
      "epoch: 4, batch: 704, loss: 0.04328010231256485\n",
      "epoch: 4, batch: 705, loss: 0.04834631830453873\n",
      "epoch: 4, batch: 706, loss: 0.011651347391307354\n",
      "epoch: 4, batch: 707, loss: 0.11013816297054291\n",
      "epoch: 4, batch: 708, loss: 0.0054052104242146015\n",
      "epoch: 4, batch: 709, loss: 0.0971589982509613\n",
      "epoch: 4, batch: 710, loss: 0.007683511357754469\n",
      "epoch: 4, batch: 711, loss: 0.14411665499210358\n",
      "epoch: 4, batch: 712, loss: 0.12872962653636932\n",
      "epoch: 4, batch: 713, loss: 0.02837093360722065\n",
      "epoch: 4, batch: 714, loss: 0.01232611108571291\n",
      "epoch: 4, batch: 715, loss: 0.2436051070690155\n",
      "epoch: 4, batch: 716, loss: 0.10933992266654968\n",
      "epoch: 4, batch: 717, loss: 0.010780162177979946\n",
      "epoch: 4, batch: 718, loss: 0.020544826984405518\n",
      "epoch: 4, batch: 719, loss: 0.0611591711640358\n",
      "epoch: 4, batch: 720, loss: 0.07925718277692795\n",
      "epoch: 4, batch: 721, loss: 0.09924094378948212\n",
      "epoch: 4, batch: 722, loss: 0.08526558429002762\n",
      "epoch: 4, batch: 723, loss: 0.13004806637763977\n",
      "epoch: 4, batch: 724, loss: 0.08924274146556854\n",
      "epoch: 4, batch: 725, loss: 0.03990738093852997\n",
      "epoch: 4, batch: 726, loss: 0.05594849959015846\n",
      "epoch: 4, batch: 727, loss: 0.029321415349841118\n",
      "epoch: 4, batch: 728, loss: 0.034395135939121246\n",
      "epoch: 4, batch: 729, loss: 0.00603343453258276\n",
      "epoch: 4, batch: 730, loss: 0.031441546976566315\n",
      "epoch: 4, batch: 731, loss: 0.08402710407972336\n",
      "epoch: 4, batch: 732, loss: 0.028619207441806793\n",
      "epoch: 4, batch: 733, loss: 0.06060106307268143\n",
      "epoch: 4, batch: 734, loss: 0.06490666419267654\n",
      "epoch: 4, batch: 735, loss: 0.056096214801073074\n",
      "epoch: 4, batch: 736, loss: 0.0449661985039711\n",
      "epoch: 4, batch: 737, loss: 0.019189786165952682\n",
      "epoch: 4, batch: 738, loss: 0.01865469291806221\n",
      "epoch: 4, batch: 739, loss: 0.0603286474943161\n",
      "epoch: 4, batch: 740, loss: 0.011267284862697124\n",
      "epoch: 4, batch: 741, loss: 0.0759577602148056\n",
      "epoch: 4, batch: 742, loss: 0.05903620272874832\n",
      "epoch: 4, batch: 743, loss: 0.06019454449415207\n",
      "epoch: 4, batch: 744, loss: 0.01074659638106823\n",
      "epoch: 4, batch: 745, loss: 0.00544144818559289\n",
      "epoch: 4, batch: 746, loss: 0.012485247105360031\n",
      "epoch: 4, batch: 747, loss: 0.027385178953409195\n",
      "epoch: 4, batch: 748, loss: 0.02025292068719864\n",
      "epoch: 4, batch: 749, loss: 0.03996950760483742\n",
      "epoch: 4, batch: 750, loss: 0.16365909576416016\n",
      "epoch: 4, batch: 751, loss: 0.019379734992980957\n",
      "epoch: 4, batch: 752, loss: 0.005212243180721998\n",
      "epoch: 4, batch: 753, loss: 0.043679654598236084\n",
      "epoch: 4, batch: 754, loss: 0.0064461687579751015\n",
      "epoch: 4, batch: 755, loss: 0.02402304671704769\n",
      "epoch: 4, batch: 756, loss: 0.06909447908401489\n",
      "epoch: 4, batch: 757, loss: 0.029248354956507683\n",
      "epoch: 4, batch: 758, loss: 0.027520885691046715\n",
      "epoch: 4, batch: 759, loss: 0.09457428008317947\n",
      "epoch: 4, batch: 760, loss: 0.02012374810874462\n",
      "epoch: 4, batch: 761, loss: 0.041748009622097015\n",
      "epoch: 4, batch: 762, loss: 0.12316472083330154\n",
      "epoch: 4, batch: 763, loss: 0.0025043408386409283\n",
      "epoch: 4, batch: 764, loss: 0.02895207889378071\n",
      "epoch: 4, batch: 765, loss: 0.06466438621282578\n",
      "epoch: 4, batch: 766, loss: 0.046792034059762955\n",
      "epoch: 4, batch: 767, loss: 0.05343583971261978\n",
      "epoch: 4, batch: 768, loss: 0.012435637414455414\n",
      "epoch: 4, batch: 769, loss: 0.0645446926355362\n",
      "epoch: 4, batch: 770, loss: 0.001557952375151217\n",
      "epoch: 4, batch: 771, loss: 0.024997031316161156\n",
      "epoch: 4, batch: 772, loss: 0.017936313524842262\n",
      "epoch: 4, batch: 773, loss: 0.019369568675756454\n",
      "epoch: 4, batch: 774, loss: 0.04464909806847572\n",
      "epoch: 4, batch: 775, loss: 0.003022523829713464\n",
      "epoch: 4, batch: 776, loss: 0.05175076052546501\n",
      "epoch: 4, batch: 777, loss: 0.040412694215774536\n",
      "epoch: 4, batch: 778, loss: 0.06736701726913452\n",
      "epoch: 4, batch: 779, loss: 0.058506473898887634\n",
      "epoch: 4, batch: 780, loss: 0.0418880432844162\n",
      "epoch: 4, batch: 781, loss: 0.04719927906990051\n",
      "epoch: 4, batch: 782, loss: 0.08602121472358704\n",
      "epoch: 4, batch: 783, loss: 0.012932398356497288\n",
      "epoch: 4, batch: 784, loss: 0.15042127668857574\n",
      "epoch: 4, batch: 785, loss: 0.037713196128606796\n",
      "epoch: 4, batch: 786, loss: 0.17950911819934845\n",
      "epoch: 4, batch: 787, loss: 0.018788669258356094\n",
      "epoch: 4, batch: 788, loss: 0.022440779954195023\n",
      "epoch: 4, batch: 789, loss: 0.0848759189248085\n",
      "epoch: 4, batch: 790, loss: 0.016664868220686913\n",
      "epoch: 4, batch: 791, loss: 0.025996459648013115\n",
      "epoch: 4, batch: 792, loss: 0.012395553290843964\n",
      "epoch: 4, batch: 793, loss: 0.028097078204154968\n",
      "epoch: 4, batch: 794, loss: 0.04724115878343582\n",
      "epoch: 4, batch: 795, loss: 0.03259892389178276\n",
      "epoch: 4, batch: 796, loss: 0.09326843172311783\n",
      "epoch: 4, batch: 797, loss: 0.1278941035270691\n",
      "epoch: 4, batch: 798, loss: 0.004345054272562265\n",
      "epoch: 4, batch: 799, loss: 0.006162081845104694\n",
      "epoch: 4, batch: 800, loss: 0.037820734083652496\n",
      "epoch: 4, batch: 801, loss: 0.058219797909259796\n",
      "epoch: 4, batch: 802, loss: 0.01622695103287697\n",
      "epoch: 4, batch: 803, loss: 0.04825347289443016\n",
      "epoch: 4, batch: 804, loss: 0.038984376937150955\n",
      "epoch: 4, batch: 805, loss: 0.08117441087961197\n",
      "epoch: 4, batch: 806, loss: 0.03994640335440636\n",
      "epoch: 4, batch: 807, loss: 0.013192151673138142\n",
      "epoch: 4, batch: 808, loss: 0.008799402974545956\n",
      "epoch: 4, batch: 809, loss: 0.1127871572971344\n",
      "epoch: 4, batch: 810, loss: 0.1135304868221283\n",
      "epoch: 4, batch: 811, loss: 0.0022016072180122137\n",
      "epoch: 4, batch: 812, loss: 0.040765199810266495\n",
      "epoch: 4, batch: 813, loss: 0.09235870838165283\n",
      "epoch: 4, batch: 814, loss: 0.01503174938261509\n",
      "epoch: 4, batch: 815, loss: 0.09086285531520844\n",
      "epoch: 4, batch: 816, loss: 0.0761742889881134\n",
      "epoch: 4, batch: 817, loss: 0.032412268221378326\n",
      "epoch: 4, batch: 818, loss: 0.014170208014547825\n",
      "epoch: 4, batch: 819, loss: 0.038975246250629425\n",
      "epoch: 4, batch: 820, loss: 0.02337777242064476\n",
      "epoch: 4, batch: 821, loss: 0.040733590722084045\n",
      "epoch: 4, batch: 822, loss: 0.03698835149407387\n",
      "epoch: 4, batch: 823, loss: 0.015055783092975616\n",
      "epoch: 4, batch: 824, loss: 0.0021384241990745068\n",
      "epoch: 4, batch: 825, loss: 0.025425445288419724\n",
      "epoch: 4, batch: 826, loss: 0.08395568281412125\n",
      "epoch: 4, batch: 827, loss: 0.09874049574136734\n",
      "epoch: 4, batch: 828, loss: 0.014695939607918262\n",
      "epoch: 4, batch: 829, loss: 0.01770232617855072\n",
      "epoch: 4, batch: 830, loss: 0.013594267889857292\n",
      "epoch: 4, batch: 831, loss: 0.01917240023612976\n",
      "epoch: 4, batch: 832, loss: 0.002809203928336501\n",
      "epoch: 4, batch: 833, loss: 0.06272511929273605\n",
      "epoch: 4, batch: 834, loss: 0.07098708301782608\n",
      "epoch: 4, batch: 835, loss: 0.10354755073785782\n",
      "epoch: 4, batch: 836, loss: 0.0522671677172184\n",
      "epoch: 4, batch: 837, loss: 0.010266984812915325\n",
      "epoch: 4, batch: 838, loss: 0.07102327048778534\n",
      "epoch: 4, batch: 839, loss: 0.04301321879029274\n",
      "epoch: 4, batch: 840, loss: 0.05479862913489342\n",
      "epoch: 4, batch: 841, loss: 0.011626590974628925\n",
      "epoch: 4, batch: 842, loss: 0.026804618537425995\n",
      "epoch: 4, batch: 843, loss: 0.02475925348699093\n",
      "epoch: 4, batch: 844, loss: 0.03767113387584686\n",
      "epoch: 4, batch: 845, loss: 0.052535280585289\n",
      "epoch: 4, batch: 846, loss: 0.21341341733932495\n",
      "epoch: 4, batch: 847, loss: 0.11215360462665558\n",
      "epoch: 4, batch: 848, loss: 0.014201861806213856\n",
      "epoch: 4, batch: 849, loss: 0.050394780933856964\n",
      "epoch: 4, batch: 850, loss: 0.03173452615737915\n",
      "epoch: 4, batch: 851, loss: 0.049935147166252136\n",
      "epoch: 4, batch: 852, loss: 0.11184217035770416\n",
      "epoch: 4, batch: 853, loss: 0.061764080077409744\n",
      "epoch: 4, batch: 854, loss: 0.0230695903301239\n",
      "epoch: 4, batch: 855, loss: 0.11391612142324448\n",
      "epoch: 4, batch: 856, loss: 0.028919681906700134\n",
      "epoch: 4, batch: 857, loss: 0.03843425214290619\n",
      "epoch: 4, batch: 858, loss: 0.017254363745450974\n",
      "epoch: 4, batch: 859, loss: 0.019587310031056404\n",
      "epoch: 4, batch: 860, loss: 0.01604396663606167\n",
      "epoch: 4, batch: 861, loss: 0.031754568219184875\n",
      "epoch: 4, batch: 862, loss: 0.004950849339365959\n",
      "epoch: 4, batch: 863, loss: 0.11683951318264008\n",
      "epoch: 4, batch: 864, loss: 0.06981190294027328\n",
      "epoch: 4, batch: 865, loss: 0.03265492618083954\n",
      "epoch: 4, batch: 866, loss: 0.02638484165072441\n",
      "epoch: 4, batch: 867, loss: 0.06880738586187363\n",
      "epoch: 4, batch: 868, loss: 0.08063224703073502\n",
      "epoch: 4, batch: 869, loss: 0.08588407933712006\n",
      "epoch: 4, batch: 870, loss: 0.01521972194314003\n",
      "epoch: 4, batch: 871, loss: 0.0017877009231597185\n",
      "epoch: 4, batch: 872, loss: 0.03449510782957077\n",
      "epoch: 4, batch: 873, loss: 0.026759767904877663\n",
      "epoch: 4, batch: 874, loss: 0.04476170614361763\n",
      "epoch: 4, batch: 875, loss: 0.029866812750697136\n",
      "epoch: 4, batch: 876, loss: 0.16610078513622284\n",
      "epoch: 4, batch: 877, loss: 0.037816885858774185\n",
      "epoch: 4, batch: 878, loss: 0.03162465617060661\n",
      "epoch: 4, batch: 879, loss: 0.02042463980615139\n",
      "epoch: 4, batch: 880, loss: 0.027857476845383644\n",
      "epoch: 4, batch: 881, loss: 0.01870344579219818\n",
      "epoch: 4, batch: 882, loss: 0.02279585413634777\n",
      "epoch: 4, batch: 883, loss: 0.028233462944626808\n",
      "epoch: 4, batch: 884, loss: 0.010042949579656124\n",
      "epoch: 4, batch: 885, loss: 0.0960623100399971\n",
      "epoch: 4, batch: 886, loss: 0.011185998097062111\n",
      "epoch: 4, batch: 887, loss: 0.07646486908197403\n",
      "epoch: 4, batch: 888, loss: 0.01747528836131096\n",
      "epoch: 4, batch: 889, loss: 0.03366698697209358\n",
      "epoch: 4, batch: 890, loss: 0.07829097658395767\n",
      "epoch: 4, batch: 891, loss: 0.021417956799268723\n",
      "epoch: 4, batch: 892, loss: 0.015016010962426662\n",
      "epoch: 4, batch: 893, loss: 0.06724701076745987\n",
      "epoch: 4, batch: 894, loss: 0.01384737715125084\n",
      "epoch: 4, batch: 895, loss: 0.027215102687478065\n",
      "epoch: 4, batch: 896, loss: 0.00869612954556942\n",
      "epoch: 4, batch: 897, loss: 0.08968937397003174\n",
      "epoch: 4, batch: 898, loss: 0.06879658997058868\n",
      "epoch: 4, batch: 899, loss: 0.042355556041002274\n",
      "epoch: 4, batch: 900, loss: 0.01275235041975975\n",
      "epoch: 4, batch: 901, loss: 0.04206172004342079\n",
      "epoch: 4, batch: 902, loss: 0.018101099878549576\n",
      "epoch: 4, batch: 903, loss: 0.008426987566053867\n",
      "epoch: 4, batch: 904, loss: 0.05632380023598671\n",
      "epoch: 4, batch: 905, loss: 0.030761538073420525\n",
      "epoch: 4, batch: 906, loss: 0.08780108392238617\n",
      "epoch: 4, batch: 907, loss: 0.012844701297581196\n",
      "epoch: 4, batch: 908, loss: 0.014057159423828125\n",
      "epoch: 4, batch: 909, loss: 0.014080836437642574\n",
      "epoch: 4, batch: 910, loss: 0.01743636094033718\n",
      "epoch: 4, batch: 911, loss: 0.05110714212059975\n",
      "epoch: 4, batch: 912, loss: 0.04545276239514351\n",
      "epoch: 4, batch: 913, loss: 0.021260391920804977\n",
      "epoch: 4, batch: 914, loss: 0.058680664747953415\n",
      "epoch: 4, batch: 915, loss: 0.09821978211402893\n",
      "epoch: 4, batch: 916, loss: 0.11620578169822693\n",
      "epoch: 4, batch: 917, loss: 0.0651322677731514\n",
      "epoch: 4, batch: 918, loss: 0.030460907146334648\n",
      "epoch: 4, batch: 919, loss: 0.015763157978653908\n",
      "epoch: 4, batch: 920, loss: 0.004600036423653364\n",
      "epoch: 4, batch: 921, loss: 0.033400870859622955\n",
      "epoch: 4, batch: 922, loss: 0.018698591738939285\n",
      "epoch: 4, batch: 923, loss: 0.037616629153490067\n",
      "epoch: 4, batch: 924, loss: 0.04469527304172516\n",
      "epoch: 4, batch: 925, loss: 0.08214538544416428\n",
      "epoch: 4, batch: 926, loss: 0.007121487986296415\n",
      "epoch: 4, batch: 927, loss: 0.020625006407499313\n",
      "epoch: 4, batch: 928, loss: 0.05279620364308357\n",
      "epoch: 4, batch: 929, loss: 0.03461062163114548\n",
      "epoch: 4, batch: 930, loss: 0.017552779987454414\n",
      "epoch: 4, batch: 931, loss: 0.04494157433509827\n",
      "epoch: 4, batch: 932, loss: 0.013270881958305836\n",
      "epoch: 4, batch: 933, loss: 0.09978147596120834\n",
      "epoch: 4, batch: 934, loss: 0.09942420572042465\n",
      "epoch: 4, batch: 935, loss: 0.06647831201553345\n",
      "epoch: 4, batch: 936, loss: 0.017587264999747276\n",
      "epoch: 4, batch: 937, loss: 0.007125232368707657\n",
      "CPU times: total: 42.3 s\n",
      "Wall time: 34.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# get model to cuda if possible\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, target) in enumerate((train_datloader)):\n",
    "\n",
    "        # get data to cuda if possible\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # feed forward the data to model\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, target)\n",
    "        print(f\"epoch: {epoch}, batch: {batch_idx}, loss: {loss}\")\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad() # This will flush the gradients from the last iteration\n",
    "        loss.backward()\n",
    "\n",
    "        # optimise the loss (gradient descent or Adam step)\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check accuracy on train and test data (Validate model accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy on training & test to see how good our model\n",
    "def check_accuracy(loader, model):\n",
    "    \"\"\"\n",
    "    Check accuracy of our trained model given a loader and a model\n",
    "\n",
    "    Parameters:\n",
    "        loader: torch.utils.data.DataLoader\n",
    "            A loader for the dataset you want to check accuracy on\n",
    "        model: nn.Module\n",
    "            The model you want to check accuracy on\n",
    "\n",
    "    Returns:\n",
    "        acc: float\n",
    "            The accuracy of the model on the dataset given by the loader\n",
    "    \"\"\"\n",
    "\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "\n",
    "    # We don't need to keep track of gradients here so we wrap it in torch.no_grad()\n",
    "    with torch.no_grad():\n",
    "        # Loop through the data\n",
    "        for x, y in loader:\n",
    "            # Move data to device\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "\n",
    "            # Forward pass\n",
    "            scores = model(x)\n",
    "            predictions = scores.argmax(1)\n",
    "\n",
    "            # Check how many we got correct\n",
    "            num_correct += (predictions == y).sum()\n",
    "\n",
    "            # Keep track of number of samples\n",
    "            num_samples += predictions.size(0)\n",
    "\n",
    "    model.train()\n",
    "    return num_correct / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 98.66\n",
      "Accuracy on test set: 98.38\n"
     ]
    }
   ],
   "source": [
    "# Check accuracy on training & test to see how good our model\n",
    "print(f\"Accuracy on training set: {check_accuracy(train_datloader, model)*100:.2f}\")\n",
    "print(f\"Accuracy on test set: {check_accuracy(test_datloader, model)*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
