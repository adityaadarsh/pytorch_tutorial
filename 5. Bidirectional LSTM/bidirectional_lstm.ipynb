{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transformations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Bidirectional Lstm Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLSTM(nn.Module):\n",
    "    def __init__(self, imp_emb_dim, hidden_units, n_layers, output_classes):\n",
    "        super(BLSTM, self).__init__()\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_units = hidden_units\n",
    "\n",
    "        self.bilstm = nn.LSTM(input_size=imp_emb_dim,\n",
    "                               hidden_size=hidden_units,\n",
    "                               num_layers=n_layers,\n",
    "                               batch_first=True,\n",
    "                               bidirectional=True)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_units*imp_emb_dim*2, output_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Initialize the hidden state and cell state first for bidrectional lstm\n",
    "        h0 = torch.zeros(self.n_layers*2, x.size(0), self.hidden_units).to(device)\n",
    "        c0 = torch.zeros(self.n_layers*2, x.size(0), self.hidden_units).to(device)\n",
    "        \n",
    "        # Forward Propagation\n",
    "        out, _ = self.bilstm(x, (h0, c0))\n",
    "        out_flatten = torch.flatten(out, 1,-1)\n",
    "        x = self.fc(out_flatten)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "# check the network graph\n",
    "model = BLSTM(28, 5, 10, 10)\n",
    "\n",
    "# create a random variable and pass it to the model to check the network graph\n",
    "device = 'cpu'\n",
    "x = torch.randn(64, 28, 28)\n",
    "h0 = torch.zeros(10*2, x.size(0), 5).to(device)\n",
    "c0 = torch.zeros(10*2, x.size(0), 5).to(device)\n",
    "\n",
    "\n",
    "print(x.shape)\n",
    "print(model(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undersatnding RNN layer\n",
    "n_layer = 10\n",
    "hidden_size = 5\n",
    "emb_dim = 28\n",
    "batch_size = 64\n",
    "max_seq_length = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_cell = nn.LSTM(input_size=emb_dim,\n",
    "                               hidden_size=hidden_size,\n",
    "                               num_layers=n_layer,\n",
    "                               batch_first=True,\n",
    "                               bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 12, 28])\n",
      "torch.Size([20, 64, 5])\n",
      "torch.Size([20, 64, 5])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create a random variable and pass it to the model to check the network graph\n",
    "x = torch.randn(batch_size, max_seq_length, emb_dim)\n",
    "print(x.shape)\n",
    "\n",
    "hidden_state = torch.zeros(n_layer*2, batch_size, hidden_size)\n",
    "cell_state = torch.zeros(n_layer*2, batch_size, hidden_size)\n",
    "print(hidden_state.shape)\n",
    "print(cell_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_cell_output = lstm_cell(x, (hidden_state, cell_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch_first=True\n",
    "len(lstm_cell_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 12, 10])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_cell_output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([20, 64, 5]), torch.Size([20, 64, 5]))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hidden gate output , cell gate output\n",
    "lstm_cell_output[1][0].shape, lstm_cell_output[1][1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_class = 10\n",
    "n_layer = 10\n",
    "hidden_size = 5\n",
    "emb_dim = 28\n",
    "batch_size = 64\n",
    "max_seq_length = 12\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and load the data from pytorch sample datasets\n",
    "# https://pytorch.org/vision/0.8/datasets.html\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    root=\"../dataset/\", train=True, transform=transformations.ToTensor(), download=True\n",
    ")\n",
    "train_datloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels batch shape: torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 8\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Understand the dataloader\n",
    "plt.title(\"sample image\")\n",
    "# Display image and label.\n",
    "train_features, train_labels = next(iter(train_datloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset shape: torch.Size([60000, 28, 28])\n",
      "['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n"
     ]
    }
   ],
   "source": [
    "# train data shape and class labels\n",
    "print(\"train_dataset shape:\", train_dataset.data.shape)\n",
    "print(train_dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_dataset shape: torch.Size([10000, 28, 28])\n",
      "['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n"
     ]
    }
   ],
   "source": [
    "test_dataset = datasets.MNIST(\n",
    "    root=\"../dataset/\", train=False, transform=transformations.ToTensor(), download=True\n",
    ")\n",
    "test_datloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(\"test_dataset shape:\", test_dataset.data.shape)\n",
    "print(test_dataset.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLSTM(\n",
      "  (bilstm): LSTM(28, 5, num_layers=10, batch_first=True, bidirectional=True)\n",
      "  (fc): Linear(in_features=280, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = BLSTM(emb_dim, hidden_size, n_layer, n_class)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise Loss And Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps to train the model\n",
    "1. For each epoch, iterate through the batch\n",
    "2. For each batch\n",
    "    * feed forward the input and target data of train to the model\n",
    "    * Calculate the loss and score\n",
    "    * Backpropogate the loss\n",
    "    * optimise the loss using optimiser() (gradient descent is one such optimiser) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch: 0, loss: 2.3142149448394775\n",
      "epoch: 0, batch: 1, loss: 2.3088390827178955\n",
      "epoch: 0, batch: 2, loss: 2.299330472946167\n",
      "epoch: 0, batch: 3, loss: 2.323676824569702\n",
      "epoch: 0, batch: 4, loss: 2.2995622158050537\n",
      "epoch: 0, batch: 5, loss: 2.2955610752105713\n",
      "epoch: 0, batch: 6, loss: 2.309849500656128\n",
      "epoch: 0, batch: 7, loss: 2.2897377014160156\n",
      "epoch: 0, batch: 8, loss: 2.3218023777008057\n",
      "epoch: 0, batch: 9, loss: 2.322451114654541\n",
      "epoch: 0, batch: 10, loss: 2.299671173095703\n",
      "epoch: 0, batch: 11, loss: 2.3158295154571533\n",
      "epoch: 0, batch: 12, loss: 2.2864468097686768\n",
      "epoch: 0, batch: 13, loss: 2.3097639083862305\n",
      "epoch: 0, batch: 14, loss: 2.303290843963623\n",
      "epoch: 0, batch: 15, loss: 2.3074047565460205\n",
      "epoch: 0, batch: 16, loss: 2.2953696250915527\n",
      "epoch: 0, batch: 17, loss: 2.3127670288085938\n",
      "epoch: 0, batch: 18, loss: 2.2971296310424805\n",
      "epoch: 0, batch: 19, loss: 2.3017539978027344\n",
      "epoch: 0, batch: 20, loss: 2.2989888191223145\n",
      "epoch: 0, batch: 21, loss: 2.30903959274292\n",
      "epoch: 0, batch: 22, loss: 2.3016092777252197\n",
      "epoch: 0, batch: 23, loss: 2.303683280944824\n",
      "epoch: 0, batch: 24, loss: 2.3198788166046143\n",
      "epoch: 0, batch: 25, loss: 2.298917770385742\n",
      "epoch: 0, batch: 26, loss: 2.3060572147369385\n",
      "epoch: 0, batch: 27, loss: 2.302618980407715\n",
      "epoch: 0, batch: 28, loss: 2.301387071609497\n",
      "epoch: 0, batch: 29, loss: 2.3086647987365723\n",
      "epoch: 0, batch: 30, loss: 2.309022903442383\n",
      "epoch: 0, batch: 31, loss: 2.30499267578125\n",
      "epoch: 0, batch: 32, loss: 2.3082616329193115\n",
      "epoch: 0, batch: 33, loss: 2.2889342308044434\n",
      "epoch: 0, batch: 34, loss: 2.291620969772339\n",
      "epoch: 0, batch: 35, loss: 2.2952561378479004\n",
      "epoch: 0, batch: 36, loss: 2.295879364013672\n",
      "epoch: 0, batch: 37, loss: 2.31994891166687\n",
      "epoch: 0, batch: 38, loss: 2.2986092567443848\n",
      "epoch: 0, batch: 39, loss: 2.2842047214508057\n",
      "epoch: 0, batch: 40, loss: 2.3007986545562744\n",
      "epoch: 0, batch: 41, loss: 2.3126394748687744\n",
      "epoch: 0, batch: 42, loss: 2.3004369735717773\n",
      "epoch: 0, batch: 43, loss: 2.2826414108276367\n",
      "epoch: 0, batch: 44, loss: 2.3050286769866943\n",
      "epoch: 0, batch: 45, loss: 2.2917189598083496\n",
      "epoch: 0, batch: 46, loss: 2.311251401901245\n",
      "epoch: 0, batch: 47, loss: 2.316526174545288\n",
      "epoch: 0, batch: 48, loss: 2.313974618911743\n",
      "epoch: 0, batch: 49, loss: 2.276486873626709\n",
      "epoch: 0, batch: 50, loss: 2.3046891689300537\n",
      "epoch: 0, batch: 51, loss: 2.3114407062530518\n",
      "epoch: 0, batch: 52, loss: 2.2912967205047607\n",
      "epoch: 0, batch: 53, loss: 2.3331656455993652\n",
      "epoch: 0, batch: 54, loss: 2.306593418121338\n",
      "epoch: 0, batch: 55, loss: 2.2999472618103027\n",
      "epoch: 0, batch: 56, loss: 2.297078847885132\n",
      "epoch: 0, batch: 57, loss: 2.319566249847412\n",
      "epoch: 0, batch: 58, loss: 2.292661190032959\n",
      "epoch: 0, batch: 59, loss: 2.3108253479003906\n",
      "epoch: 0, batch: 60, loss: 2.310943603515625\n",
      "epoch: 0, batch: 61, loss: 2.3230645656585693\n",
      "epoch: 0, batch: 62, loss: 2.3087656497955322\n",
      "epoch: 0, batch: 63, loss: 2.3129777908325195\n",
      "epoch: 0, batch: 64, loss: 2.2960193157196045\n",
      "epoch: 0, batch: 65, loss: 2.30485463142395\n",
      "epoch: 0, batch: 66, loss: 2.3078739643096924\n",
      "epoch: 0, batch: 67, loss: 2.298741102218628\n",
      "epoch: 0, batch: 68, loss: 2.308445930480957\n",
      "epoch: 0, batch: 69, loss: 2.3117499351501465\n",
      "epoch: 0, batch: 70, loss: 2.3005645275115967\n",
      "epoch: 0, batch: 71, loss: 2.312319040298462\n",
      "epoch: 0, batch: 72, loss: 2.3055899143218994\n",
      "epoch: 0, batch: 73, loss: 2.3079299926757812\n",
      "epoch: 0, batch: 74, loss: 2.310760021209717\n",
      "epoch: 0, batch: 75, loss: 2.2984609603881836\n",
      "epoch: 0, batch: 76, loss: 2.3018527030944824\n",
      "epoch: 0, batch: 77, loss: 2.29030704498291\n",
      "epoch: 0, batch: 78, loss: 2.313143014907837\n",
      "epoch: 0, batch: 79, loss: 2.3083508014678955\n",
      "epoch: 0, batch: 80, loss: 2.308509588241577\n",
      "epoch: 0, batch: 81, loss: 2.2914974689483643\n",
      "epoch: 0, batch: 82, loss: 2.303607940673828\n",
      "epoch: 0, batch: 83, loss: 2.3004395961761475\n",
      "epoch: 0, batch: 84, loss: 2.304506540298462\n",
      "epoch: 0, batch: 85, loss: 2.310107707977295\n",
      "epoch: 0, batch: 86, loss: 2.312201976776123\n",
      "epoch: 0, batch: 87, loss: 2.3116989135742188\n",
      "epoch: 0, batch: 88, loss: 2.3043437004089355\n",
      "epoch: 0, batch: 89, loss: 2.291466474533081\n",
      "epoch: 0, batch: 90, loss: 2.3066558837890625\n",
      "epoch: 0, batch: 91, loss: 2.3022477626800537\n",
      "epoch: 0, batch: 92, loss: 2.3051841259002686\n",
      "epoch: 0, batch: 93, loss: 2.3112680912017822\n",
      "epoch: 0, batch: 94, loss: 2.306037664413452\n",
      "epoch: 0, batch: 95, loss: 2.304147243499756\n",
      "epoch: 0, batch: 96, loss: 2.3052875995635986\n",
      "epoch: 0, batch: 97, loss: 2.299793004989624\n",
      "epoch: 0, batch: 98, loss: 2.3068387508392334\n",
      "epoch: 0, batch: 99, loss: 2.298656463623047\n",
      "epoch: 0, batch: 100, loss: 2.3062050342559814\n",
      "epoch: 0, batch: 101, loss: 2.2988569736480713\n",
      "epoch: 0, batch: 102, loss: 2.2886383533477783\n",
      "epoch: 0, batch: 103, loss: 2.3103015422821045\n",
      "epoch: 0, batch: 104, loss: 2.300368309020996\n",
      "epoch: 0, batch: 105, loss: 2.2843337059020996\n",
      "epoch: 0, batch: 106, loss: 2.297370433807373\n",
      "epoch: 0, batch: 107, loss: 2.3015053272247314\n",
      "epoch: 0, batch: 108, loss: 2.325821876525879\n",
      "epoch: 0, batch: 109, loss: 2.314352512359619\n",
      "epoch: 0, batch: 110, loss: 2.279677152633667\n",
      "epoch: 0, batch: 111, loss: 2.3025388717651367\n",
      "epoch: 0, batch: 112, loss: 2.289644479751587\n",
      "epoch: 0, batch: 113, loss: 2.3037705421447754\n",
      "epoch: 0, batch: 114, loss: 2.297973155975342\n",
      "epoch: 0, batch: 115, loss: 2.299618721008301\n",
      "epoch: 0, batch: 116, loss: 2.2909631729125977\n",
      "epoch: 0, batch: 117, loss: 2.289017677307129\n",
      "epoch: 0, batch: 118, loss: 2.2858505249023438\n",
      "epoch: 0, batch: 119, loss: 2.2951204776763916\n",
      "epoch: 0, batch: 120, loss: 2.2771332263946533\n",
      "epoch: 0, batch: 121, loss: 2.281435966491699\n",
      "epoch: 0, batch: 122, loss: 2.255232095718384\n",
      "epoch: 0, batch: 123, loss: 2.2566256523132324\n",
      "epoch: 0, batch: 124, loss: 2.2589261531829834\n",
      "epoch: 0, batch: 125, loss: 2.2411296367645264\n",
      "epoch: 0, batch: 126, loss: 2.2448298931121826\n",
      "epoch: 0, batch: 127, loss: 2.25290584564209\n",
      "epoch: 0, batch: 128, loss: 2.2234277725219727\n",
      "epoch: 0, batch: 129, loss: 2.212869167327881\n",
      "epoch: 0, batch: 130, loss: 2.1666457653045654\n",
      "epoch: 0, batch: 131, loss: 2.179062604904175\n",
      "epoch: 0, batch: 132, loss: 2.1512463092803955\n",
      "epoch: 0, batch: 133, loss: 2.127589702606201\n",
      "epoch: 0, batch: 134, loss: 2.026338815689087\n",
      "epoch: 0, batch: 135, loss: 2.1011884212493896\n",
      "epoch: 0, batch: 136, loss: 2.1241140365600586\n",
      "epoch: 0, batch: 137, loss: 2.152275800704956\n",
      "epoch: 0, batch: 138, loss: 2.0539088249206543\n",
      "epoch: 0, batch: 139, loss: 2.1280970573425293\n",
      "epoch: 0, batch: 140, loss: 2.0028481483459473\n",
      "epoch: 0, batch: 141, loss: 2.0363783836364746\n",
      "epoch: 0, batch: 142, loss: 2.0275213718414307\n",
      "epoch: 0, batch: 143, loss: 2.042346239089966\n",
      "epoch: 0, batch: 144, loss: 1.9640100002288818\n",
      "epoch: 0, batch: 145, loss: 2.066265106201172\n",
      "epoch: 0, batch: 146, loss: 1.9590562582015991\n",
      "epoch: 0, batch: 147, loss: 1.8907440900802612\n",
      "epoch: 0, batch: 148, loss: 2.161658763885498\n",
      "epoch: 0, batch: 149, loss: 2.093367338180542\n",
      "epoch: 0, batch: 150, loss: 2.038015842437744\n",
      "epoch: 0, batch: 151, loss: 1.8954130411148071\n",
      "epoch: 0, batch: 152, loss: 2.139600992202759\n",
      "epoch: 0, batch: 153, loss: 2.022799491882324\n",
      "epoch: 0, batch: 154, loss: 2.137104034423828\n",
      "epoch: 0, batch: 155, loss: 1.9300705194473267\n",
      "epoch: 0, batch: 156, loss: 1.9008197784423828\n",
      "epoch: 0, batch: 157, loss: 1.9226018190383911\n",
      "epoch: 0, batch: 158, loss: 2.0447747707366943\n",
      "epoch: 0, batch: 159, loss: 1.9951502084732056\n",
      "epoch: 0, batch: 160, loss: 2.163477659225464\n",
      "epoch: 0, batch: 161, loss: 1.9965920448303223\n",
      "epoch: 0, batch: 162, loss: 1.8709046840667725\n",
      "epoch: 0, batch: 163, loss: 1.8663188219070435\n",
      "epoch: 0, batch: 164, loss: 1.8888747692108154\n",
      "epoch: 0, batch: 165, loss: 1.8838846683502197\n",
      "epoch: 0, batch: 166, loss: 1.9565645456314087\n",
      "epoch: 0, batch: 167, loss: 1.8041024208068848\n",
      "epoch: 0, batch: 168, loss: 1.8405992984771729\n",
      "epoch: 0, batch: 169, loss: 1.919893503189087\n",
      "epoch: 0, batch: 170, loss: 1.8909432888031006\n",
      "epoch: 0, batch: 171, loss: 1.8540834188461304\n",
      "epoch: 0, batch: 172, loss: 1.9019145965576172\n",
      "epoch: 0, batch: 173, loss: 1.807882308959961\n",
      "epoch: 0, batch: 174, loss: 1.8281893730163574\n",
      "epoch: 0, batch: 175, loss: 1.856528401374817\n",
      "epoch: 0, batch: 176, loss: 1.8657574653625488\n",
      "epoch: 0, batch: 177, loss: 1.7993333339691162\n",
      "epoch: 0, batch: 178, loss: 1.8167763948440552\n",
      "epoch: 0, batch: 179, loss: 1.8402525186538696\n",
      "epoch: 0, batch: 180, loss: 1.8104965686798096\n",
      "epoch: 0, batch: 181, loss: 1.8546708822250366\n",
      "epoch: 0, batch: 182, loss: 1.8524614572525024\n",
      "epoch: 0, batch: 183, loss: 1.8263027667999268\n",
      "epoch: 0, batch: 184, loss: 1.8234474658966064\n",
      "epoch: 0, batch: 185, loss: 1.775634765625\n",
      "epoch: 0, batch: 186, loss: 1.6906359195709229\n",
      "epoch: 0, batch: 187, loss: 1.8603692054748535\n",
      "epoch: 0, batch: 188, loss: 1.814009666442871\n",
      "epoch: 0, batch: 189, loss: 1.8404121398925781\n",
      "epoch: 0, batch: 190, loss: 1.8004156351089478\n",
      "epoch: 0, batch: 191, loss: 1.6005380153656006\n",
      "epoch: 0, batch: 192, loss: 1.8105828762054443\n",
      "epoch: 0, batch: 193, loss: 1.849517822265625\n",
      "epoch: 0, batch: 194, loss: 1.7077373266220093\n",
      "epoch: 0, batch: 195, loss: 1.7880144119262695\n",
      "epoch: 0, batch: 196, loss: 1.7380648851394653\n",
      "epoch: 0, batch: 197, loss: 1.7358968257904053\n",
      "epoch: 0, batch: 198, loss: 1.7663556337356567\n",
      "epoch: 0, batch: 199, loss: 1.6834853887557983\n",
      "epoch: 0, batch: 200, loss: 1.7319539785385132\n",
      "epoch: 0, batch: 201, loss: 1.8217347860336304\n",
      "epoch: 0, batch: 202, loss: 1.7127504348754883\n",
      "epoch: 0, batch: 203, loss: 1.7026954889297485\n",
      "epoch: 0, batch: 204, loss: 1.8552323579788208\n",
      "epoch: 0, batch: 205, loss: 1.8361120223999023\n",
      "epoch: 0, batch: 206, loss: 1.7612054347991943\n",
      "epoch: 0, batch: 207, loss: 1.7151074409484863\n",
      "epoch: 0, batch: 208, loss: 1.8567931652069092\n",
      "epoch: 0, batch: 209, loss: 1.933442234992981\n",
      "epoch: 0, batch: 210, loss: 1.8474280834197998\n",
      "epoch: 0, batch: 211, loss: 1.6964203119277954\n",
      "epoch: 0, batch: 212, loss: 1.8715835809707642\n",
      "epoch: 0, batch: 213, loss: 1.7014707326889038\n",
      "epoch: 0, batch: 214, loss: 2.1056463718414307\n",
      "epoch: 0, batch: 215, loss: 1.8024349212646484\n",
      "epoch: 0, batch: 216, loss: 1.8908408880233765\n",
      "epoch: 0, batch: 217, loss: 1.7055786848068237\n",
      "epoch: 0, batch: 218, loss: 1.9219812154769897\n",
      "epoch: 0, batch: 219, loss: 1.766728401184082\n",
      "epoch: 0, batch: 220, loss: 1.7529089450836182\n",
      "epoch: 0, batch: 221, loss: 1.898040771484375\n",
      "epoch: 0, batch: 222, loss: 1.6803220510482788\n",
      "epoch: 0, batch: 223, loss: 1.6231569051742554\n",
      "epoch: 0, batch: 224, loss: 1.672787070274353\n",
      "epoch: 0, batch: 225, loss: 1.647225022315979\n",
      "epoch: 0, batch: 226, loss: 1.7014145851135254\n",
      "epoch: 0, batch: 227, loss: 1.9214566946029663\n",
      "epoch: 0, batch: 228, loss: 1.6692124605178833\n",
      "epoch: 0, batch: 229, loss: 1.685898780822754\n",
      "epoch: 0, batch: 230, loss: 1.674749493598938\n",
      "epoch: 0, batch: 231, loss: 1.880987286567688\n",
      "epoch: 0, batch: 232, loss: 1.6081957817077637\n",
      "epoch: 0, batch: 233, loss: 1.750246286392212\n",
      "epoch: 0, batch: 234, loss: 1.5706573724746704\n",
      "epoch: 0, batch: 235, loss: 1.819614052772522\n",
      "epoch: 0, batch: 236, loss: 1.8597970008850098\n",
      "epoch: 0, batch: 237, loss: 1.6121717691421509\n",
      "epoch: 0, batch: 238, loss: 1.738144040107727\n",
      "epoch: 0, batch: 239, loss: 1.9830503463745117\n",
      "epoch: 0, batch: 240, loss: 1.9193501472473145\n",
      "epoch: 0, batch: 241, loss: 1.8857085704803467\n",
      "epoch: 0, batch: 242, loss: 1.9095853567123413\n",
      "epoch: 0, batch: 243, loss: 1.8530313968658447\n",
      "epoch: 0, batch: 244, loss: 1.7444515228271484\n",
      "epoch: 0, batch: 245, loss: 1.6436476707458496\n",
      "epoch: 0, batch: 246, loss: 1.7273904085159302\n",
      "epoch: 0, batch: 247, loss: 1.478108286857605\n",
      "epoch: 0, batch: 248, loss: 1.6951652765274048\n",
      "epoch: 0, batch: 249, loss: 1.5482748746871948\n",
      "epoch: 0, batch: 250, loss: 1.679735779762268\n",
      "epoch: 0, batch: 251, loss: 1.6340664625167847\n",
      "epoch: 0, batch: 252, loss: 1.699705719947815\n",
      "epoch: 0, batch: 253, loss: 1.6537593603134155\n",
      "epoch: 0, batch: 254, loss: 1.7915232181549072\n",
      "epoch: 0, batch: 255, loss: 1.667466163635254\n",
      "epoch: 0, batch: 256, loss: 1.6876853704452515\n",
      "epoch: 0, batch: 257, loss: 1.5078188180923462\n",
      "epoch: 0, batch: 258, loss: 1.7973535060882568\n",
      "epoch: 0, batch: 259, loss: 1.825670599937439\n",
      "epoch: 0, batch: 260, loss: 1.6049928665161133\n",
      "epoch: 0, batch: 261, loss: 1.5799020528793335\n",
      "epoch: 0, batch: 262, loss: 1.6387698650360107\n",
      "epoch: 0, batch: 263, loss: 1.665336012840271\n",
      "epoch: 0, batch: 264, loss: 1.6233174800872803\n",
      "epoch: 0, batch: 265, loss: 1.5286108255386353\n",
      "epoch: 0, batch: 266, loss: 1.648932933807373\n",
      "epoch: 0, batch: 267, loss: 1.8255105018615723\n",
      "epoch: 0, batch: 268, loss: 1.4563302993774414\n",
      "epoch: 0, batch: 269, loss: 1.7500183582305908\n",
      "epoch: 0, batch: 270, loss: 1.4077898263931274\n",
      "epoch: 0, batch: 271, loss: 1.6132597923278809\n",
      "epoch: 0, batch: 272, loss: 1.573969841003418\n",
      "epoch: 0, batch: 273, loss: 1.5322827100753784\n",
      "epoch: 0, batch: 274, loss: 1.7676175832748413\n",
      "epoch: 0, batch: 275, loss: 1.5330421924591064\n",
      "epoch: 0, batch: 276, loss: 1.7103769779205322\n",
      "epoch: 0, batch: 277, loss: 1.6546251773834229\n",
      "epoch: 0, batch: 278, loss: 1.823502540588379\n",
      "epoch: 0, batch: 279, loss: 1.5602115392684937\n",
      "epoch: 0, batch: 280, loss: 1.587565302848816\n",
      "epoch: 0, batch: 281, loss: 1.4838881492614746\n",
      "epoch: 0, batch: 282, loss: 1.580939531326294\n",
      "epoch: 0, batch: 283, loss: 1.5891677141189575\n",
      "epoch: 0, batch: 284, loss: 1.8314094543457031\n",
      "epoch: 0, batch: 285, loss: 1.806071162223816\n",
      "epoch: 0, batch: 286, loss: 1.6131173372268677\n",
      "epoch: 0, batch: 287, loss: 1.5622807741165161\n",
      "epoch: 0, batch: 288, loss: 1.5355451107025146\n",
      "epoch: 0, batch: 289, loss: 1.4916672706604004\n",
      "epoch: 0, batch: 290, loss: 1.5903557538986206\n",
      "epoch: 0, batch: 291, loss: 1.492714285850525\n",
      "epoch: 0, batch: 292, loss: 1.4085556268692017\n",
      "epoch: 0, batch: 293, loss: 1.7428356409072876\n",
      "epoch: 0, batch: 294, loss: 1.524114727973938\n",
      "epoch: 0, batch: 295, loss: 1.5279009342193604\n",
      "epoch: 0, batch: 296, loss: 1.436592698097229\n",
      "epoch: 0, batch: 297, loss: 1.570422649383545\n",
      "epoch: 0, batch: 298, loss: 1.4737118482589722\n",
      "epoch: 0, batch: 299, loss: 1.519806981086731\n",
      "epoch: 0, batch: 300, loss: 1.620818018913269\n",
      "epoch: 0, batch: 301, loss: 1.6140791177749634\n",
      "epoch: 0, batch: 302, loss: 1.459526777267456\n",
      "epoch: 0, batch: 303, loss: 1.5600260496139526\n",
      "epoch: 0, batch: 304, loss: 1.8587672710418701\n",
      "epoch: 0, batch: 305, loss: 1.4057549238204956\n",
      "epoch: 0, batch: 306, loss: 1.3944510221481323\n",
      "epoch: 0, batch: 307, loss: 1.4925962686538696\n",
      "epoch: 0, batch: 308, loss: 1.6273537874221802\n",
      "epoch: 0, batch: 309, loss: 1.6484732627868652\n",
      "epoch: 0, batch: 310, loss: 1.3483092784881592\n",
      "epoch: 0, batch: 311, loss: 1.4420820474624634\n",
      "epoch: 0, batch: 312, loss: 1.626805067062378\n",
      "epoch: 0, batch: 313, loss: 1.682017207145691\n",
      "epoch: 0, batch: 314, loss: 1.45968759059906\n",
      "epoch: 0, batch: 315, loss: 1.5169048309326172\n",
      "epoch: 0, batch: 316, loss: 1.6053917407989502\n",
      "epoch: 0, batch: 317, loss: 1.5909128189086914\n",
      "epoch: 0, batch: 318, loss: 1.387765645980835\n",
      "epoch: 0, batch: 319, loss: 1.4295257329940796\n",
      "epoch: 0, batch: 320, loss: 1.73724365234375\n",
      "epoch: 0, batch: 321, loss: 1.2168495655059814\n",
      "epoch: 0, batch: 322, loss: 1.3591233491897583\n",
      "epoch: 0, batch: 323, loss: 1.4661072492599487\n",
      "epoch: 0, batch: 324, loss: 1.4149909019470215\n",
      "epoch: 0, batch: 325, loss: 1.6874130964279175\n",
      "epoch: 0, batch: 326, loss: 1.381253719329834\n",
      "epoch: 0, batch: 327, loss: 1.5035358667373657\n",
      "epoch: 0, batch: 328, loss: 1.4547336101531982\n",
      "epoch: 0, batch: 329, loss: 1.726939082145691\n",
      "epoch: 0, batch: 330, loss: 1.2791029214859009\n",
      "epoch: 0, batch: 331, loss: 1.5370842218399048\n",
      "epoch: 0, batch: 332, loss: 1.3767088651657104\n",
      "epoch: 0, batch: 333, loss: 1.3336939811706543\n",
      "epoch: 0, batch: 334, loss: 1.6422700881958008\n",
      "epoch: 0, batch: 335, loss: 1.3346083164215088\n",
      "epoch: 0, batch: 336, loss: 1.5113869905471802\n",
      "epoch: 0, batch: 337, loss: 1.5552138090133667\n",
      "epoch: 0, batch: 338, loss: 1.3475358486175537\n",
      "epoch: 0, batch: 339, loss: 1.3680202960968018\n",
      "epoch: 0, batch: 340, loss: 1.4853631258010864\n",
      "epoch: 0, batch: 341, loss: 1.581060528755188\n",
      "epoch: 0, batch: 342, loss: 1.436264991760254\n",
      "epoch: 0, batch: 343, loss: 1.1569852828979492\n",
      "epoch: 0, batch: 344, loss: 1.581181287765503\n",
      "epoch: 0, batch: 345, loss: 1.3429441452026367\n",
      "epoch: 0, batch: 346, loss: 1.520439624786377\n",
      "epoch: 0, batch: 347, loss: 1.3543424606323242\n",
      "epoch: 0, batch: 348, loss: 1.5243809223175049\n",
      "epoch: 0, batch: 349, loss: 1.3755464553833008\n",
      "epoch: 0, batch: 350, loss: 1.451069712638855\n",
      "epoch: 0, batch: 351, loss: 1.481530785560608\n",
      "epoch: 0, batch: 352, loss: 1.317995548248291\n",
      "epoch: 0, batch: 353, loss: 1.3720102310180664\n",
      "epoch: 0, batch: 354, loss: 1.574066162109375\n",
      "epoch: 0, batch: 355, loss: 1.5160574913024902\n",
      "epoch: 0, batch: 356, loss: 1.598008632659912\n",
      "epoch: 0, batch: 357, loss: 1.354357361793518\n",
      "epoch: 0, batch: 358, loss: 1.3649524450302124\n",
      "epoch: 0, batch: 359, loss: 1.2340614795684814\n",
      "epoch: 0, batch: 360, loss: 1.1438482999801636\n",
      "epoch: 0, batch: 361, loss: 1.4967026710510254\n",
      "epoch: 0, batch: 362, loss: 1.326225757598877\n",
      "epoch: 0, batch: 363, loss: 1.3505171537399292\n",
      "epoch: 0, batch: 364, loss: 1.5371192693710327\n",
      "epoch: 0, batch: 365, loss: 1.5095728635787964\n",
      "epoch: 0, batch: 366, loss: 1.580946683883667\n",
      "epoch: 0, batch: 367, loss: 1.53217613697052\n",
      "epoch: 0, batch: 368, loss: 1.176272988319397\n",
      "epoch: 0, batch: 369, loss: 1.6454523801803589\n",
      "epoch: 0, batch: 370, loss: 1.2976266145706177\n",
      "epoch: 0, batch: 371, loss: 1.2355576753616333\n",
      "epoch: 0, batch: 372, loss: 1.6025187969207764\n",
      "epoch: 0, batch: 373, loss: 1.427388310432434\n",
      "epoch: 0, batch: 374, loss: 1.283242106437683\n",
      "epoch: 0, batch: 375, loss: 1.3858132362365723\n",
      "epoch: 0, batch: 376, loss: 1.513092041015625\n",
      "epoch: 0, batch: 377, loss: 1.3156325817108154\n",
      "epoch: 0, batch: 378, loss: 1.352065920829773\n",
      "epoch: 0, batch: 379, loss: 1.3880157470703125\n",
      "epoch: 0, batch: 380, loss: 1.4743403196334839\n",
      "epoch: 0, batch: 381, loss: 1.3197946548461914\n",
      "epoch: 0, batch: 382, loss: 1.6154738664627075\n",
      "epoch: 0, batch: 383, loss: 1.2917317152023315\n",
      "epoch: 0, batch: 384, loss: 1.481263518333435\n",
      "epoch: 0, batch: 385, loss: 1.2872161865234375\n",
      "epoch: 0, batch: 386, loss: 1.6889418363571167\n",
      "epoch: 0, batch: 387, loss: 1.3885953426361084\n",
      "epoch: 0, batch: 388, loss: 1.3347090482711792\n",
      "epoch: 0, batch: 389, loss: 1.177992343902588\n",
      "epoch: 0, batch: 390, loss: 1.094594955444336\n",
      "epoch: 0, batch: 391, loss: 1.5372662544250488\n",
      "epoch: 0, batch: 392, loss: 1.4337157011032104\n",
      "epoch: 0, batch: 393, loss: 1.299235463142395\n",
      "epoch: 0, batch: 394, loss: 1.4139260053634644\n",
      "epoch: 0, batch: 395, loss: 1.293962001800537\n",
      "epoch: 0, batch: 396, loss: 1.5949170589447021\n",
      "epoch: 0, batch: 397, loss: 1.2776421308517456\n",
      "epoch: 0, batch: 398, loss: 1.2764229774475098\n",
      "epoch: 0, batch: 399, loss: 1.3897572755813599\n",
      "epoch: 0, batch: 400, loss: 1.3148682117462158\n",
      "epoch: 0, batch: 401, loss: 1.294189691543579\n",
      "epoch: 0, batch: 402, loss: 1.5947377681732178\n",
      "epoch: 0, batch: 403, loss: 1.3145899772644043\n",
      "epoch: 0, batch: 404, loss: 1.3486002683639526\n",
      "epoch: 0, batch: 405, loss: 1.4664087295532227\n",
      "epoch: 0, batch: 406, loss: 1.414478063583374\n",
      "epoch: 0, batch: 407, loss: 1.2599233388900757\n",
      "epoch: 0, batch: 408, loss: 1.357012152671814\n",
      "epoch: 0, batch: 409, loss: 1.2812516689300537\n",
      "epoch: 0, batch: 410, loss: 1.5436553955078125\n",
      "epoch: 0, batch: 411, loss: 1.2732714414596558\n",
      "epoch: 0, batch: 412, loss: 1.4462382793426514\n",
      "epoch: 0, batch: 413, loss: 1.1510024070739746\n",
      "epoch: 0, batch: 414, loss: 1.0728331804275513\n",
      "epoch: 0, batch: 415, loss: 1.325697660446167\n",
      "epoch: 0, batch: 416, loss: 1.4306458234786987\n",
      "epoch: 0, batch: 417, loss: 1.467987298965454\n",
      "epoch: 0, batch: 418, loss: 1.2223631143569946\n",
      "epoch: 0, batch: 419, loss: 1.2532635927200317\n",
      "epoch: 0, batch: 420, loss: 1.4635730981826782\n",
      "epoch: 0, batch: 421, loss: 1.1985524892807007\n",
      "epoch: 0, batch: 422, loss: 1.3722949028015137\n",
      "epoch: 0, batch: 423, loss: 1.1892387866973877\n",
      "epoch: 0, batch: 424, loss: 1.3515558242797852\n",
      "epoch: 0, batch: 425, loss: 1.2474054098129272\n",
      "epoch: 0, batch: 426, loss: 1.5251156091690063\n",
      "epoch: 0, batch: 427, loss: 1.3819941282272339\n",
      "epoch: 0, batch: 428, loss: 1.4085623025894165\n",
      "epoch: 0, batch: 429, loss: 1.2945291996002197\n",
      "epoch: 0, batch: 430, loss: 1.5548737049102783\n",
      "epoch: 0, batch: 431, loss: 1.0862687826156616\n",
      "epoch: 0, batch: 432, loss: 1.1421891450881958\n",
      "epoch: 0, batch: 433, loss: 1.1510372161865234\n",
      "epoch: 0, batch: 434, loss: 1.1960488557815552\n",
      "epoch: 0, batch: 435, loss: 1.392857313156128\n",
      "epoch: 0, batch: 436, loss: 1.2948291301727295\n",
      "epoch: 0, batch: 437, loss: 1.143251657485962\n",
      "epoch: 0, batch: 438, loss: 1.176688313484192\n",
      "epoch: 0, batch: 439, loss: 1.4837623834609985\n",
      "epoch: 0, batch: 440, loss: 1.179226040840149\n",
      "epoch: 0, batch: 441, loss: 1.4322339296340942\n",
      "epoch: 0, batch: 442, loss: 1.1868842840194702\n",
      "epoch: 0, batch: 443, loss: 1.4350956678390503\n",
      "epoch: 0, batch: 444, loss: 1.144350528717041\n",
      "epoch: 0, batch: 445, loss: 1.214675784111023\n",
      "epoch: 0, batch: 446, loss: 1.184164047241211\n",
      "epoch: 0, batch: 447, loss: 1.1205674409866333\n",
      "epoch: 0, batch: 448, loss: 1.3001035451889038\n",
      "epoch: 0, batch: 449, loss: 1.196108102798462\n",
      "epoch: 0, batch: 450, loss: 1.2222580909729004\n",
      "epoch: 0, batch: 451, loss: 1.2390943765640259\n",
      "epoch: 0, batch: 452, loss: 1.377681851387024\n",
      "epoch: 0, batch: 453, loss: 1.1645928621292114\n",
      "epoch: 0, batch: 454, loss: 1.17668616771698\n",
      "epoch: 0, batch: 455, loss: 1.5412569046020508\n",
      "epoch: 0, batch: 456, loss: 1.1703217029571533\n",
      "epoch: 0, batch: 457, loss: 1.0532681941986084\n",
      "epoch: 0, batch: 458, loss: 1.2407829761505127\n",
      "epoch: 0, batch: 459, loss: 1.2678020000457764\n",
      "epoch: 0, batch: 460, loss: 1.1217305660247803\n",
      "epoch: 0, batch: 461, loss: 1.1553473472595215\n",
      "epoch: 0, batch: 462, loss: 1.208112120628357\n",
      "epoch: 0, batch: 463, loss: 1.2476489543914795\n",
      "epoch: 0, batch: 464, loss: 1.032265067100525\n",
      "epoch: 0, batch: 465, loss: 1.3546297550201416\n",
      "epoch: 0, batch: 466, loss: 1.111808180809021\n",
      "epoch: 0, batch: 467, loss: 1.4467483758926392\n",
      "epoch: 0, batch: 468, loss: 1.4709278345108032\n",
      "epoch: 0, batch: 469, loss: 1.2395051717758179\n",
      "epoch: 0, batch: 470, loss: 1.1075265407562256\n",
      "epoch: 0, batch: 471, loss: 1.1271837949752808\n",
      "epoch: 0, batch: 472, loss: 1.3655459880828857\n",
      "epoch: 0, batch: 473, loss: 1.116281509399414\n",
      "epoch: 0, batch: 474, loss: 1.6684601306915283\n",
      "epoch: 0, batch: 475, loss: 1.4705822467803955\n",
      "epoch: 0, batch: 476, loss: 1.1393728256225586\n",
      "epoch: 0, batch: 477, loss: 1.491457462310791\n",
      "epoch: 0, batch: 478, loss: 1.1242610216140747\n",
      "epoch: 0, batch: 479, loss: 1.2613059282302856\n",
      "epoch: 0, batch: 480, loss: 1.1917401552200317\n",
      "epoch: 0, batch: 481, loss: 1.0883498191833496\n",
      "epoch: 0, batch: 482, loss: 1.2501325607299805\n",
      "epoch: 0, batch: 483, loss: 1.3164057731628418\n",
      "epoch: 0, batch: 484, loss: 1.191772699356079\n",
      "epoch: 0, batch: 485, loss: 1.0512197017669678\n",
      "epoch: 0, batch: 486, loss: 1.089211106300354\n",
      "epoch: 0, batch: 487, loss: 1.2294429540634155\n",
      "epoch: 0, batch: 488, loss: 1.2300539016723633\n",
      "epoch: 0, batch: 489, loss: 1.192839503288269\n",
      "epoch: 0, batch: 490, loss: 1.2939453125\n",
      "epoch: 0, batch: 491, loss: 1.1600277423858643\n",
      "epoch: 0, batch: 492, loss: 1.3061013221740723\n",
      "epoch: 0, batch: 493, loss: 1.059819221496582\n",
      "epoch: 0, batch: 494, loss: 1.338230013847351\n",
      "epoch: 0, batch: 495, loss: 1.2391148805618286\n",
      "epoch: 0, batch: 496, loss: 1.18510103225708\n",
      "epoch: 0, batch: 497, loss: 1.3874256610870361\n",
      "epoch: 0, batch: 498, loss: 1.1262887716293335\n",
      "epoch: 0, batch: 499, loss: 1.1784913539886475\n",
      "epoch: 0, batch: 500, loss: 1.3061192035675049\n",
      "epoch: 0, batch: 501, loss: 1.2560817003250122\n",
      "epoch: 0, batch: 502, loss: 1.2943376302719116\n",
      "epoch: 0, batch: 503, loss: 1.212339162826538\n",
      "epoch: 0, batch: 504, loss: 1.0850038528442383\n",
      "epoch: 0, batch: 505, loss: 1.2689588069915771\n",
      "epoch: 0, batch: 506, loss: 1.2494025230407715\n",
      "epoch: 0, batch: 507, loss: 1.0780779123306274\n",
      "epoch: 0, batch: 508, loss: 1.0580474138259888\n",
      "epoch: 0, batch: 509, loss: 0.9826343059539795\n",
      "epoch: 0, batch: 510, loss: 1.2195124626159668\n",
      "epoch: 0, batch: 511, loss: 1.1254796981811523\n",
      "epoch: 0, batch: 512, loss: 1.2002071142196655\n",
      "epoch: 0, batch: 513, loss: 1.1785023212432861\n",
      "epoch: 0, batch: 514, loss: 1.2019405364990234\n",
      "epoch: 0, batch: 515, loss: 1.4439393281936646\n",
      "epoch: 0, batch: 516, loss: 1.051404356956482\n",
      "epoch: 0, batch: 517, loss: 1.0494478940963745\n",
      "epoch: 0, batch: 518, loss: 1.3971948623657227\n",
      "epoch: 0, batch: 519, loss: 1.1744049787521362\n",
      "epoch: 0, batch: 520, loss: 1.1061114072799683\n",
      "epoch: 0, batch: 521, loss: 1.1640664339065552\n",
      "epoch: 0, batch: 522, loss: 1.079636812210083\n",
      "epoch: 0, batch: 523, loss: 0.9780349731445312\n",
      "epoch: 0, batch: 524, loss: 0.9112627506256104\n",
      "epoch: 0, batch: 525, loss: 1.460755705833435\n",
      "epoch: 0, batch: 526, loss: 1.4277921915054321\n",
      "epoch: 0, batch: 527, loss: 1.102391004562378\n",
      "epoch: 0, batch: 528, loss: 1.244262456893921\n",
      "epoch: 0, batch: 529, loss: 1.1586047410964966\n",
      "epoch: 0, batch: 530, loss: 1.1399675607681274\n",
      "epoch: 0, batch: 531, loss: 1.2271257638931274\n",
      "epoch: 0, batch: 532, loss: 1.039162039756775\n",
      "epoch: 0, batch: 533, loss: 1.2029410600662231\n",
      "epoch: 0, batch: 534, loss: 0.9839692711830139\n",
      "epoch: 0, batch: 535, loss: 1.0611138343811035\n",
      "epoch: 0, batch: 536, loss: 1.1654119491577148\n",
      "epoch: 0, batch: 537, loss: 1.3360788822174072\n",
      "epoch: 0, batch: 538, loss: 1.131715178489685\n",
      "epoch: 0, batch: 539, loss: 1.093660831451416\n",
      "epoch: 0, batch: 540, loss: 1.067728877067566\n",
      "epoch: 0, batch: 541, loss: 1.2433507442474365\n",
      "epoch: 0, batch: 542, loss: 1.2134982347488403\n",
      "epoch: 0, batch: 543, loss: 1.0709195137023926\n",
      "epoch: 0, batch: 544, loss: 1.2369449138641357\n",
      "epoch: 0, batch: 545, loss: 1.2583730220794678\n",
      "epoch: 0, batch: 546, loss: 0.9802471399307251\n",
      "epoch: 0, batch: 547, loss: 1.1333953142166138\n",
      "epoch: 0, batch: 548, loss: 1.2715109586715698\n",
      "epoch: 0, batch: 549, loss: 1.1294569969177246\n",
      "epoch: 0, batch: 550, loss: 1.111696481704712\n",
      "epoch: 0, batch: 551, loss: 0.9624464511871338\n",
      "epoch: 0, batch: 552, loss: 1.1941074132919312\n",
      "epoch: 0, batch: 553, loss: 0.9100686311721802\n",
      "epoch: 0, batch: 554, loss: 1.2322601079940796\n",
      "epoch: 0, batch: 555, loss: 1.1546064615249634\n",
      "epoch: 0, batch: 556, loss: 1.3492329120635986\n",
      "epoch: 0, batch: 557, loss: 1.3739229440689087\n",
      "epoch: 0, batch: 558, loss: 0.955621600151062\n",
      "epoch: 0, batch: 559, loss: 1.0978782176971436\n",
      "epoch: 0, batch: 560, loss: 1.234859585762024\n",
      "epoch: 0, batch: 561, loss: 1.1318652629852295\n",
      "epoch: 0, batch: 562, loss: 1.1645818948745728\n",
      "epoch: 0, batch: 563, loss: 1.1446744203567505\n",
      "epoch: 0, batch: 564, loss: 1.1349990367889404\n",
      "epoch: 0, batch: 565, loss: 1.1992179155349731\n",
      "epoch: 0, batch: 566, loss: 1.405187964439392\n",
      "epoch: 0, batch: 567, loss: 1.214812994003296\n",
      "epoch: 0, batch: 568, loss: 0.8587192296981812\n",
      "epoch: 0, batch: 569, loss: 1.0902701616287231\n",
      "epoch: 0, batch: 570, loss: 1.058401107788086\n",
      "epoch: 0, batch: 571, loss: 1.036217212677002\n",
      "epoch: 0, batch: 572, loss: 1.2633638381958008\n",
      "epoch: 0, batch: 573, loss: 1.1612615585327148\n",
      "epoch: 0, batch: 574, loss: 0.9503403306007385\n",
      "epoch: 0, batch: 575, loss: 0.8295630216598511\n",
      "epoch: 0, batch: 576, loss: 0.9482988119125366\n",
      "epoch: 0, batch: 577, loss: 1.2213937044143677\n",
      "epoch: 0, batch: 578, loss: 1.1308778524398804\n",
      "epoch: 0, batch: 579, loss: 0.9700658917427063\n",
      "epoch: 0, batch: 580, loss: 1.1309962272644043\n",
      "epoch: 0, batch: 581, loss: 0.9295907020568848\n",
      "epoch: 0, batch: 582, loss: 1.2042839527130127\n",
      "epoch: 0, batch: 583, loss: 1.1081758737564087\n",
      "epoch: 0, batch: 584, loss: 1.1159504652023315\n",
      "epoch: 0, batch: 585, loss: 0.9977691173553467\n",
      "epoch: 0, batch: 586, loss: 1.0077413320541382\n",
      "epoch: 0, batch: 587, loss: 0.9637033939361572\n",
      "epoch: 0, batch: 588, loss: 1.3412868976593018\n",
      "epoch: 0, batch: 589, loss: 1.3265236616134644\n",
      "epoch: 0, batch: 590, loss: 0.9223445057868958\n",
      "epoch: 0, batch: 591, loss: 1.1217832565307617\n",
      "epoch: 0, batch: 592, loss: 1.146588921546936\n",
      "epoch: 0, batch: 593, loss: 0.9473018050193787\n",
      "epoch: 0, batch: 594, loss: 0.9998200535774231\n",
      "epoch: 0, batch: 595, loss: 1.4324744939804077\n",
      "epoch: 0, batch: 596, loss: 0.9862819910049438\n",
      "epoch: 0, batch: 597, loss: 0.9765577912330627\n",
      "epoch: 0, batch: 598, loss: 1.0263261795043945\n",
      "epoch: 0, batch: 599, loss: 0.9750502705574036\n",
      "epoch: 0, batch: 600, loss: 1.1631253957748413\n",
      "epoch: 0, batch: 601, loss: 1.2756651639938354\n",
      "epoch: 0, batch: 602, loss: 1.0767778158187866\n",
      "epoch: 0, batch: 603, loss: 1.256354570388794\n",
      "epoch: 0, batch: 604, loss: 1.1245743036270142\n",
      "epoch: 0, batch: 605, loss: 1.2503328323364258\n",
      "epoch: 0, batch: 606, loss: 1.0443836450576782\n",
      "epoch: 0, batch: 607, loss: 1.0393366813659668\n",
      "epoch: 0, batch: 608, loss: 0.9163735508918762\n",
      "epoch: 0, batch: 609, loss: 1.199568748474121\n",
      "epoch: 0, batch: 610, loss: 1.1143712997436523\n",
      "epoch: 0, batch: 611, loss: 1.1409573554992676\n",
      "epoch: 0, batch: 612, loss: 0.8985300660133362\n",
      "epoch: 0, batch: 613, loss: 1.2258458137512207\n",
      "epoch: 0, batch: 614, loss: 1.1089588403701782\n",
      "epoch: 0, batch: 615, loss: 1.104824185371399\n",
      "epoch: 0, batch: 616, loss: 1.3756814002990723\n",
      "epoch: 0, batch: 617, loss: 1.0609469413757324\n",
      "epoch: 0, batch: 618, loss: 1.3466556072235107\n",
      "epoch: 0, batch: 619, loss: 1.119904637336731\n",
      "epoch: 0, batch: 620, loss: 0.8078525066375732\n",
      "epoch: 0, batch: 621, loss: 0.8959686160087585\n",
      "epoch: 0, batch: 622, loss: 0.9578192830085754\n",
      "epoch: 0, batch: 623, loss: 1.2478420734405518\n",
      "epoch: 0, batch: 624, loss: 0.9952213764190674\n",
      "epoch: 0, batch: 625, loss: 0.7691121101379395\n",
      "epoch: 0, batch: 626, loss: 1.041143774986267\n",
      "epoch: 0, batch: 627, loss: 0.9139548540115356\n",
      "epoch: 0, batch: 628, loss: 1.004417896270752\n",
      "epoch: 0, batch: 629, loss: 0.8351937532424927\n",
      "epoch: 0, batch: 630, loss: 1.0776482820510864\n",
      "epoch: 0, batch: 631, loss: 0.9887530207633972\n",
      "epoch: 0, batch: 632, loss: 0.9624518156051636\n",
      "epoch: 0, batch: 633, loss: 1.003620982170105\n",
      "epoch: 0, batch: 634, loss: 1.0650159120559692\n",
      "epoch: 0, batch: 635, loss: 1.0214498043060303\n",
      "epoch: 0, batch: 636, loss: 1.037322759628296\n",
      "epoch: 0, batch: 637, loss: 1.0456926822662354\n",
      "epoch: 0, batch: 638, loss: 1.2642463445663452\n",
      "epoch: 0, batch: 639, loss: 1.0244096517562866\n",
      "epoch: 0, batch: 640, loss: 1.0222543478012085\n",
      "epoch: 0, batch: 641, loss: 1.106438398361206\n",
      "epoch: 0, batch: 642, loss: 1.0548338890075684\n",
      "epoch: 0, batch: 643, loss: 1.1736009120941162\n",
      "epoch: 0, batch: 644, loss: 0.8780775666236877\n",
      "epoch: 0, batch: 645, loss: 1.152573823928833\n",
      "epoch: 0, batch: 646, loss: 0.7682005167007446\n",
      "epoch: 0, batch: 647, loss: 0.7424653172492981\n",
      "epoch: 0, batch: 648, loss: 1.152231216430664\n",
      "epoch: 0, batch: 649, loss: 1.0042333602905273\n",
      "epoch: 0, batch: 650, loss: 0.9754902124404907\n",
      "epoch: 0, batch: 651, loss: 0.9585782289505005\n",
      "epoch: 0, batch: 652, loss: 1.2714070081710815\n",
      "epoch: 0, batch: 653, loss: 1.234181523323059\n",
      "epoch: 0, batch: 654, loss: 1.0341243743896484\n",
      "epoch: 0, batch: 655, loss: 0.8428632616996765\n",
      "epoch: 0, batch: 656, loss: 1.30322265625\n",
      "epoch: 0, batch: 657, loss: 0.9081384539604187\n",
      "epoch: 0, batch: 658, loss: 1.037848711013794\n",
      "epoch: 0, batch: 659, loss: 1.4435352087020874\n",
      "epoch: 0, batch: 660, loss: 0.7978548407554626\n",
      "epoch: 0, batch: 661, loss: 1.310175895690918\n",
      "epoch: 0, batch: 662, loss: 1.0591908693313599\n",
      "epoch: 0, batch: 663, loss: 1.151342511177063\n",
      "epoch: 0, batch: 664, loss: 1.3273981809616089\n",
      "epoch: 0, batch: 665, loss: 0.8213021159172058\n",
      "epoch: 0, batch: 666, loss: 1.0294197797775269\n",
      "epoch: 0, batch: 667, loss: 1.124342679977417\n",
      "epoch: 0, batch: 668, loss: 1.1203689575195312\n",
      "epoch: 0, batch: 669, loss: 0.888700008392334\n",
      "epoch: 0, batch: 670, loss: 0.9755299091339111\n",
      "epoch: 0, batch: 671, loss: 0.931978166103363\n",
      "epoch: 0, batch: 672, loss: 1.1973621845245361\n",
      "epoch: 0, batch: 673, loss: 1.0699843168258667\n",
      "epoch: 0, batch: 674, loss: 0.8913342952728271\n",
      "epoch: 0, batch: 675, loss: 0.7457765936851501\n",
      "epoch: 0, batch: 676, loss: 0.9344088435173035\n",
      "epoch: 0, batch: 677, loss: 1.1723123788833618\n",
      "epoch: 0, batch: 678, loss: 0.98072749376297\n",
      "epoch: 0, batch: 679, loss: 0.8237148523330688\n",
      "epoch: 0, batch: 680, loss: 0.7866330146789551\n",
      "epoch: 0, batch: 681, loss: 0.9258885383605957\n",
      "epoch: 0, batch: 682, loss: 0.9712619781494141\n",
      "epoch: 0, batch: 683, loss: 0.8483537435531616\n",
      "epoch: 0, batch: 684, loss: 0.779071569442749\n",
      "epoch: 0, batch: 685, loss: 1.2077592611312866\n",
      "epoch: 0, batch: 686, loss: 1.1962329149246216\n",
      "epoch: 0, batch: 687, loss: 0.880067765712738\n",
      "epoch: 0, batch: 688, loss: 1.2137417793273926\n",
      "epoch: 0, batch: 689, loss: 0.9016869068145752\n",
      "epoch: 0, batch: 690, loss: 0.9618735313415527\n",
      "epoch: 0, batch: 691, loss: 1.0599228143692017\n",
      "epoch: 0, batch: 692, loss: 0.917026698589325\n",
      "epoch: 0, batch: 693, loss: 1.078650951385498\n",
      "epoch: 0, batch: 694, loss: 1.1293401718139648\n",
      "epoch: 0, batch: 695, loss: 0.821186900138855\n",
      "epoch: 0, batch: 696, loss: 0.9325322508811951\n",
      "epoch: 0, batch: 697, loss: 1.0811351537704468\n",
      "epoch: 0, batch: 698, loss: 0.8579659461975098\n",
      "epoch: 0, batch: 699, loss: 0.8924026489257812\n",
      "epoch: 0, batch: 700, loss: 1.0581928491592407\n",
      "epoch: 0, batch: 701, loss: 0.948192834854126\n",
      "epoch: 0, batch: 702, loss: 0.9893502593040466\n",
      "epoch: 0, batch: 703, loss: 1.099898099899292\n",
      "epoch: 0, batch: 704, loss: 0.832395613193512\n",
      "epoch: 0, batch: 705, loss: 0.9601961970329285\n",
      "epoch: 0, batch: 706, loss: 0.8615946769714355\n",
      "epoch: 0, batch: 707, loss: 0.9379948973655701\n",
      "epoch: 0, batch: 708, loss: 0.7959815263748169\n",
      "epoch: 0, batch: 709, loss: 1.0838900804519653\n",
      "epoch: 0, batch: 710, loss: 0.9710536003112793\n",
      "epoch: 0, batch: 711, loss: 1.0411388874053955\n",
      "epoch: 0, batch: 712, loss: 1.109593152999878\n",
      "epoch: 0, batch: 713, loss: 0.9384639263153076\n",
      "epoch: 0, batch: 714, loss: 0.9930371046066284\n",
      "epoch: 0, batch: 715, loss: 1.00266695022583\n",
      "epoch: 0, batch: 716, loss: 1.1887507438659668\n",
      "epoch: 0, batch: 717, loss: 1.01655113697052\n",
      "epoch: 0, batch: 718, loss: 0.9237577319145203\n",
      "epoch: 0, batch: 719, loss: 1.0457594394683838\n",
      "epoch: 0, batch: 720, loss: 0.8836438059806824\n",
      "epoch: 0, batch: 721, loss: 0.8546701669692993\n",
      "epoch: 0, batch: 722, loss: 0.979052722454071\n",
      "epoch: 0, batch: 723, loss: 1.0408324003219604\n",
      "epoch: 0, batch: 724, loss: 1.2534972429275513\n",
      "epoch: 0, batch: 725, loss: 1.0012633800506592\n",
      "epoch: 0, batch: 726, loss: 1.0573197603225708\n",
      "epoch: 0, batch: 727, loss: 0.7796288132667542\n",
      "epoch: 0, batch: 728, loss: 0.9204559922218323\n",
      "epoch: 0, batch: 729, loss: 0.9322935342788696\n",
      "epoch: 0, batch: 730, loss: 0.8594930768013\n",
      "epoch: 0, batch: 731, loss: 0.8834842443466187\n",
      "epoch: 0, batch: 732, loss: 1.0961108207702637\n",
      "epoch: 0, batch: 733, loss: 0.9793031215667725\n",
      "epoch: 0, batch: 734, loss: 0.9709396362304688\n",
      "epoch: 0, batch: 735, loss: 0.8454898595809937\n",
      "epoch: 0, batch: 736, loss: 0.7545167803764343\n",
      "epoch: 0, batch: 737, loss: 0.9899544715881348\n",
      "epoch: 0, batch: 738, loss: 1.1441789865493774\n",
      "epoch: 0, batch: 739, loss: 0.9282243847846985\n",
      "epoch: 0, batch: 740, loss: 1.1307204961776733\n",
      "epoch: 0, batch: 741, loss: 0.6507270336151123\n",
      "epoch: 0, batch: 742, loss: 1.0097559690475464\n",
      "epoch: 0, batch: 743, loss: 0.959881067276001\n",
      "epoch: 0, batch: 744, loss: 1.2344247102737427\n",
      "epoch: 0, batch: 745, loss: 0.9896180033683777\n",
      "epoch: 0, batch: 746, loss: 0.9815492033958435\n",
      "epoch: 0, batch: 747, loss: 0.6810381412506104\n",
      "epoch: 0, batch: 748, loss: 0.9029064178466797\n",
      "epoch: 0, batch: 749, loss: 0.7966242432594299\n",
      "epoch: 0, batch: 750, loss: 0.9575915336608887\n",
      "epoch: 0, batch: 751, loss: 0.9144465923309326\n",
      "epoch: 0, batch: 752, loss: 1.0939834117889404\n",
      "epoch: 0, batch: 753, loss: 0.9588263034820557\n",
      "epoch: 0, batch: 754, loss: 0.7993283271789551\n",
      "epoch: 0, batch: 755, loss: 0.939887523651123\n",
      "epoch: 0, batch: 756, loss: 0.7740187048912048\n",
      "epoch: 0, batch: 757, loss: 0.9503393173217773\n",
      "epoch: 0, batch: 758, loss: 1.0656017065048218\n",
      "epoch: 0, batch: 759, loss: 0.8407704830169678\n",
      "epoch: 0, batch: 760, loss: 0.9584901928901672\n",
      "epoch: 0, batch: 761, loss: 0.9120759963989258\n",
      "epoch: 0, batch: 762, loss: 0.8736302256584167\n",
      "epoch: 0, batch: 763, loss: 0.8793299794197083\n",
      "epoch: 0, batch: 764, loss: 0.8207505345344543\n",
      "epoch: 0, batch: 765, loss: 0.9917359352111816\n",
      "epoch: 0, batch: 766, loss: 0.8316029906272888\n",
      "epoch: 0, batch: 767, loss: 0.7314038872718811\n",
      "epoch: 0, batch: 768, loss: 1.0392452478408813\n",
      "epoch: 0, batch: 769, loss: 1.0843453407287598\n",
      "epoch: 0, batch: 770, loss: 0.9144071340560913\n",
      "epoch: 0, batch: 771, loss: 0.9310908317565918\n",
      "epoch: 0, batch: 772, loss: 0.9310715794563293\n",
      "epoch: 0, batch: 773, loss: 0.7296987771987915\n",
      "epoch: 0, batch: 774, loss: 0.7793765068054199\n",
      "epoch: 0, batch: 775, loss: 0.8273530006408691\n",
      "epoch: 0, batch: 776, loss: 0.9949772357940674\n",
      "epoch: 0, batch: 777, loss: 0.8210340738296509\n",
      "epoch: 0, batch: 778, loss: 0.767399251461029\n",
      "epoch: 0, batch: 779, loss: 0.8319833278656006\n",
      "epoch: 0, batch: 780, loss: 0.9520667791366577\n",
      "epoch: 0, batch: 781, loss: 0.7913885116577148\n",
      "epoch: 0, batch: 782, loss: 0.9570544958114624\n",
      "epoch: 0, batch: 783, loss: 0.9920508861541748\n",
      "epoch: 0, batch: 784, loss: 0.7204325199127197\n",
      "epoch: 0, batch: 785, loss: 1.0078072547912598\n",
      "epoch: 0, batch: 786, loss: 0.8085175156593323\n",
      "epoch: 0, batch: 787, loss: 0.8621098399162292\n",
      "epoch: 0, batch: 788, loss: 0.8201413750648499\n",
      "epoch: 0, batch: 789, loss: 0.907278299331665\n",
      "epoch: 0, batch: 790, loss: 0.7437716722488403\n",
      "epoch: 0, batch: 791, loss: 1.0209753513336182\n",
      "epoch: 0, batch: 792, loss: 0.8156928420066833\n",
      "epoch: 0, batch: 793, loss: 0.918171763420105\n",
      "epoch: 0, batch: 794, loss: 1.0116721391677856\n",
      "epoch: 0, batch: 795, loss: 0.7138282656669617\n",
      "epoch: 0, batch: 796, loss: 1.061773419380188\n",
      "epoch: 0, batch: 797, loss: 0.8166926503181458\n",
      "epoch: 0, batch: 798, loss: 0.7122847437858582\n",
      "epoch: 0, batch: 799, loss: 0.872518002986908\n",
      "epoch: 0, batch: 800, loss: 0.7733100652694702\n",
      "epoch: 0, batch: 801, loss: 0.9367648363113403\n",
      "epoch: 0, batch: 802, loss: 0.7298673391342163\n",
      "epoch: 0, batch: 803, loss: 0.9985866546630859\n",
      "epoch: 0, batch: 804, loss: 0.6144055128097534\n",
      "epoch: 0, batch: 805, loss: 1.1055396795272827\n",
      "epoch: 0, batch: 806, loss: 0.9417568445205688\n",
      "epoch: 0, batch: 807, loss: 0.698163628578186\n",
      "epoch: 0, batch: 808, loss: 0.9878745675086975\n",
      "epoch: 0, batch: 809, loss: 0.8257641196250916\n",
      "epoch: 0, batch: 810, loss: 0.5010460019111633\n",
      "epoch: 0, batch: 811, loss: 1.0144492387771606\n",
      "epoch: 0, batch: 812, loss: 0.8622191548347473\n",
      "epoch: 0, batch: 813, loss: 0.7379080653190613\n",
      "epoch: 0, batch: 814, loss: 0.8009878993034363\n",
      "epoch: 0, batch: 815, loss: 1.0145071744918823\n",
      "epoch: 0, batch: 816, loss: 0.8085742592811584\n",
      "epoch: 0, batch: 817, loss: 0.8696244955062866\n",
      "epoch: 0, batch: 818, loss: 1.0359529256820679\n",
      "epoch: 0, batch: 819, loss: 0.9373522996902466\n",
      "epoch: 0, batch: 820, loss: 0.7007684707641602\n",
      "epoch: 0, batch: 821, loss: 0.8293514847755432\n",
      "epoch: 0, batch: 822, loss: 0.9844838976860046\n",
      "epoch: 0, batch: 823, loss: 0.6210836172103882\n",
      "epoch: 0, batch: 824, loss: 0.638741135597229\n",
      "epoch: 0, batch: 825, loss: 0.8186473846435547\n",
      "epoch: 0, batch: 826, loss: 0.9103995561599731\n",
      "epoch: 0, batch: 827, loss: 0.7232823371887207\n",
      "epoch: 0, batch: 828, loss: 0.8326678276062012\n",
      "epoch: 0, batch: 829, loss: 0.7276871800422668\n",
      "epoch: 0, batch: 830, loss: 0.8048506379127502\n",
      "epoch: 0, batch: 831, loss: 0.7570379376411438\n",
      "epoch: 0, batch: 832, loss: 0.8531386256217957\n",
      "epoch: 0, batch: 833, loss: 0.7551230788230896\n",
      "epoch: 0, batch: 834, loss: 1.1575968265533447\n",
      "epoch: 0, batch: 835, loss: 0.7569956183433533\n",
      "epoch: 0, batch: 836, loss: 0.8982582688331604\n",
      "epoch: 0, batch: 837, loss: 0.9591445326805115\n",
      "epoch: 0, batch: 838, loss: 0.7054535150527954\n",
      "epoch: 0, batch: 839, loss: 0.6166959404945374\n",
      "epoch: 0, batch: 840, loss: 0.6846300959587097\n",
      "epoch: 0, batch: 841, loss: 0.5743656754493713\n",
      "epoch: 0, batch: 842, loss: 1.056168794631958\n",
      "epoch: 0, batch: 843, loss: 1.042522668838501\n",
      "epoch: 0, batch: 844, loss: 0.763545572757721\n",
      "epoch: 0, batch: 845, loss: 0.8539290428161621\n",
      "epoch: 0, batch: 846, loss: 0.6967262029647827\n",
      "epoch: 0, batch: 847, loss: 0.8934682011604309\n",
      "epoch: 0, batch: 848, loss: 0.7028045654296875\n",
      "epoch: 0, batch: 849, loss: 0.7113271951675415\n",
      "epoch: 0, batch: 850, loss: 0.8264462351799011\n",
      "epoch: 0, batch: 851, loss: 0.7329646348953247\n",
      "epoch: 0, batch: 852, loss: 1.0249449014663696\n",
      "epoch: 0, batch: 853, loss: 0.8639369010925293\n",
      "epoch: 0, batch: 854, loss: 0.7893621921539307\n",
      "epoch: 0, batch: 855, loss: 0.6515721082687378\n",
      "epoch: 0, batch: 856, loss: 0.5978013873100281\n",
      "epoch: 0, batch: 857, loss: 0.857040286064148\n",
      "epoch: 0, batch: 858, loss: 0.6761298179626465\n",
      "epoch: 0, batch: 859, loss: 0.9232618808746338\n",
      "epoch: 0, batch: 860, loss: 0.9573697447776794\n",
      "epoch: 0, batch: 861, loss: 0.7799921631813049\n",
      "epoch: 0, batch: 862, loss: 0.8657186031341553\n",
      "epoch: 0, batch: 863, loss: 0.9361217021942139\n",
      "epoch: 0, batch: 864, loss: 0.9095546007156372\n",
      "epoch: 0, batch: 865, loss: 0.7737564444541931\n",
      "epoch: 0, batch: 866, loss: 0.7891449332237244\n",
      "epoch: 0, batch: 867, loss: 0.769099235534668\n",
      "epoch: 0, batch: 868, loss: 0.6985125541687012\n",
      "epoch: 0, batch: 869, loss: 0.8833286166191101\n",
      "epoch: 0, batch: 870, loss: 0.9419587850570679\n",
      "epoch: 0, batch: 871, loss: 1.0348981618881226\n",
      "epoch: 0, batch: 872, loss: 0.8061421513557434\n",
      "epoch: 0, batch: 873, loss: 0.6477019786834717\n",
      "epoch: 0, batch: 874, loss: 0.920587420463562\n",
      "epoch: 0, batch: 875, loss: 0.693719208240509\n",
      "epoch: 0, batch: 876, loss: 0.5949795246124268\n",
      "epoch: 0, batch: 877, loss: 0.9549435377120972\n",
      "epoch: 0, batch: 878, loss: 0.7971458435058594\n",
      "epoch: 0, batch: 879, loss: 0.8334133625030518\n",
      "epoch: 0, batch: 880, loss: 0.7219133377075195\n",
      "epoch: 0, batch: 881, loss: 0.9809966087341309\n",
      "epoch: 0, batch: 882, loss: 0.9629953503608704\n",
      "epoch: 0, batch: 883, loss: 0.8613925576210022\n",
      "epoch: 0, batch: 884, loss: 0.7529352903366089\n",
      "epoch: 0, batch: 885, loss: 0.9125173091888428\n",
      "epoch: 0, batch: 886, loss: 0.7391979694366455\n",
      "epoch: 0, batch: 887, loss: 0.6782207489013672\n",
      "epoch: 0, batch: 888, loss: 0.6847764849662781\n",
      "epoch: 0, batch: 889, loss: 1.0099706649780273\n",
      "epoch: 0, batch: 890, loss: 0.622573733329773\n",
      "epoch: 0, batch: 891, loss: 0.7858290076255798\n",
      "epoch: 0, batch: 892, loss: 0.7733721733093262\n",
      "epoch: 0, batch: 893, loss: 0.7859217524528503\n",
      "epoch: 0, batch: 894, loss: 0.9257200360298157\n",
      "epoch: 0, batch: 895, loss: 0.6577969789505005\n",
      "epoch: 0, batch: 896, loss: 0.8264603614807129\n",
      "epoch: 0, batch: 897, loss: 0.684983491897583\n",
      "epoch: 0, batch: 898, loss: 0.642943799495697\n",
      "epoch: 0, batch: 899, loss: 0.5908961296081543\n",
      "epoch: 0, batch: 900, loss: 0.7056686878204346\n",
      "epoch: 0, batch: 901, loss: 0.8729727268218994\n",
      "epoch: 0, batch: 902, loss: 0.8815670013427734\n",
      "epoch: 0, batch: 903, loss: 0.8843292593955994\n",
      "epoch: 0, batch: 904, loss: 0.6348163485527039\n",
      "epoch: 0, batch: 905, loss: 0.8035118579864502\n",
      "epoch: 0, batch: 906, loss: 0.8122889995574951\n",
      "epoch: 0, batch: 907, loss: 0.697624146938324\n",
      "epoch: 0, batch: 908, loss: 0.6628395318984985\n",
      "epoch: 0, batch: 909, loss: 0.7898196578025818\n",
      "epoch: 0, batch: 910, loss: 0.8444409966468811\n",
      "epoch: 0, batch: 911, loss: 0.7847760319709778\n",
      "epoch: 0, batch: 912, loss: 0.7391737699508667\n",
      "epoch: 0, batch: 913, loss: 0.689985454082489\n",
      "epoch: 0, batch: 914, loss: 0.8126125931739807\n",
      "epoch: 0, batch: 915, loss: 0.7094106674194336\n",
      "epoch: 0, batch: 916, loss: 0.809567391872406\n",
      "epoch: 0, batch: 917, loss: 0.6764683127403259\n",
      "epoch: 0, batch: 918, loss: 0.6609057784080505\n",
      "epoch: 0, batch: 919, loss: 0.800850510597229\n",
      "epoch: 0, batch: 920, loss: 0.8093197345733643\n",
      "epoch: 0, batch: 921, loss: 0.7260700464248657\n",
      "epoch: 0, batch: 922, loss: 0.8667657375335693\n",
      "epoch: 0, batch: 923, loss: 0.8506861329078674\n",
      "epoch: 0, batch: 924, loss: 0.8207129240036011\n",
      "epoch: 0, batch: 925, loss: 0.619094729423523\n",
      "epoch: 0, batch: 926, loss: 0.7341934442520142\n",
      "epoch: 0, batch: 927, loss: 0.8600185513496399\n",
      "epoch: 0, batch: 928, loss: 0.6712809205055237\n",
      "epoch: 0, batch: 929, loss: 0.7627301216125488\n",
      "epoch: 0, batch: 930, loss: 0.9898709058761597\n",
      "epoch: 0, batch: 931, loss: 0.8902387022972107\n",
      "epoch: 0, batch: 932, loss: 0.7740866541862488\n",
      "epoch: 0, batch: 933, loss: 0.7720074653625488\n",
      "epoch: 0, batch: 934, loss: 0.736851692199707\n",
      "epoch: 0, batch: 935, loss: 0.7795073390007019\n",
      "epoch: 0, batch: 936, loss: 0.5224252939224243\n",
      "epoch: 0, batch: 937, loss: 0.6599394083023071\n",
      "epoch: 1, batch: 0, loss: 0.7079959511756897\n",
      "epoch: 1, batch: 1, loss: 0.8819655179977417\n",
      "epoch: 1, batch: 2, loss: 0.6855202913284302\n",
      "epoch: 1, batch: 3, loss: 0.7211542725563049\n",
      "epoch: 1, batch: 4, loss: 0.7954926490783691\n",
      "epoch: 1, batch: 5, loss: 0.8216951489448547\n",
      "epoch: 1, batch: 6, loss: 0.7981327176094055\n",
      "epoch: 1, batch: 7, loss: 0.7432230114936829\n",
      "epoch: 1, batch: 8, loss: 0.7072839736938477\n",
      "epoch: 1, batch: 9, loss: 0.9972004294395447\n",
      "epoch: 1, batch: 10, loss: 0.6669604182243347\n",
      "epoch: 1, batch: 11, loss: 0.4419623017311096\n",
      "epoch: 1, batch: 12, loss: 0.6344605088233948\n",
      "epoch: 1, batch: 13, loss: 0.5466502904891968\n",
      "epoch: 1, batch: 14, loss: 0.8312015533447266\n",
      "epoch: 1, batch: 15, loss: 0.7583919763565063\n",
      "epoch: 1, batch: 16, loss: 0.8062472343444824\n",
      "epoch: 1, batch: 17, loss: 0.697486937046051\n",
      "epoch: 1, batch: 18, loss: 0.7211351990699768\n",
      "epoch: 1, batch: 19, loss: 0.6112479567527771\n",
      "epoch: 1, batch: 20, loss: 0.6406702399253845\n",
      "epoch: 1, batch: 21, loss: 0.9021409153938293\n",
      "epoch: 1, batch: 22, loss: 0.7418801188468933\n",
      "epoch: 1, batch: 23, loss: 0.5626273155212402\n",
      "epoch: 1, batch: 24, loss: 0.6576290130615234\n",
      "epoch: 1, batch: 25, loss: 0.7318114638328552\n",
      "epoch: 1, batch: 26, loss: 0.7161855101585388\n",
      "epoch: 1, batch: 27, loss: 0.6556843519210815\n",
      "epoch: 1, batch: 28, loss: 0.8856092691421509\n",
      "epoch: 1, batch: 29, loss: 0.5016912221908569\n",
      "epoch: 1, batch: 30, loss: 0.8199018836021423\n",
      "epoch: 1, batch: 31, loss: 0.755587100982666\n",
      "epoch: 1, batch: 32, loss: 0.7738279104232788\n",
      "epoch: 1, batch: 33, loss: 0.7369759678840637\n",
      "epoch: 1, batch: 34, loss: 0.5657103657722473\n",
      "epoch: 1, batch: 35, loss: 0.611787736415863\n",
      "epoch: 1, batch: 36, loss: 0.8309395909309387\n",
      "epoch: 1, batch: 37, loss: 0.596401572227478\n",
      "epoch: 1, batch: 38, loss: 0.9808370471000671\n",
      "epoch: 1, batch: 39, loss: 0.6989585757255554\n",
      "epoch: 1, batch: 40, loss: 1.0182533264160156\n",
      "epoch: 1, batch: 41, loss: 0.7053022384643555\n",
      "epoch: 1, batch: 42, loss: 0.7377974390983582\n",
      "epoch: 1, batch: 43, loss: 1.0914113521575928\n",
      "epoch: 1, batch: 44, loss: 0.5734846591949463\n",
      "epoch: 1, batch: 45, loss: 0.5962238311767578\n",
      "epoch: 1, batch: 46, loss: 0.5635243058204651\n",
      "epoch: 1, batch: 47, loss: 0.5458155870437622\n",
      "epoch: 1, batch: 48, loss: 0.7768910527229309\n",
      "epoch: 1, batch: 49, loss: 0.8113032579421997\n",
      "epoch: 1, batch: 50, loss: 0.6209229230880737\n",
      "epoch: 1, batch: 51, loss: 0.45752882957458496\n",
      "epoch: 1, batch: 52, loss: 0.46962663531303406\n",
      "epoch: 1, batch: 53, loss: 0.5416639447212219\n",
      "epoch: 1, batch: 54, loss: 0.7905109524726868\n",
      "epoch: 1, batch: 55, loss: 0.7285172343254089\n",
      "epoch: 1, batch: 56, loss: 0.7669569849967957\n",
      "epoch: 1, batch: 57, loss: 0.7662729024887085\n",
      "epoch: 1, batch: 58, loss: 0.6616008281707764\n",
      "epoch: 1, batch: 59, loss: 0.5813221335411072\n",
      "epoch: 1, batch: 60, loss: 0.6859675645828247\n",
      "epoch: 1, batch: 61, loss: 0.6173232197761536\n",
      "epoch: 1, batch: 62, loss: 0.6780707240104675\n",
      "epoch: 1, batch: 63, loss: 0.6860690116882324\n",
      "epoch: 1, batch: 64, loss: 0.431927889585495\n",
      "epoch: 1, batch: 65, loss: 0.822928786277771\n",
      "epoch: 1, batch: 66, loss: 0.44577741622924805\n",
      "epoch: 1, batch: 67, loss: 0.6900772452354431\n",
      "epoch: 1, batch: 68, loss: 0.7202692031860352\n",
      "epoch: 1, batch: 69, loss: 0.49492955207824707\n",
      "epoch: 1, batch: 70, loss: 0.6710442304611206\n",
      "epoch: 1, batch: 71, loss: 0.7377985119819641\n",
      "epoch: 1, batch: 72, loss: 0.4305608868598938\n",
      "epoch: 1, batch: 73, loss: 0.8071950674057007\n",
      "epoch: 1, batch: 74, loss: 0.7209866642951965\n",
      "epoch: 1, batch: 75, loss: 0.4588126540184021\n",
      "epoch: 1, batch: 76, loss: 0.7522919178009033\n",
      "epoch: 1, batch: 77, loss: 0.7215521335601807\n",
      "epoch: 1, batch: 78, loss: 0.570260763168335\n",
      "epoch: 1, batch: 79, loss: 0.7907796502113342\n",
      "epoch: 1, batch: 80, loss: 0.6959058046340942\n",
      "epoch: 1, batch: 81, loss: 0.6487531065940857\n",
      "epoch: 1, batch: 82, loss: 0.623761773109436\n",
      "epoch: 1, batch: 83, loss: 0.8460521101951599\n",
      "epoch: 1, batch: 84, loss: 0.5987787842750549\n",
      "epoch: 1, batch: 85, loss: 0.7017691135406494\n",
      "epoch: 1, batch: 86, loss: 0.5494239330291748\n",
      "epoch: 1, batch: 87, loss: 0.8771458268165588\n",
      "epoch: 1, batch: 88, loss: 0.7457634210586548\n",
      "epoch: 1, batch: 89, loss: 0.7557032704353333\n",
      "epoch: 1, batch: 90, loss: 0.8867077827453613\n",
      "epoch: 1, batch: 91, loss: 0.688117504119873\n",
      "epoch: 1, batch: 92, loss: 0.7453474402427673\n",
      "epoch: 1, batch: 93, loss: 0.7707966566085815\n",
      "epoch: 1, batch: 94, loss: 0.8592097759246826\n",
      "epoch: 1, batch: 95, loss: 0.5810142159461975\n",
      "epoch: 1, batch: 96, loss: 0.5589486360549927\n",
      "epoch: 1, batch: 97, loss: 0.736102283000946\n",
      "epoch: 1, batch: 98, loss: 0.7214032411575317\n",
      "epoch: 1, batch: 99, loss: 0.4523201882839203\n",
      "epoch: 1, batch: 100, loss: 0.4507533609867096\n",
      "epoch: 1, batch: 101, loss: 0.6739280819892883\n",
      "epoch: 1, batch: 102, loss: 0.6386770009994507\n",
      "epoch: 1, batch: 103, loss: 0.7184017896652222\n",
      "epoch: 1, batch: 104, loss: 0.47288432717323303\n",
      "epoch: 1, batch: 105, loss: 0.6060193181037903\n",
      "epoch: 1, batch: 106, loss: 0.86472487449646\n",
      "epoch: 1, batch: 107, loss: 0.7850931286811829\n",
      "epoch: 1, batch: 108, loss: 0.7387941479682922\n",
      "epoch: 1, batch: 109, loss: 0.6152628660202026\n",
      "epoch: 1, batch: 110, loss: 0.6434507966041565\n",
      "epoch: 1, batch: 111, loss: 0.6550379395484924\n",
      "epoch: 1, batch: 112, loss: 0.6313242316246033\n",
      "epoch: 1, batch: 113, loss: 0.6697510480880737\n",
      "epoch: 1, batch: 114, loss: 0.7642493844032288\n",
      "epoch: 1, batch: 115, loss: 0.5382149815559387\n",
      "epoch: 1, batch: 116, loss: 0.6553055644035339\n",
      "epoch: 1, batch: 117, loss: 0.7117444276809692\n",
      "epoch: 1, batch: 118, loss: 0.6281591653823853\n",
      "epoch: 1, batch: 119, loss: 0.7398743033409119\n",
      "epoch: 1, batch: 120, loss: 0.4877852499485016\n",
      "epoch: 1, batch: 121, loss: 0.4967881143093109\n",
      "epoch: 1, batch: 122, loss: 0.6501513719558716\n",
      "epoch: 1, batch: 123, loss: 0.6119269728660583\n",
      "epoch: 1, batch: 124, loss: 0.6867326498031616\n",
      "epoch: 1, batch: 125, loss: 0.6313923597335815\n",
      "epoch: 1, batch: 126, loss: 0.4963323771953583\n",
      "epoch: 1, batch: 127, loss: 0.495047003030777\n",
      "epoch: 1, batch: 128, loss: 0.6045950055122375\n",
      "epoch: 1, batch: 129, loss: 0.5287649631500244\n",
      "epoch: 1, batch: 130, loss: 0.5088710188865662\n",
      "epoch: 1, batch: 131, loss: 0.5377726554870605\n",
      "epoch: 1, batch: 132, loss: 0.7340760231018066\n",
      "epoch: 1, batch: 133, loss: 0.5934810638427734\n",
      "epoch: 1, batch: 134, loss: 0.5531591176986694\n",
      "epoch: 1, batch: 135, loss: 0.678480863571167\n",
      "epoch: 1, batch: 136, loss: 0.39264869689941406\n",
      "epoch: 1, batch: 137, loss: 0.657630443572998\n",
      "epoch: 1, batch: 138, loss: 0.7249490022659302\n",
      "epoch: 1, batch: 139, loss: 0.6255647540092468\n",
      "epoch: 1, batch: 140, loss: 0.6151841282844543\n",
      "epoch: 1, batch: 141, loss: 0.5826727151870728\n",
      "epoch: 1, batch: 142, loss: 0.8140800595283508\n",
      "epoch: 1, batch: 143, loss: 0.4541415870189667\n",
      "epoch: 1, batch: 144, loss: 0.5006674528121948\n",
      "epoch: 1, batch: 145, loss: 0.5908263921737671\n",
      "epoch: 1, batch: 146, loss: 0.6052286028862\n",
      "epoch: 1, batch: 147, loss: 0.73759925365448\n",
      "epoch: 1, batch: 148, loss: 0.4859269857406616\n",
      "epoch: 1, batch: 149, loss: 0.7242414951324463\n",
      "epoch: 1, batch: 150, loss: 0.783033013343811\n",
      "epoch: 1, batch: 151, loss: 0.774803876876831\n",
      "epoch: 1, batch: 152, loss: 0.7491216659545898\n",
      "epoch: 1, batch: 153, loss: 0.6189088225364685\n",
      "epoch: 1, batch: 154, loss: 0.6817864775657654\n",
      "epoch: 1, batch: 155, loss: 0.7173691391944885\n",
      "epoch: 1, batch: 156, loss: 0.7313889861106873\n",
      "epoch: 1, batch: 157, loss: 0.6534629464149475\n",
      "epoch: 1, batch: 158, loss: 0.7112954258918762\n",
      "epoch: 1, batch: 159, loss: 0.7041782140731812\n",
      "epoch: 1, batch: 160, loss: 0.8063718676567078\n",
      "epoch: 1, batch: 161, loss: 0.991012692451477\n",
      "epoch: 1, batch: 162, loss: 0.676944375038147\n",
      "epoch: 1, batch: 163, loss: 0.6189790964126587\n",
      "epoch: 1, batch: 164, loss: 0.6929304003715515\n",
      "epoch: 1, batch: 165, loss: 0.7957834005355835\n",
      "epoch: 1, batch: 166, loss: 0.5525166988372803\n",
      "epoch: 1, batch: 167, loss: 0.7416701912879944\n",
      "epoch: 1, batch: 168, loss: 0.49482715129852295\n",
      "epoch: 1, batch: 169, loss: 0.8497042059898376\n",
      "epoch: 1, batch: 170, loss: 0.7902563810348511\n",
      "epoch: 1, batch: 171, loss: 0.8064892888069153\n",
      "epoch: 1, batch: 172, loss: 0.8246392607688904\n",
      "epoch: 1, batch: 173, loss: 0.8583431243896484\n",
      "epoch: 1, batch: 174, loss: 0.7548020482063293\n",
      "epoch: 1, batch: 175, loss: 0.6839151382446289\n",
      "epoch: 1, batch: 176, loss: 0.5718873739242554\n",
      "epoch: 1, batch: 177, loss: 0.6794887781143188\n",
      "epoch: 1, batch: 178, loss: 0.5872071385383606\n",
      "epoch: 1, batch: 179, loss: 0.7611052989959717\n",
      "epoch: 1, batch: 180, loss: 0.663004994392395\n",
      "epoch: 1, batch: 181, loss: 0.6790629625320435\n",
      "epoch: 1, batch: 182, loss: 0.7693026661872864\n",
      "epoch: 1, batch: 183, loss: 0.6964756846427917\n",
      "epoch: 1, batch: 184, loss: 0.5420438647270203\n",
      "epoch: 1, batch: 185, loss: 0.5778558254241943\n",
      "epoch: 1, batch: 186, loss: 0.7575827836990356\n",
      "epoch: 1, batch: 187, loss: 0.6701025366783142\n",
      "epoch: 1, batch: 188, loss: 0.6598995327949524\n",
      "epoch: 1, batch: 189, loss: 0.6408805847167969\n",
      "epoch: 1, batch: 190, loss: 0.5246407389640808\n",
      "epoch: 1, batch: 191, loss: 0.6414129734039307\n",
      "epoch: 1, batch: 192, loss: 0.4759978950023651\n",
      "epoch: 1, batch: 193, loss: 0.45550721883773804\n",
      "epoch: 1, batch: 194, loss: 0.524117112159729\n",
      "epoch: 1, batch: 195, loss: 0.6160045862197876\n",
      "epoch: 1, batch: 196, loss: 0.4242766201496124\n",
      "epoch: 1, batch: 197, loss: 0.49746042490005493\n",
      "epoch: 1, batch: 198, loss: 0.6551571488380432\n",
      "epoch: 1, batch: 199, loss: 0.5758655667304993\n",
      "epoch: 1, batch: 200, loss: 0.38723620772361755\n",
      "epoch: 1, batch: 201, loss: 0.5802836418151855\n",
      "epoch: 1, batch: 202, loss: 0.4013877213001251\n",
      "epoch: 1, batch: 203, loss: 0.581935465335846\n",
      "epoch: 1, batch: 204, loss: 0.5690797567367554\n",
      "epoch: 1, batch: 205, loss: 0.7071256041526794\n",
      "epoch: 1, batch: 206, loss: 0.582324206829071\n",
      "epoch: 1, batch: 207, loss: 0.40071579813957214\n",
      "epoch: 1, batch: 208, loss: 0.6156429648399353\n",
      "epoch: 1, batch: 209, loss: 0.6113400459289551\n",
      "epoch: 1, batch: 210, loss: 0.6253100037574768\n",
      "epoch: 1, batch: 211, loss: 0.4710935354232788\n",
      "epoch: 1, batch: 212, loss: 0.9085134267807007\n",
      "epoch: 1, batch: 213, loss: 0.5919879078865051\n",
      "epoch: 1, batch: 214, loss: 0.7477908134460449\n",
      "epoch: 1, batch: 215, loss: 0.6252138614654541\n",
      "epoch: 1, batch: 216, loss: 0.9941850900650024\n",
      "epoch: 1, batch: 217, loss: 0.5211060047149658\n",
      "epoch: 1, batch: 218, loss: 0.44693905115127563\n",
      "epoch: 1, batch: 219, loss: 0.5661017298698425\n",
      "epoch: 1, batch: 220, loss: 0.7369494438171387\n",
      "epoch: 1, batch: 221, loss: 0.6641851663589478\n",
      "epoch: 1, batch: 222, loss: 0.614773154258728\n",
      "epoch: 1, batch: 223, loss: 0.7682902216911316\n",
      "epoch: 1, batch: 224, loss: 0.4336221218109131\n",
      "epoch: 1, batch: 225, loss: 0.6519441604614258\n",
      "epoch: 1, batch: 226, loss: 0.514386773109436\n",
      "epoch: 1, batch: 227, loss: 0.557668149471283\n",
      "epoch: 1, batch: 228, loss: 0.609358549118042\n",
      "epoch: 1, batch: 229, loss: 0.7654302716255188\n",
      "epoch: 1, batch: 230, loss: 0.5155930519104004\n",
      "epoch: 1, batch: 231, loss: 0.5492202043533325\n",
      "epoch: 1, batch: 232, loss: 0.5397149324417114\n",
      "epoch: 1, batch: 233, loss: 0.5846336483955383\n",
      "epoch: 1, batch: 234, loss: 0.48902428150177\n",
      "epoch: 1, batch: 235, loss: 0.35249564051628113\n",
      "epoch: 1, batch: 236, loss: 0.5990241169929504\n",
      "epoch: 1, batch: 237, loss: 0.6745229959487915\n",
      "epoch: 1, batch: 238, loss: 0.6533157229423523\n",
      "epoch: 1, batch: 239, loss: 0.5009766817092896\n",
      "epoch: 1, batch: 240, loss: 0.7144226431846619\n",
      "epoch: 1, batch: 241, loss: 0.6189563274383545\n",
      "epoch: 1, batch: 242, loss: 0.4813389778137207\n",
      "epoch: 1, batch: 243, loss: 0.6458747386932373\n",
      "epoch: 1, batch: 244, loss: 1.0098319053649902\n",
      "epoch: 1, batch: 245, loss: 0.6633057594299316\n",
      "epoch: 1, batch: 246, loss: 0.6753842830657959\n",
      "epoch: 1, batch: 247, loss: 0.634163498878479\n",
      "epoch: 1, batch: 248, loss: 0.694750189781189\n",
      "epoch: 1, batch: 249, loss: 0.4940476417541504\n",
      "epoch: 1, batch: 250, loss: 0.7158843874931335\n",
      "epoch: 1, batch: 251, loss: 0.46616601943969727\n",
      "epoch: 1, batch: 252, loss: 0.671061635017395\n",
      "epoch: 1, batch: 253, loss: 0.5233588218688965\n",
      "epoch: 1, batch: 254, loss: 0.665282666683197\n",
      "epoch: 1, batch: 255, loss: 0.7961642146110535\n",
      "epoch: 1, batch: 256, loss: 0.6390506029129028\n",
      "epoch: 1, batch: 257, loss: 0.5300533175468445\n",
      "epoch: 1, batch: 258, loss: 0.5905027985572815\n",
      "epoch: 1, batch: 259, loss: 0.7458596229553223\n",
      "epoch: 1, batch: 260, loss: 0.5268425345420837\n",
      "epoch: 1, batch: 261, loss: 0.6111549139022827\n",
      "epoch: 1, batch: 262, loss: 0.6656287908554077\n",
      "epoch: 1, batch: 263, loss: 0.7113143801689148\n",
      "epoch: 1, batch: 264, loss: 0.7518162131309509\n",
      "epoch: 1, batch: 265, loss: 0.5027432441711426\n",
      "epoch: 1, batch: 266, loss: 0.7016900777816772\n",
      "epoch: 1, batch: 267, loss: 0.838624119758606\n",
      "epoch: 1, batch: 268, loss: 0.5012309551239014\n",
      "epoch: 1, batch: 269, loss: 0.4682130813598633\n",
      "epoch: 1, batch: 270, loss: 0.423336386680603\n",
      "epoch: 1, batch: 271, loss: 0.4752780497074127\n",
      "epoch: 1, batch: 272, loss: 0.5553265810012817\n",
      "epoch: 1, batch: 273, loss: 0.4940676987171173\n",
      "epoch: 1, batch: 274, loss: 0.6066568493843079\n",
      "epoch: 1, batch: 275, loss: 0.48266535997390747\n",
      "epoch: 1, batch: 276, loss: 0.5338912010192871\n",
      "epoch: 1, batch: 277, loss: 0.4350430369377136\n",
      "epoch: 1, batch: 278, loss: 0.6352217793464661\n",
      "epoch: 1, batch: 279, loss: 0.6392186284065247\n",
      "epoch: 1, batch: 280, loss: 0.607530951499939\n",
      "epoch: 1, batch: 281, loss: 0.40180155634880066\n",
      "epoch: 1, batch: 282, loss: 0.5735634565353394\n",
      "epoch: 1, batch: 283, loss: 0.49200257658958435\n",
      "epoch: 1, batch: 284, loss: 0.7202339172363281\n",
      "epoch: 1, batch: 285, loss: 0.6133996844291687\n",
      "epoch: 1, batch: 286, loss: 0.6347228288650513\n",
      "epoch: 1, batch: 287, loss: 0.6360974907875061\n",
      "epoch: 1, batch: 288, loss: 0.45340508222579956\n",
      "epoch: 1, batch: 289, loss: 0.49993085861206055\n",
      "epoch: 1, batch: 290, loss: 0.38398298621177673\n",
      "epoch: 1, batch: 291, loss: 0.6539437770843506\n",
      "epoch: 1, batch: 292, loss: 0.40756046772003174\n",
      "epoch: 1, batch: 293, loss: 0.5170567631721497\n",
      "epoch: 1, batch: 294, loss: 0.5288383960723877\n",
      "epoch: 1, batch: 295, loss: 0.5691035985946655\n",
      "epoch: 1, batch: 296, loss: 0.543796181678772\n",
      "epoch: 1, batch: 297, loss: 0.5211603045463562\n",
      "epoch: 1, batch: 298, loss: 0.5569950342178345\n",
      "epoch: 1, batch: 299, loss: 0.5513290762901306\n",
      "epoch: 1, batch: 300, loss: 0.5308895111083984\n",
      "epoch: 1, batch: 301, loss: 0.43381601572036743\n",
      "epoch: 1, batch: 302, loss: 0.4788053631782532\n",
      "epoch: 1, batch: 303, loss: 0.6097363829612732\n",
      "epoch: 1, batch: 304, loss: 0.4795534908771515\n",
      "epoch: 1, batch: 305, loss: 0.3479723036289215\n",
      "epoch: 1, batch: 306, loss: 0.37001630663871765\n",
      "epoch: 1, batch: 307, loss: 0.6746386885643005\n",
      "epoch: 1, batch: 308, loss: 0.6572492122650146\n",
      "epoch: 1, batch: 309, loss: 0.43393683433532715\n",
      "epoch: 1, batch: 310, loss: 0.562170147895813\n",
      "epoch: 1, batch: 311, loss: 0.3172786235809326\n",
      "epoch: 1, batch: 312, loss: 0.39533114433288574\n",
      "epoch: 1, batch: 313, loss: 0.5087426900863647\n",
      "epoch: 1, batch: 314, loss: 0.3669472932815552\n",
      "epoch: 1, batch: 315, loss: 0.45658865571022034\n",
      "epoch: 1, batch: 316, loss: 0.4776822030544281\n",
      "epoch: 1, batch: 317, loss: 0.5785536766052246\n",
      "epoch: 1, batch: 318, loss: 0.3312673568725586\n",
      "epoch: 1, batch: 319, loss: 0.5798473954200745\n",
      "epoch: 1, batch: 320, loss: 0.34314125776290894\n",
      "epoch: 1, batch: 321, loss: 0.6701613068580627\n",
      "epoch: 1, batch: 322, loss: 0.5614933371543884\n",
      "epoch: 1, batch: 323, loss: 0.8508366942405701\n",
      "epoch: 1, batch: 324, loss: 0.3786144256591797\n",
      "epoch: 1, batch: 325, loss: 0.6365959644317627\n",
      "epoch: 1, batch: 326, loss: 0.3491629660129547\n",
      "epoch: 1, batch: 327, loss: 0.41764625906944275\n",
      "epoch: 1, batch: 328, loss: 0.5075510144233704\n",
      "epoch: 1, batch: 329, loss: 0.3668356239795685\n",
      "epoch: 1, batch: 330, loss: 0.5299023389816284\n",
      "epoch: 1, batch: 331, loss: 0.4598168730735779\n",
      "epoch: 1, batch: 332, loss: 0.38905611634254456\n",
      "epoch: 1, batch: 333, loss: 0.5024468302726746\n",
      "epoch: 1, batch: 334, loss: 0.3084196448326111\n",
      "epoch: 1, batch: 335, loss: 0.4706507623195648\n",
      "epoch: 1, batch: 336, loss: 0.510208010673523\n",
      "epoch: 1, batch: 337, loss: 0.49117979407310486\n",
      "epoch: 1, batch: 338, loss: 0.549297034740448\n",
      "epoch: 1, batch: 339, loss: 0.6150903701782227\n",
      "epoch: 1, batch: 340, loss: 0.6300114393234253\n",
      "epoch: 1, batch: 341, loss: 0.6243268251419067\n",
      "epoch: 1, batch: 342, loss: 0.42009249329566956\n",
      "epoch: 1, batch: 343, loss: 0.46120685338974\n",
      "epoch: 1, batch: 344, loss: 0.42676296830177307\n",
      "epoch: 1, batch: 345, loss: 0.48198333382606506\n",
      "epoch: 1, batch: 346, loss: 0.4591006338596344\n",
      "epoch: 1, batch: 347, loss: 0.5508268475532532\n",
      "epoch: 1, batch: 348, loss: 0.51784747838974\n",
      "epoch: 1, batch: 349, loss: 0.5150370597839355\n",
      "epoch: 1, batch: 350, loss: 0.5166637301445007\n",
      "epoch: 1, batch: 351, loss: 0.5615158677101135\n",
      "epoch: 1, batch: 352, loss: 0.6140041947364807\n",
      "epoch: 1, batch: 353, loss: 0.6965522766113281\n",
      "epoch: 1, batch: 354, loss: 0.3026242256164551\n",
      "epoch: 1, batch: 355, loss: 0.3674967288970947\n",
      "epoch: 1, batch: 356, loss: 0.42055952548980713\n",
      "epoch: 1, batch: 357, loss: 0.429992139339447\n",
      "epoch: 1, batch: 358, loss: 0.604241669178009\n",
      "epoch: 1, batch: 359, loss: 0.5347678065299988\n",
      "epoch: 1, batch: 360, loss: 0.2786073386669159\n",
      "epoch: 1, batch: 361, loss: 0.4138038754463196\n",
      "epoch: 1, batch: 362, loss: 0.5131334662437439\n",
      "epoch: 1, batch: 363, loss: 0.29560190439224243\n",
      "epoch: 1, batch: 364, loss: 0.3530055582523346\n",
      "epoch: 1, batch: 365, loss: 0.49797317385673523\n",
      "epoch: 1, batch: 366, loss: 0.4972990155220032\n",
      "epoch: 1, batch: 367, loss: 0.39882463216781616\n",
      "epoch: 1, batch: 368, loss: 0.5942749381065369\n",
      "epoch: 1, batch: 369, loss: 0.6533713340759277\n",
      "epoch: 1, batch: 370, loss: 0.3383761942386627\n",
      "epoch: 1, batch: 371, loss: 0.4741359055042267\n",
      "epoch: 1, batch: 372, loss: 0.5732333660125732\n",
      "epoch: 1, batch: 373, loss: 0.5713613033294678\n",
      "epoch: 1, batch: 374, loss: 0.5284247398376465\n",
      "epoch: 1, batch: 375, loss: 0.3605872094631195\n",
      "epoch: 1, batch: 376, loss: 0.4424804151058197\n",
      "epoch: 1, batch: 377, loss: 0.4008709192276001\n",
      "epoch: 1, batch: 378, loss: 0.32005977630615234\n",
      "epoch: 1, batch: 379, loss: 0.39603567123413086\n",
      "epoch: 1, batch: 380, loss: 0.447189062833786\n",
      "epoch: 1, batch: 381, loss: 0.5882989764213562\n",
      "epoch: 1, batch: 382, loss: 0.6734607815742493\n",
      "epoch: 1, batch: 383, loss: 0.4094972014427185\n",
      "epoch: 1, batch: 384, loss: 0.4362002909183502\n",
      "epoch: 1, batch: 385, loss: 0.30863335728645325\n",
      "epoch: 1, batch: 386, loss: 0.527991533279419\n",
      "epoch: 1, batch: 387, loss: 0.5988640189170837\n",
      "epoch: 1, batch: 388, loss: 0.4531468152999878\n",
      "epoch: 1, batch: 389, loss: 0.5154819488525391\n",
      "epoch: 1, batch: 390, loss: 0.5032115578651428\n",
      "epoch: 1, batch: 391, loss: 0.6189814805984497\n",
      "epoch: 1, batch: 392, loss: 0.43315327167510986\n",
      "epoch: 1, batch: 393, loss: 0.6082730293273926\n",
      "epoch: 1, batch: 394, loss: 0.50284743309021\n",
      "epoch: 1, batch: 395, loss: 0.33258187770843506\n",
      "epoch: 1, batch: 396, loss: 0.36150187253952026\n",
      "epoch: 1, batch: 397, loss: 0.4884997010231018\n",
      "epoch: 1, batch: 398, loss: 0.5223343968391418\n",
      "epoch: 1, batch: 399, loss: 0.44424864649772644\n",
      "epoch: 1, batch: 400, loss: 0.55671226978302\n",
      "epoch: 1, batch: 401, loss: 0.34439659118652344\n",
      "epoch: 1, batch: 402, loss: 0.5086547136306763\n",
      "epoch: 1, batch: 403, loss: 0.40695810317993164\n",
      "epoch: 1, batch: 404, loss: 0.4537353217601776\n",
      "epoch: 1, batch: 405, loss: 0.5214306712150574\n",
      "epoch: 1, batch: 406, loss: 0.292510986328125\n",
      "epoch: 1, batch: 407, loss: 0.33196064829826355\n",
      "epoch: 1, batch: 408, loss: 0.6345697641372681\n",
      "epoch: 1, batch: 409, loss: 0.5545784831047058\n",
      "epoch: 1, batch: 410, loss: 0.41560155153274536\n",
      "epoch: 1, batch: 411, loss: 0.40123462677001953\n",
      "epoch: 1, batch: 412, loss: 0.6323936581611633\n",
      "epoch: 1, batch: 413, loss: 0.4328683316707611\n",
      "epoch: 1, batch: 414, loss: 0.7286475896835327\n",
      "epoch: 1, batch: 415, loss: 0.33073869347572327\n",
      "epoch: 1, batch: 416, loss: 0.6333531737327576\n",
      "epoch: 1, batch: 417, loss: 0.35653573274612427\n",
      "epoch: 1, batch: 418, loss: 0.4015091061592102\n",
      "epoch: 1, batch: 419, loss: 0.48949670791625977\n",
      "epoch: 1, batch: 420, loss: 0.38850530982017517\n",
      "epoch: 1, batch: 421, loss: 0.5274922847747803\n",
      "epoch: 1, batch: 422, loss: 0.4545840322971344\n",
      "epoch: 1, batch: 423, loss: 0.39162638783454895\n",
      "epoch: 1, batch: 424, loss: 0.6658538579940796\n",
      "epoch: 1, batch: 425, loss: 0.4095867872238159\n",
      "epoch: 1, batch: 426, loss: 0.5228197574615479\n",
      "epoch: 1, batch: 427, loss: 0.3111010789871216\n",
      "epoch: 1, batch: 428, loss: 0.44884270429611206\n",
      "epoch: 1, batch: 429, loss: 0.5446715354919434\n",
      "epoch: 1, batch: 430, loss: 0.3989655375480652\n",
      "epoch: 1, batch: 431, loss: 0.4637320935726166\n",
      "epoch: 1, batch: 432, loss: 0.6539947390556335\n",
      "epoch: 1, batch: 433, loss: 0.4319305717945099\n",
      "epoch: 1, batch: 434, loss: 0.4837767481803894\n",
      "epoch: 1, batch: 435, loss: 0.7465248703956604\n",
      "epoch: 1, batch: 436, loss: 0.4672035872936249\n",
      "epoch: 1, batch: 437, loss: 0.495882511138916\n",
      "epoch: 1, batch: 438, loss: 0.33380258083343506\n",
      "epoch: 1, batch: 439, loss: 0.3605762720108032\n",
      "epoch: 1, batch: 440, loss: 0.43365541100502014\n",
      "epoch: 1, batch: 441, loss: 0.483082115650177\n",
      "epoch: 1, batch: 442, loss: 0.48113641142845154\n",
      "epoch: 1, batch: 443, loss: 0.5347477197647095\n",
      "epoch: 1, batch: 444, loss: 0.48604124784469604\n",
      "epoch: 1, batch: 445, loss: 0.519045889377594\n",
      "epoch: 1, batch: 446, loss: 0.7283569574356079\n",
      "epoch: 1, batch: 447, loss: 0.3109954595565796\n",
      "epoch: 1, batch: 448, loss: 0.4291186034679413\n",
      "epoch: 1, batch: 449, loss: 0.5336685180664062\n",
      "epoch: 1, batch: 450, loss: 0.452268123626709\n",
      "epoch: 1, batch: 451, loss: 0.5156761407852173\n",
      "epoch: 1, batch: 452, loss: 0.6085493564605713\n",
      "epoch: 1, batch: 453, loss: 0.27871477603912354\n",
      "epoch: 1, batch: 454, loss: 0.5550034642219543\n",
      "epoch: 1, batch: 455, loss: 0.4073188602924347\n",
      "epoch: 1, batch: 456, loss: 0.5068199038505554\n",
      "epoch: 1, batch: 457, loss: 0.49225887656211853\n",
      "epoch: 1, batch: 458, loss: 0.35655805468559265\n",
      "epoch: 1, batch: 459, loss: 0.4393995404243469\n",
      "epoch: 1, batch: 460, loss: 0.4167425334453583\n",
      "epoch: 1, batch: 461, loss: 0.39319518208503723\n",
      "epoch: 1, batch: 462, loss: 0.3368047773838043\n",
      "epoch: 1, batch: 463, loss: 0.5078036189079285\n",
      "epoch: 1, batch: 464, loss: 0.3660895526409149\n",
      "epoch: 1, batch: 465, loss: 0.41375958919525146\n",
      "epoch: 1, batch: 466, loss: 0.46932825446128845\n",
      "epoch: 1, batch: 467, loss: 0.45772430300712585\n",
      "epoch: 1, batch: 468, loss: 0.4924992620944977\n",
      "epoch: 1, batch: 469, loss: 0.38430485129356384\n",
      "epoch: 1, batch: 470, loss: 0.5379266738891602\n",
      "epoch: 1, batch: 471, loss: 0.5015553832054138\n",
      "epoch: 1, batch: 472, loss: 0.37886956334114075\n",
      "epoch: 1, batch: 473, loss: 0.4958946406841278\n",
      "epoch: 1, batch: 474, loss: 0.38576990365982056\n",
      "epoch: 1, batch: 475, loss: 0.30967485904693604\n",
      "epoch: 1, batch: 476, loss: 0.5362869501113892\n",
      "epoch: 1, batch: 477, loss: 0.4828556180000305\n",
      "epoch: 1, batch: 478, loss: 0.5038819909095764\n",
      "epoch: 1, batch: 479, loss: 0.4604431390762329\n",
      "epoch: 1, batch: 480, loss: 0.44022995233535767\n",
      "epoch: 1, batch: 481, loss: 0.4658931791782379\n",
      "epoch: 1, batch: 482, loss: 0.5179154276847839\n",
      "epoch: 1, batch: 483, loss: 0.29154905676841736\n",
      "epoch: 1, batch: 484, loss: 0.6159282922744751\n",
      "epoch: 1, batch: 485, loss: 0.5319136381149292\n",
      "epoch: 1, batch: 486, loss: 0.3718644380569458\n",
      "epoch: 1, batch: 487, loss: 0.3923306465148926\n",
      "epoch: 1, batch: 488, loss: 0.4437476694583893\n",
      "epoch: 1, batch: 489, loss: 0.5438054800033569\n",
      "epoch: 1, batch: 490, loss: 0.48379790782928467\n",
      "epoch: 1, batch: 491, loss: 0.4506334066390991\n",
      "epoch: 1, batch: 492, loss: 0.34587135910987854\n",
      "epoch: 1, batch: 493, loss: 0.3919498920440674\n",
      "epoch: 1, batch: 494, loss: 0.40274861454963684\n",
      "epoch: 1, batch: 495, loss: 0.38264796137809753\n",
      "epoch: 1, batch: 496, loss: 0.3991073668003082\n",
      "epoch: 1, batch: 497, loss: 0.5856268405914307\n",
      "epoch: 1, batch: 498, loss: 0.3867681622505188\n",
      "epoch: 1, batch: 499, loss: 0.4399113059043884\n",
      "epoch: 1, batch: 500, loss: 0.4492521584033966\n",
      "epoch: 1, batch: 501, loss: 0.49303463101387024\n",
      "epoch: 1, batch: 502, loss: 0.21739237010478973\n",
      "epoch: 1, batch: 503, loss: 0.4734216332435608\n",
      "epoch: 1, batch: 504, loss: 0.467284619808197\n",
      "epoch: 1, batch: 505, loss: 0.3869989216327667\n",
      "epoch: 1, batch: 506, loss: 0.41450050473213196\n",
      "epoch: 1, batch: 507, loss: 0.37739792466163635\n",
      "epoch: 1, batch: 508, loss: 0.4795677065849304\n",
      "epoch: 1, batch: 509, loss: 0.3435930013656616\n",
      "epoch: 1, batch: 510, loss: 0.28309598565101624\n",
      "epoch: 1, batch: 511, loss: 0.246234729886055\n",
      "epoch: 1, batch: 512, loss: 0.4439273476600647\n",
      "epoch: 1, batch: 513, loss: 0.29598408937454224\n",
      "epoch: 1, batch: 514, loss: 0.45665881037712097\n",
      "epoch: 1, batch: 515, loss: 0.21140044927597046\n",
      "epoch: 1, batch: 516, loss: 0.36178216338157654\n",
      "epoch: 1, batch: 517, loss: 0.2891155183315277\n",
      "epoch: 1, batch: 518, loss: 0.49333423376083374\n",
      "epoch: 1, batch: 519, loss: 0.41919559240341187\n",
      "epoch: 1, batch: 520, loss: 0.37474459409713745\n",
      "epoch: 1, batch: 521, loss: 0.5858684182167053\n",
      "epoch: 1, batch: 522, loss: 0.3298588693141937\n",
      "epoch: 1, batch: 523, loss: 0.539228618144989\n",
      "epoch: 1, batch: 524, loss: 0.4807601273059845\n",
      "epoch: 1, batch: 525, loss: 0.5035768747329712\n",
      "epoch: 1, batch: 526, loss: 0.3355574309825897\n",
      "epoch: 1, batch: 527, loss: 0.4514714479446411\n",
      "epoch: 1, batch: 528, loss: 0.2683807909488678\n",
      "epoch: 1, batch: 529, loss: 0.5320576429367065\n",
      "epoch: 1, batch: 530, loss: 0.59858638048172\n",
      "epoch: 1, batch: 531, loss: 0.676777184009552\n",
      "epoch: 1, batch: 532, loss: 0.6016185879707336\n",
      "epoch: 1, batch: 533, loss: 0.4133617579936981\n",
      "epoch: 1, batch: 534, loss: 0.46581557393074036\n",
      "epoch: 1, batch: 535, loss: 0.4564402997493744\n",
      "epoch: 1, batch: 536, loss: 0.4495322108268738\n",
      "epoch: 1, batch: 537, loss: 0.3654311001300812\n",
      "epoch: 1, batch: 538, loss: 0.46434158086776733\n",
      "epoch: 1, batch: 539, loss: 0.2971133887767792\n",
      "epoch: 1, batch: 540, loss: 0.5554623007774353\n",
      "epoch: 1, batch: 541, loss: 0.2968565821647644\n",
      "epoch: 1, batch: 542, loss: 0.48546895384788513\n",
      "epoch: 1, batch: 543, loss: 0.4915357530117035\n",
      "epoch: 1, batch: 544, loss: 0.5411094427108765\n",
      "epoch: 1, batch: 545, loss: 0.5298416018486023\n",
      "epoch: 1, batch: 546, loss: 0.5047976970672607\n",
      "epoch: 1, batch: 547, loss: 0.3410761058330536\n",
      "epoch: 1, batch: 548, loss: 0.2867347002029419\n",
      "epoch: 1, batch: 549, loss: 0.33010783791542053\n",
      "epoch: 1, batch: 550, loss: 0.3815233111381531\n",
      "epoch: 1, batch: 551, loss: 0.8108184933662415\n",
      "epoch: 1, batch: 552, loss: 0.38939762115478516\n",
      "epoch: 1, batch: 553, loss: 0.6208038330078125\n",
      "epoch: 1, batch: 554, loss: 0.45413610339164734\n",
      "epoch: 1, batch: 555, loss: 0.647347629070282\n",
      "epoch: 1, batch: 556, loss: 0.4090040326118469\n",
      "epoch: 1, batch: 557, loss: 0.4705522358417511\n",
      "epoch: 1, batch: 558, loss: 0.5667529702186584\n",
      "epoch: 1, batch: 559, loss: 0.2790542542934418\n",
      "epoch: 1, batch: 560, loss: 0.47816628217697144\n",
      "epoch: 1, batch: 561, loss: 0.6608838438987732\n",
      "epoch: 1, batch: 562, loss: 0.3086096942424774\n",
      "epoch: 1, batch: 563, loss: 0.5710716843605042\n",
      "epoch: 1, batch: 564, loss: 0.7288056015968323\n",
      "epoch: 1, batch: 565, loss: 0.529111921787262\n",
      "epoch: 1, batch: 566, loss: 0.3887985050678253\n",
      "epoch: 1, batch: 567, loss: 0.5084901452064514\n",
      "epoch: 1, batch: 568, loss: 0.342456191778183\n",
      "epoch: 1, batch: 569, loss: 0.4857909381389618\n",
      "epoch: 1, batch: 570, loss: 0.43595463037490845\n",
      "epoch: 1, batch: 571, loss: 0.4395492970943451\n",
      "epoch: 1, batch: 572, loss: 0.4685406982898712\n",
      "epoch: 1, batch: 573, loss: 0.2796115279197693\n",
      "epoch: 1, batch: 574, loss: 0.427104651927948\n",
      "epoch: 1, batch: 575, loss: 0.7636617422103882\n",
      "epoch: 1, batch: 576, loss: 0.34206101298332214\n",
      "epoch: 1, batch: 577, loss: 0.5169509053230286\n",
      "epoch: 1, batch: 578, loss: 0.20025846362113953\n",
      "epoch: 1, batch: 579, loss: 0.34750622510910034\n",
      "epoch: 1, batch: 580, loss: 0.49811920523643494\n",
      "epoch: 1, batch: 581, loss: 0.5040146112442017\n",
      "epoch: 1, batch: 582, loss: 0.45806214213371277\n",
      "epoch: 1, batch: 583, loss: 0.42788970470428467\n",
      "epoch: 1, batch: 584, loss: 0.198160320520401\n",
      "epoch: 1, batch: 585, loss: 0.28311774134635925\n",
      "epoch: 1, batch: 586, loss: 0.6037931442260742\n",
      "epoch: 1, batch: 587, loss: 0.4895552396774292\n",
      "epoch: 1, batch: 588, loss: 0.47706010937690735\n",
      "epoch: 1, batch: 589, loss: 0.44527915120124817\n",
      "epoch: 1, batch: 590, loss: 0.4785768687725067\n",
      "epoch: 1, batch: 591, loss: 0.2545188367366791\n",
      "epoch: 1, batch: 592, loss: 0.47156864404678345\n",
      "epoch: 1, batch: 593, loss: 0.24940979480743408\n",
      "epoch: 1, batch: 594, loss: 0.4047951102256775\n",
      "epoch: 1, batch: 595, loss: 0.25694698095321655\n",
      "epoch: 1, batch: 596, loss: 0.21969406306743622\n",
      "epoch: 1, batch: 597, loss: 0.4948374629020691\n",
      "epoch: 1, batch: 598, loss: 0.28252407908439636\n",
      "epoch: 1, batch: 599, loss: 0.6076909303665161\n",
      "epoch: 1, batch: 600, loss: 0.4332911968231201\n",
      "epoch: 1, batch: 601, loss: 0.4104822278022766\n",
      "epoch: 1, batch: 602, loss: 0.31790703535079956\n",
      "epoch: 1, batch: 603, loss: 0.4821193814277649\n",
      "epoch: 1, batch: 604, loss: 0.3556847870349884\n",
      "epoch: 1, batch: 605, loss: 0.2981262505054474\n",
      "epoch: 1, batch: 606, loss: 0.3327082097530365\n",
      "epoch: 1, batch: 607, loss: 0.46976563334465027\n",
      "epoch: 1, batch: 608, loss: 0.5118516087532043\n",
      "epoch: 1, batch: 609, loss: 0.33400288224220276\n",
      "epoch: 1, batch: 610, loss: 0.327696830034256\n",
      "epoch: 1, batch: 611, loss: 0.24586758017539978\n",
      "epoch: 1, batch: 612, loss: 0.3139278292655945\n",
      "epoch: 1, batch: 613, loss: 0.28652337193489075\n",
      "epoch: 1, batch: 614, loss: 0.38717326521873474\n",
      "epoch: 1, batch: 615, loss: 0.49497127532958984\n",
      "epoch: 1, batch: 616, loss: 0.24756117165088654\n",
      "epoch: 1, batch: 617, loss: 0.4444970190525055\n",
      "epoch: 1, batch: 618, loss: 0.3511376976966858\n",
      "epoch: 1, batch: 619, loss: 0.3667498230934143\n",
      "epoch: 1, batch: 620, loss: 0.49250733852386475\n",
      "epoch: 1, batch: 621, loss: 0.4282985031604767\n",
      "epoch: 1, batch: 622, loss: 0.39361077547073364\n",
      "epoch: 1, batch: 623, loss: 0.32730522751808167\n",
      "epoch: 1, batch: 624, loss: 0.3418777287006378\n",
      "epoch: 1, batch: 625, loss: 0.22977448999881744\n",
      "epoch: 1, batch: 626, loss: 0.5591673851013184\n",
      "epoch: 1, batch: 627, loss: 0.589739203453064\n",
      "epoch: 1, batch: 628, loss: 0.5084637999534607\n",
      "epoch: 1, batch: 629, loss: 0.32378777861595154\n",
      "epoch: 1, batch: 630, loss: 0.317914217710495\n",
      "epoch: 1, batch: 631, loss: 0.4316208064556122\n",
      "epoch: 1, batch: 632, loss: 0.4416143000125885\n",
      "epoch: 1, batch: 633, loss: 0.4567910134792328\n",
      "epoch: 1, batch: 634, loss: 0.28020718693733215\n",
      "epoch: 1, batch: 635, loss: 0.3353078365325928\n",
      "epoch: 1, batch: 636, loss: 0.468948096036911\n",
      "epoch: 1, batch: 637, loss: 0.331855833530426\n",
      "epoch: 1, batch: 638, loss: 0.4820699989795685\n",
      "epoch: 1, batch: 639, loss: 0.2800593376159668\n",
      "epoch: 1, batch: 640, loss: 0.2798406481742859\n",
      "epoch: 1, batch: 641, loss: 0.4947150647640228\n",
      "epoch: 1, batch: 642, loss: 0.3057662844657898\n",
      "epoch: 1, batch: 643, loss: 0.44859376549720764\n",
      "epoch: 1, batch: 644, loss: 0.328424334526062\n",
      "epoch: 1, batch: 645, loss: 0.3362675905227661\n",
      "epoch: 1, batch: 646, loss: 0.5200588703155518\n",
      "epoch: 1, batch: 647, loss: 0.3544657528400421\n",
      "epoch: 1, batch: 648, loss: 0.44542956352233887\n",
      "epoch: 1, batch: 649, loss: 0.3384726345539093\n",
      "epoch: 1, batch: 650, loss: 0.5807846784591675\n",
      "epoch: 1, batch: 651, loss: 0.27240249514579773\n",
      "epoch: 1, batch: 652, loss: 0.5455408692359924\n",
      "epoch: 1, batch: 653, loss: 0.39014798402786255\n",
      "epoch: 1, batch: 654, loss: 0.3761354982852936\n",
      "epoch: 1, batch: 655, loss: 0.3883534073829651\n",
      "epoch: 1, batch: 656, loss: 0.5085853338241577\n",
      "epoch: 1, batch: 657, loss: 0.4242840111255646\n",
      "epoch: 1, batch: 658, loss: 0.2972085773944855\n",
      "epoch: 1, batch: 659, loss: 0.4903181493282318\n",
      "epoch: 1, batch: 660, loss: 0.3476329743862152\n",
      "epoch: 1, batch: 661, loss: 0.4701632559299469\n",
      "epoch: 1, batch: 662, loss: 0.37413161993026733\n",
      "epoch: 1, batch: 663, loss: 0.33688610792160034\n",
      "epoch: 1, batch: 664, loss: 0.3436068594455719\n",
      "epoch: 1, batch: 665, loss: 0.4009758234024048\n",
      "epoch: 1, batch: 666, loss: 0.23912011086940765\n",
      "epoch: 1, batch: 667, loss: 0.4001728892326355\n",
      "epoch: 1, batch: 668, loss: 0.44230836629867554\n",
      "epoch: 1, batch: 669, loss: 0.2738264203071594\n",
      "epoch: 1, batch: 670, loss: 0.23885208368301392\n",
      "epoch: 1, batch: 671, loss: 0.3517628014087677\n",
      "epoch: 1, batch: 672, loss: 0.5254168510437012\n",
      "epoch: 1, batch: 673, loss: 0.4734441339969635\n",
      "epoch: 1, batch: 674, loss: 0.3513955771923065\n",
      "epoch: 1, batch: 675, loss: 0.453904390335083\n",
      "epoch: 1, batch: 676, loss: 0.5060577988624573\n",
      "epoch: 1, batch: 677, loss: 0.3938770890235901\n",
      "epoch: 1, batch: 678, loss: 0.44085291028022766\n",
      "epoch: 1, batch: 679, loss: 0.5250979065895081\n",
      "epoch: 1, batch: 680, loss: 0.5521818995475769\n",
      "epoch: 1, batch: 681, loss: 0.5086479187011719\n",
      "epoch: 1, batch: 682, loss: 0.297769695520401\n",
      "epoch: 1, batch: 683, loss: 0.28686779737472534\n",
      "epoch: 1, batch: 684, loss: 0.43190282583236694\n",
      "epoch: 1, batch: 685, loss: 0.36910125613212585\n",
      "epoch: 1, batch: 686, loss: 0.5934340357780457\n",
      "epoch: 1, batch: 687, loss: 0.264334499835968\n",
      "epoch: 1, batch: 688, loss: 0.4795045256614685\n",
      "epoch: 1, batch: 689, loss: 0.5372134447097778\n",
      "epoch: 1, batch: 690, loss: 0.3588005304336548\n",
      "epoch: 1, batch: 691, loss: 0.46322304010391235\n",
      "epoch: 1, batch: 692, loss: 0.33755823969841003\n",
      "epoch: 1, batch: 693, loss: 0.3201216757297516\n",
      "epoch: 1, batch: 694, loss: 0.23704075813293457\n",
      "epoch: 1, batch: 695, loss: 0.22826401889324188\n",
      "epoch: 1, batch: 696, loss: 0.42318499088287354\n",
      "epoch: 1, batch: 697, loss: 0.4354380667209625\n",
      "epoch: 1, batch: 698, loss: 0.7177157402038574\n",
      "epoch: 1, batch: 699, loss: 0.27066951990127563\n",
      "epoch: 1, batch: 700, loss: 0.26490914821624756\n",
      "epoch: 1, batch: 701, loss: 0.32658717036247253\n",
      "epoch: 1, batch: 702, loss: 0.36572667956352234\n",
      "epoch: 1, batch: 703, loss: 0.4907848536968231\n",
      "epoch: 1, batch: 704, loss: 0.3390084207057953\n",
      "epoch: 1, batch: 705, loss: 0.2200918048620224\n",
      "epoch: 1, batch: 706, loss: 0.2716226875782013\n",
      "epoch: 1, batch: 707, loss: 0.6208686828613281\n",
      "epoch: 1, batch: 708, loss: 0.3775023818016052\n",
      "epoch: 1, batch: 709, loss: 0.4111814498901367\n",
      "epoch: 1, batch: 710, loss: 0.32716548442840576\n",
      "epoch: 1, batch: 711, loss: 0.410351037979126\n",
      "epoch: 1, batch: 712, loss: 0.3948090076446533\n",
      "epoch: 1, batch: 713, loss: 0.3117476999759674\n",
      "epoch: 1, batch: 714, loss: 0.6197050213813782\n",
      "epoch: 1, batch: 715, loss: 0.4881112575531006\n",
      "epoch: 1, batch: 716, loss: 0.4716848134994507\n",
      "epoch: 1, batch: 717, loss: 0.269255131483078\n",
      "epoch: 1, batch: 718, loss: 0.2723066806793213\n",
      "epoch: 1, batch: 719, loss: 0.35814693570137024\n",
      "epoch: 1, batch: 720, loss: 0.41576114296913147\n",
      "epoch: 1, batch: 721, loss: 0.3183375597000122\n",
      "epoch: 1, batch: 722, loss: 0.2248062640428543\n",
      "epoch: 1, batch: 723, loss: 0.5087933540344238\n",
      "epoch: 1, batch: 724, loss: 0.4119281768798828\n",
      "epoch: 1, batch: 725, loss: 0.23249571025371552\n",
      "epoch: 1, batch: 726, loss: 0.38986557722091675\n",
      "epoch: 1, batch: 727, loss: 0.3591511845588684\n",
      "epoch: 1, batch: 728, loss: 0.3979693055152893\n",
      "epoch: 1, batch: 729, loss: 0.28426188230514526\n",
      "epoch: 1, batch: 730, loss: 0.34375396370887756\n",
      "epoch: 1, batch: 731, loss: 0.49634233117103577\n",
      "epoch: 1, batch: 732, loss: 0.2292645126581192\n",
      "epoch: 1, batch: 733, loss: 0.4932972192764282\n",
      "epoch: 1, batch: 734, loss: 0.4372287392616272\n",
      "epoch: 1, batch: 735, loss: 0.3760000467300415\n",
      "epoch: 1, batch: 736, loss: 0.18108166754245758\n",
      "epoch: 1, batch: 737, loss: 0.47240886092185974\n",
      "epoch: 1, batch: 738, loss: 0.46883633732795715\n",
      "epoch: 1, batch: 739, loss: 0.3016798198223114\n",
      "epoch: 1, batch: 740, loss: 0.48228633403778076\n",
      "epoch: 1, batch: 741, loss: 0.3001965284347534\n",
      "epoch: 1, batch: 742, loss: 0.4187301993370056\n",
      "epoch: 1, batch: 743, loss: 0.24456003308296204\n",
      "epoch: 1, batch: 744, loss: 0.3399626314640045\n",
      "epoch: 1, batch: 745, loss: 0.26154792308807373\n",
      "epoch: 1, batch: 746, loss: 0.4793083071708679\n",
      "epoch: 1, batch: 747, loss: 0.4606597423553467\n",
      "epoch: 1, batch: 748, loss: 0.46276357769966125\n",
      "epoch: 1, batch: 749, loss: 0.3508228063583374\n",
      "epoch: 1, batch: 750, loss: 0.36298805475234985\n",
      "epoch: 1, batch: 751, loss: 0.3617492616176605\n",
      "epoch: 1, batch: 752, loss: 0.42631107568740845\n",
      "epoch: 1, batch: 753, loss: 0.23899021744728088\n",
      "epoch: 1, batch: 754, loss: 0.46618759632110596\n",
      "epoch: 1, batch: 755, loss: 0.3523385226726532\n",
      "epoch: 1, batch: 756, loss: 0.25698521733283997\n",
      "epoch: 1, batch: 757, loss: 0.30503493547439575\n",
      "epoch: 1, batch: 758, loss: 0.40862348675727844\n",
      "epoch: 1, batch: 759, loss: 0.23670022189617157\n",
      "epoch: 1, batch: 760, loss: 0.3864891827106476\n",
      "epoch: 1, batch: 761, loss: 0.4238753318786621\n",
      "epoch: 1, batch: 762, loss: 0.5478726625442505\n",
      "epoch: 1, batch: 763, loss: 0.3904070258140564\n",
      "epoch: 1, batch: 764, loss: 0.5018818974494934\n",
      "epoch: 1, batch: 765, loss: 0.344992071390152\n",
      "epoch: 1, batch: 766, loss: 0.3615376651287079\n",
      "epoch: 1, batch: 767, loss: 0.45474764704704285\n",
      "epoch: 1, batch: 768, loss: 0.3260885775089264\n",
      "epoch: 1, batch: 769, loss: 0.24192233383655548\n",
      "epoch: 1, batch: 770, loss: 0.29289788007736206\n",
      "epoch: 1, batch: 771, loss: 0.41298848390579224\n",
      "epoch: 1, batch: 772, loss: 0.364911824464798\n",
      "epoch: 1, batch: 773, loss: 0.1786746084690094\n",
      "epoch: 1, batch: 774, loss: 0.2484169900417328\n",
      "epoch: 1, batch: 775, loss: 0.28749263286590576\n",
      "epoch: 1, batch: 776, loss: 0.3407532572746277\n",
      "epoch: 1, batch: 777, loss: 0.5883662700653076\n",
      "epoch: 1, batch: 778, loss: 0.40930208563804626\n",
      "epoch: 1, batch: 779, loss: 0.2728988826274872\n",
      "epoch: 1, batch: 780, loss: 0.2893819212913513\n",
      "epoch: 1, batch: 781, loss: 0.34052345156669617\n",
      "epoch: 1, batch: 782, loss: 0.2548893094062805\n",
      "epoch: 1, batch: 783, loss: 0.2926008403301239\n",
      "epoch: 1, batch: 784, loss: 0.5640755295753479\n",
      "epoch: 1, batch: 785, loss: 0.2543735206127167\n",
      "epoch: 1, batch: 786, loss: 0.2674795389175415\n",
      "epoch: 1, batch: 787, loss: 0.33580413460731506\n",
      "epoch: 1, batch: 788, loss: 0.4355319142341614\n",
      "epoch: 1, batch: 789, loss: 0.3708758056163788\n",
      "epoch: 1, batch: 790, loss: 0.3307375907897949\n",
      "epoch: 1, batch: 791, loss: 0.30948835611343384\n",
      "epoch: 1, batch: 792, loss: 0.438326895236969\n",
      "epoch: 1, batch: 793, loss: 0.2994932234287262\n",
      "epoch: 1, batch: 794, loss: 0.6187334060668945\n",
      "epoch: 1, batch: 795, loss: 0.32285282015800476\n",
      "epoch: 1, batch: 796, loss: 0.42225411534309387\n",
      "epoch: 1, batch: 797, loss: 0.31758278608322144\n",
      "epoch: 1, batch: 798, loss: 0.31137534976005554\n",
      "epoch: 1, batch: 799, loss: 0.3523921072483063\n",
      "epoch: 1, batch: 800, loss: 0.5332539677619934\n",
      "epoch: 1, batch: 801, loss: 0.23922060430049896\n",
      "epoch: 1, batch: 802, loss: 0.2847801148891449\n",
      "epoch: 1, batch: 803, loss: 0.23715975880622864\n",
      "epoch: 1, batch: 804, loss: 0.22017529606819153\n",
      "epoch: 1, batch: 805, loss: 0.2510192096233368\n",
      "epoch: 1, batch: 806, loss: 0.6707631349563599\n",
      "epoch: 1, batch: 807, loss: 0.2757633924484253\n",
      "epoch: 1, batch: 808, loss: 0.30060315132141113\n",
      "epoch: 1, batch: 809, loss: 0.4607723355293274\n",
      "epoch: 1, batch: 810, loss: 0.4359281361103058\n",
      "epoch: 1, batch: 811, loss: 0.5177882313728333\n",
      "epoch: 1, batch: 812, loss: 0.4057801365852356\n",
      "epoch: 1, batch: 813, loss: 0.4851202368736267\n",
      "epoch: 1, batch: 814, loss: 0.1280696541070938\n",
      "epoch: 1, batch: 815, loss: 0.17575277388095856\n",
      "epoch: 1, batch: 816, loss: 0.3787343204021454\n",
      "epoch: 1, batch: 817, loss: 0.36462438106536865\n",
      "epoch: 1, batch: 818, loss: 0.26752519607543945\n",
      "epoch: 1, batch: 819, loss: 0.23320157825946808\n",
      "epoch: 1, batch: 820, loss: 0.5813053250312805\n",
      "epoch: 1, batch: 821, loss: 0.24546518921852112\n",
      "epoch: 1, batch: 822, loss: 0.1696617305278778\n",
      "epoch: 1, batch: 823, loss: 0.27695438265800476\n",
      "epoch: 1, batch: 824, loss: 0.2703186869621277\n",
      "epoch: 1, batch: 825, loss: 0.3701499104499817\n",
      "epoch: 1, batch: 826, loss: 0.4033583104610443\n",
      "epoch: 1, batch: 827, loss: 0.2838328182697296\n",
      "epoch: 1, batch: 828, loss: 0.3813304901123047\n",
      "epoch: 1, batch: 829, loss: 0.45457684993743896\n",
      "epoch: 1, batch: 830, loss: 0.4488334059715271\n",
      "epoch: 1, batch: 831, loss: 0.29663193225860596\n",
      "epoch: 1, batch: 832, loss: 0.15255099534988403\n",
      "epoch: 1, batch: 833, loss: 0.36600732803344727\n",
      "epoch: 1, batch: 834, loss: 0.38797348737716675\n",
      "epoch: 1, batch: 835, loss: 0.39348840713500977\n",
      "epoch: 1, batch: 836, loss: 0.296212762594223\n",
      "epoch: 1, batch: 837, loss: 0.3552398979663849\n",
      "epoch: 1, batch: 838, loss: 0.32686904072761536\n",
      "epoch: 1, batch: 839, loss: 0.2868827283382416\n",
      "epoch: 1, batch: 840, loss: 0.3606516718864441\n",
      "epoch: 1, batch: 841, loss: 0.3408878445625305\n",
      "epoch: 1, batch: 842, loss: 0.304654061794281\n",
      "epoch: 1, batch: 843, loss: 0.1744561493396759\n",
      "epoch: 1, batch: 844, loss: 0.41268908977508545\n",
      "epoch: 1, batch: 845, loss: 0.3266846537590027\n",
      "epoch: 1, batch: 846, loss: 0.4542405903339386\n",
      "epoch: 1, batch: 847, loss: 0.4101182818412781\n",
      "epoch: 1, batch: 848, loss: 0.25684624910354614\n",
      "epoch: 1, batch: 849, loss: 0.3807327151298523\n",
      "epoch: 1, batch: 850, loss: 0.28582891821861267\n",
      "epoch: 1, batch: 851, loss: 0.27923643589019775\n",
      "epoch: 1, batch: 852, loss: 0.42410826683044434\n",
      "epoch: 1, batch: 853, loss: 0.29355186223983765\n",
      "epoch: 1, batch: 854, loss: 0.5644174814224243\n",
      "epoch: 1, batch: 855, loss: 0.3804241120815277\n",
      "epoch: 1, batch: 856, loss: 0.23084257543087006\n",
      "epoch: 1, batch: 857, loss: 0.2609160840511322\n",
      "epoch: 1, batch: 858, loss: 0.41102197766304016\n",
      "epoch: 1, batch: 859, loss: 0.3254128396511078\n",
      "epoch: 1, batch: 860, loss: 0.235420361161232\n",
      "epoch: 1, batch: 861, loss: 0.2538851201534271\n",
      "epoch: 1, batch: 862, loss: 0.33691421151161194\n",
      "epoch: 1, batch: 863, loss: 0.5441927313804626\n",
      "epoch: 1, batch: 864, loss: 0.3807820677757263\n",
      "epoch: 1, batch: 865, loss: 0.32726582884788513\n",
      "epoch: 1, batch: 866, loss: 0.2628759741783142\n",
      "epoch: 1, batch: 867, loss: 0.4337824583053589\n",
      "epoch: 1, batch: 868, loss: 0.31316065788269043\n",
      "epoch: 1, batch: 869, loss: 0.22614748775959015\n",
      "epoch: 1, batch: 870, loss: 0.2456323355436325\n",
      "epoch: 1, batch: 871, loss: 0.25911426544189453\n",
      "epoch: 1, batch: 872, loss: 0.44361674785614014\n",
      "epoch: 1, batch: 873, loss: 0.21981364488601685\n",
      "epoch: 1, batch: 874, loss: 0.28112807869911194\n",
      "epoch: 1, batch: 875, loss: 0.40604814887046814\n",
      "epoch: 1, batch: 876, loss: 0.3379524052143097\n",
      "epoch: 1, batch: 877, loss: 0.07839860022068024\n",
      "epoch: 1, batch: 878, loss: 0.2474449723958969\n",
      "epoch: 1, batch: 879, loss: 0.2572939693927765\n",
      "epoch: 1, batch: 880, loss: 0.28201305866241455\n",
      "epoch: 1, batch: 881, loss: 0.3335920572280884\n",
      "epoch: 1, batch: 882, loss: 0.24577714502811432\n",
      "epoch: 1, batch: 883, loss: 0.3942282795906067\n",
      "epoch: 1, batch: 884, loss: 0.28850632905960083\n",
      "epoch: 1, batch: 885, loss: 0.28033673763275146\n",
      "epoch: 1, batch: 886, loss: 0.40848907828330994\n",
      "epoch: 1, batch: 887, loss: 0.418713241815567\n",
      "epoch: 1, batch: 888, loss: 0.4873688220977783\n",
      "epoch: 1, batch: 889, loss: 0.409816712141037\n",
      "epoch: 1, batch: 890, loss: 0.3125913441181183\n",
      "epoch: 1, batch: 891, loss: 0.6049178242683411\n",
      "epoch: 1, batch: 892, loss: 0.32694002985954285\n",
      "epoch: 1, batch: 893, loss: 0.6294113397598267\n",
      "epoch: 1, batch: 894, loss: 0.2731131315231323\n",
      "epoch: 1, batch: 895, loss: 0.23057298362255096\n",
      "epoch: 1, batch: 896, loss: 0.46397608518600464\n",
      "epoch: 1, batch: 897, loss: 0.32173603773117065\n",
      "epoch: 1, batch: 898, loss: 0.24420006573200226\n",
      "epoch: 1, batch: 899, loss: 0.46370601654052734\n",
      "epoch: 1, batch: 900, loss: 0.27290788292884827\n",
      "epoch: 1, batch: 901, loss: 0.42191562056541443\n",
      "epoch: 1, batch: 902, loss: 0.2416570633649826\n",
      "epoch: 1, batch: 903, loss: 0.42111292481422424\n",
      "epoch: 1, batch: 904, loss: 0.258396178483963\n",
      "epoch: 1, batch: 905, loss: 0.28063902258872986\n",
      "epoch: 1, batch: 906, loss: 0.2088266760110855\n",
      "epoch: 1, batch: 907, loss: 0.23799696564674377\n",
      "epoch: 1, batch: 908, loss: 0.19883134961128235\n",
      "epoch: 1, batch: 909, loss: 0.3086084723472595\n",
      "epoch: 1, batch: 910, loss: 0.5668652057647705\n",
      "epoch: 1, batch: 911, loss: 0.2885318100452423\n",
      "epoch: 1, batch: 912, loss: 0.38829588890075684\n",
      "epoch: 1, batch: 913, loss: 0.24256739020347595\n",
      "epoch: 1, batch: 914, loss: 0.47355297207832336\n",
      "epoch: 1, batch: 915, loss: 0.45336422324180603\n",
      "epoch: 1, batch: 916, loss: 0.39038828015327454\n",
      "epoch: 1, batch: 917, loss: 0.3630850911140442\n",
      "epoch: 1, batch: 918, loss: 0.4830518960952759\n",
      "epoch: 1, batch: 919, loss: 0.3759475648403168\n",
      "epoch: 1, batch: 920, loss: 0.2780424952507019\n",
      "epoch: 1, batch: 921, loss: 0.32174813747406006\n",
      "epoch: 1, batch: 922, loss: 0.4803481996059418\n",
      "epoch: 1, batch: 923, loss: 0.4514632821083069\n",
      "epoch: 1, batch: 924, loss: 0.21902644634246826\n",
      "epoch: 1, batch: 925, loss: 0.2078825831413269\n",
      "epoch: 1, batch: 926, loss: 0.2210141122341156\n",
      "epoch: 1, batch: 927, loss: 0.4422796368598938\n",
      "epoch: 1, batch: 928, loss: 0.4329301416873932\n",
      "epoch: 1, batch: 929, loss: 0.3834109902381897\n",
      "epoch: 1, batch: 930, loss: 0.40809664130210876\n",
      "epoch: 1, batch: 931, loss: 0.4070895314216614\n",
      "epoch: 1, batch: 932, loss: 0.3580135703086853\n",
      "epoch: 1, batch: 933, loss: 0.4789988696575165\n",
      "epoch: 1, batch: 934, loss: 0.4611664414405823\n",
      "epoch: 1, batch: 935, loss: 0.40678754448890686\n",
      "epoch: 1, batch: 936, loss: 0.3578723073005676\n",
      "epoch: 1, batch: 937, loss: 0.2139742374420166\n",
      "epoch: 2, batch: 0, loss: 0.376203328371048\n",
      "epoch: 2, batch: 1, loss: 0.2738693058490753\n",
      "epoch: 2, batch: 2, loss: 0.25957024097442627\n",
      "epoch: 2, batch: 3, loss: 0.42764925956726074\n",
      "epoch: 2, batch: 4, loss: 0.4263668358325958\n",
      "epoch: 2, batch: 5, loss: 0.3775274157524109\n",
      "epoch: 2, batch: 6, loss: 0.29603347182273865\n",
      "epoch: 2, batch: 7, loss: 0.28234976530075073\n",
      "epoch: 2, batch: 8, loss: 0.25494930148124695\n",
      "epoch: 2, batch: 9, loss: 0.3044011890888214\n",
      "epoch: 2, batch: 10, loss: 0.4164648950099945\n",
      "epoch: 2, batch: 11, loss: 0.25707584619522095\n",
      "epoch: 2, batch: 12, loss: 0.23864834010601044\n",
      "epoch: 2, batch: 13, loss: 0.1972716748714447\n",
      "epoch: 2, batch: 14, loss: 0.27602654695510864\n",
      "epoch: 2, batch: 15, loss: 0.2798023521900177\n",
      "epoch: 2, batch: 16, loss: 0.3273871839046478\n",
      "epoch: 2, batch: 17, loss: 0.4376847743988037\n",
      "epoch: 2, batch: 18, loss: 0.5137788653373718\n",
      "epoch: 2, batch: 19, loss: 0.342454195022583\n",
      "epoch: 2, batch: 20, loss: 0.2311049848794937\n",
      "epoch: 2, batch: 21, loss: 0.2905474901199341\n",
      "epoch: 2, batch: 22, loss: 0.5623083114624023\n",
      "epoch: 2, batch: 23, loss: 0.27400273084640503\n",
      "epoch: 2, batch: 24, loss: 0.47788292169570923\n",
      "epoch: 2, batch: 25, loss: 0.28983432054519653\n",
      "epoch: 2, batch: 26, loss: 0.1458970308303833\n",
      "epoch: 2, batch: 27, loss: 0.2083555907011032\n",
      "epoch: 2, batch: 28, loss: 0.3283434510231018\n",
      "epoch: 2, batch: 29, loss: 0.35707753896713257\n",
      "epoch: 2, batch: 30, loss: 0.21517720818519592\n",
      "epoch: 2, batch: 31, loss: 0.3966149687767029\n",
      "epoch: 2, batch: 32, loss: 0.4490068554878235\n",
      "epoch: 2, batch: 33, loss: 0.3185865879058838\n",
      "epoch: 2, batch: 34, loss: 0.34143924713134766\n",
      "epoch: 2, batch: 35, loss: 0.3143124282360077\n",
      "epoch: 2, batch: 36, loss: 0.5314578413963318\n",
      "epoch: 2, batch: 37, loss: 0.31754758954048157\n",
      "epoch: 2, batch: 38, loss: 0.2713256776332855\n",
      "epoch: 2, batch: 39, loss: 0.27669548988342285\n",
      "epoch: 2, batch: 40, loss: 0.24484634399414062\n",
      "epoch: 2, batch: 41, loss: 0.17829746007919312\n",
      "epoch: 2, batch: 42, loss: 0.6045715808868408\n",
      "epoch: 2, batch: 43, loss: 0.3288987874984741\n",
      "epoch: 2, batch: 44, loss: 0.3031204640865326\n",
      "epoch: 2, batch: 45, loss: 0.22777266800403595\n",
      "epoch: 2, batch: 46, loss: 0.39089274406433105\n",
      "epoch: 2, batch: 47, loss: 0.41593149304389954\n",
      "epoch: 2, batch: 48, loss: 0.6556786894798279\n",
      "epoch: 2, batch: 49, loss: 0.21559832990169525\n",
      "epoch: 2, batch: 50, loss: 0.14534229040145874\n",
      "epoch: 2, batch: 51, loss: 0.3473442792892456\n",
      "epoch: 2, batch: 52, loss: 0.303404301404953\n",
      "epoch: 2, batch: 53, loss: 0.3174402713775635\n",
      "epoch: 2, batch: 54, loss: 0.3276571035385132\n",
      "epoch: 2, batch: 55, loss: 0.4784083962440491\n",
      "epoch: 2, batch: 56, loss: 0.2380414605140686\n",
      "epoch: 2, batch: 57, loss: 0.19921395182609558\n",
      "epoch: 2, batch: 58, loss: 0.32579702138900757\n",
      "epoch: 2, batch: 59, loss: 0.278611421585083\n",
      "epoch: 2, batch: 60, loss: 0.3486662209033966\n",
      "epoch: 2, batch: 61, loss: 0.32778996229171753\n",
      "epoch: 2, batch: 62, loss: 0.39469605684280396\n",
      "epoch: 2, batch: 63, loss: 0.1378483921289444\n",
      "epoch: 2, batch: 64, loss: 0.37767958641052246\n",
      "epoch: 2, batch: 65, loss: 0.5023162364959717\n",
      "epoch: 2, batch: 66, loss: 0.3446679413318634\n",
      "epoch: 2, batch: 67, loss: 0.17420747876167297\n",
      "epoch: 2, batch: 68, loss: 0.23263034224510193\n",
      "epoch: 2, batch: 69, loss: 0.33176660537719727\n",
      "epoch: 2, batch: 70, loss: 0.2552890181541443\n",
      "epoch: 2, batch: 71, loss: 0.19514100253582\n",
      "epoch: 2, batch: 72, loss: 0.14574451744556427\n",
      "epoch: 2, batch: 73, loss: 0.20484720170497894\n",
      "epoch: 2, batch: 74, loss: 0.4038545787334442\n",
      "epoch: 2, batch: 75, loss: 0.10353921353816986\n",
      "epoch: 2, batch: 76, loss: 0.26986780762672424\n",
      "epoch: 2, batch: 77, loss: 0.426631897687912\n",
      "epoch: 2, batch: 78, loss: 0.4722957909107208\n",
      "epoch: 2, batch: 79, loss: 0.21451564133167267\n",
      "epoch: 2, batch: 80, loss: 0.3372747302055359\n",
      "epoch: 2, batch: 81, loss: 0.29318687319755554\n",
      "epoch: 2, batch: 82, loss: 0.43495863676071167\n",
      "epoch: 2, batch: 83, loss: 0.5752801299095154\n",
      "epoch: 2, batch: 84, loss: 0.4025301933288574\n",
      "epoch: 2, batch: 85, loss: 0.3909342586994171\n",
      "epoch: 2, batch: 86, loss: 0.1847914457321167\n",
      "epoch: 2, batch: 87, loss: 0.31400519609451294\n",
      "epoch: 2, batch: 88, loss: 0.45784810185432434\n",
      "epoch: 2, batch: 89, loss: 0.3699200451374054\n",
      "epoch: 2, batch: 90, loss: 0.36082974076271057\n",
      "epoch: 2, batch: 91, loss: 0.500295877456665\n",
      "epoch: 2, batch: 92, loss: 0.3381873071193695\n",
      "epoch: 2, batch: 93, loss: 0.20829057693481445\n",
      "epoch: 2, batch: 94, loss: 0.6477915644645691\n",
      "epoch: 2, batch: 95, loss: 0.22544129192829132\n",
      "epoch: 2, batch: 96, loss: 0.3422142267227173\n",
      "epoch: 2, batch: 97, loss: 0.3409084975719452\n",
      "epoch: 2, batch: 98, loss: 0.3485075831413269\n",
      "epoch: 2, batch: 99, loss: 0.42123374342918396\n",
      "epoch: 2, batch: 100, loss: 0.30888882279396057\n",
      "epoch: 2, batch: 101, loss: 0.37171459197998047\n",
      "epoch: 2, batch: 102, loss: 0.313495934009552\n",
      "epoch: 2, batch: 103, loss: 0.3682820200920105\n",
      "epoch: 2, batch: 104, loss: 0.3002075254917145\n",
      "epoch: 2, batch: 105, loss: 0.19901371002197266\n",
      "epoch: 2, batch: 106, loss: 0.20733791589736938\n",
      "epoch: 2, batch: 107, loss: 0.5806434750556946\n",
      "epoch: 2, batch: 108, loss: 0.37591078877449036\n",
      "epoch: 2, batch: 109, loss: 0.3333025872707367\n",
      "epoch: 2, batch: 110, loss: 0.5058145523071289\n",
      "epoch: 2, batch: 111, loss: 0.2742122709751129\n",
      "epoch: 2, batch: 112, loss: 0.5335027575492859\n",
      "epoch: 2, batch: 113, loss: 0.30510836839675903\n",
      "epoch: 2, batch: 114, loss: 0.11467736214399338\n",
      "epoch: 2, batch: 115, loss: 0.29584500193595886\n",
      "epoch: 2, batch: 116, loss: 0.36949995160102844\n",
      "epoch: 2, batch: 117, loss: 0.15771476924419403\n",
      "epoch: 2, batch: 118, loss: 0.3430899381637573\n",
      "epoch: 2, batch: 119, loss: 0.1935006082057953\n",
      "epoch: 2, batch: 120, loss: 0.4161723852157593\n",
      "epoch: 2, batch: 121, loss: 0.4518883526325226\n",
      "epoch: 2, batch: 122, loss: 0.3724227249622345\n",
      "epoch: 2, batch: 123, loss: 0.26569443941116333\n",
      "epoch: 2, batch: 124, loss: 0.2541775107383728\n",
      "epoch: 2, batch: 125, loss: 0.30899205803871155\n",
      "epoch: 2, batch: 126, loss: 0.2899591624736786\n",
      "epoch: 2, batch: 127, loss: 0.35168924927711487\n",
      "epoch: 2, batch: 128, loss: 0.32786720991134644\n",
      "epoch: 2, batch: 129, loss: 0.22049543261528015\n",
      "epoch: 2, batch: 130, loss: 0.21329057216644287\n",
      "epoch: 2, batch: 131, loss: 0.4281230568885803\n",
      "epoch: 2, batch: 132, loss: 0.3004821836948395\n",
      "epoch: 2, batch: 133, loss: 0.4291369616985321\n",
      "epoch: 2, batch: 134, loss: 0.49561163783073425\n",
      "epoch: 2, batch: 135, loss: 0.26519638299942017\n",
      "epoch: 2, batch: 136, loss: 0.30322012305259705\n",
      "epoch: 2, batch: 137, loss: 0.269179105758667\n",
      "epoch: 2, batch: 138, loss: 0.3598593473434448\n",
      "epoch: 2, batch: 139, loss: 0.1752798706293106\n",
      "epoch: 2, batch: 140, loss: 0.5544501543045044\n",
      "epoch: 2, batch: 141, loss: 0.21919859945774078\n",
      "epoch: 2, batch: 142, loss: 0.29585006833076477\n",
      "epoch: 2, batch: 143, loss: 0.2267398238182068\n",
      "epoch: 2, batch: 144, loss: 0.2252606302499771\n",
      "epoch: 2, batch: 145, loss: 0.29585373401641846\n",
      "epoch: 2, batch: 146, loss: 0.4001848101615906\n",
      "epoch: 2, batch: 147, loss: 0.274766206741333\n",
      "epoch: 2, batch: 148, loss: 0.16551744937896729\n",
      "epoch: 2, batch: 149, loss: 0.2984737455844879\n",
      "epoch: 2, batch: 150, loss: 0.20754170417785645\n",
      "epoch: 2, batch: 151, loss: 0.20719128847122192\n",
      "epoch: 2, batch: 152, loss: 0.29057830572128296\n",
      "epoch: 2, batch: 153, loss: 0.12798140943050385\n",
      "epoch: 2, batch: 154, loss: 0.20564204454421997\n",
      "epoch: 2, batch: 155, loss: 0.33079901337623596\n",
      "epoch: 2, batch: 156, loss: 0.1822977215051651\n",
      "epoch: 2, batch: 157, loss: 0.16898512840270996\n",
      "epoch: 2, batch: 158, loss: 0.24704942107200623\n",
      "epoch: 2, batch: 159, loss: 0.3346717655658722\n",
      "epoch: 2, batch: 160, loss: 0.25795993208885193\n",
      "epoch: 2, batch: 161, loss: 0.1378728449344635\n",
      "epoch: 2, batch: 162, loss: 0.3875811994075775\n",
      "epoch: 2, batch: 163, loss: 0.3048873543739319\n",
      "epoch: 2, batch: 164, loss: 0.11130005121231079\n",
      "epoch: 2, batch: 165, loss: 0.17973721027374268\n",
      "epoch: 2, batch: 166, loss: 0.13189005851745605\n",
      "epoch: 2, batch: 167, loss: 0.45448049902915955\n",
      "epoch: 2, batch: 168, loss: 0.4115408658981323\n",
      "epoch: 2, batch: 169, loss: 0.3383823335170746\n",
      "epoch: 2, batch: 170, loss: 0.3284071087837219\n",
      "epoch: 2, batch: 171, loss: 0.2564009130001068\n",
      "epoch: 2, batch: 172, loss: 0.29224494099617004\n",
      "epoch: 2, batch: 173, loss: 0.2793761193752289\n",
      "epoch: 2, batch: 174, loss: 0.3667706847190857\n",
      "epoch: 2, batch: 175, loss: 0.1970096379518509\n",
      "epoch: 2, batch: 176, loss: 0.501344621181488\n",
      "epoch: 2, batch: 177, loss: 0.20804156363010406\n",
      "epoch: 2, batch: 178, loss: 0.41450536251068115\n",
      "epoch: 2, batch: 179, loss: 0.17641709744930267\n",
      "epoch: 2, batch: 180, loss: 0.27082061767578125\n",
      "epoch: 2, batch: 181, loss: 0.2888888716697693\n",
      "epoch: 2, batch: 182, loss: 0.20039589703083038\n",
      "epoch: 2, batch: 183, loss: 0.38598448038101196\n",
      "epoch: 2, batch: 184, loss: 0.2998104691505432\n",
      "epoch: 2, batch: 185, loss: 0.2161564975976944\n",
      "epoch: 2, batch: 186, loss: 0.37321850657463074\n",
      "epoch: 2, batch: 187, loss: 0.1017659455537796\n",
      "epoch: 2, batch: 188, loss: 0.17890822887420654\n",
      "epoch: 2, batch: 189, loss: 0.25371405482292175\n",
      "epoch: 2, batch: 190, loss: 0.346229612827301\n",
      "epoch: 2, batch: 191, loss: 0.2829161584377289\n",
      "epoch: 2, batch: 192, loss: 0.11353282630443573\n",
      "epoch: 2, batch: 193, loss: 0.3613273501396179\n",
      "epoch: 2, batch: 194, loss: 0.41104674339294434\n",
      "epoch: 2, batch: 195, loss: 0.14086849987506866\n",
      "epoch: 2, batch: 196, loss: 0.3657386302947998\n",
      "epoch: 2, batch: 197, loss: 0.4495856761932373\n",
      "epoch: 2, batch: 198, loss: 0.23523421585559845\n",
      "epoch: 2, batch: 199, loss: 0.34890928864479065\n",
      "epoch: 2, batch: 200, loss: 0.5725452303886414\n",
      "epoch: 2, batch: 201, loss: 0.1159338653087616\n",
      "epoch: 2, batch: 202, loss: 0.4612782597541809\n",
      "epoch: 2, batch: 203, loss: 0.3597569465637207\n",
      "epoch: 2, batch: 204, loss: 0.2767258286476135\n",
      "epoch: 2, batch: 205, loss: 0.353596031665802\n",
      "epoch: 2, batch: 206, loss: 0.1751542091369629\n",
      "epoch: 2, batch: 207, loss: 0.4028063714504242\n",
      "epoch: 2, batch: 208, loss: 0.25232255458831787\n",
      "epoch: 2, batch: 209, loss: 0.3667447566986084\n",
      "epoch: 2, batch: 210, loss: 0.2872040271759033\n",
      "epoch: 2, batch: 211, loss: 0.29989805817604065\n",
      "epoch: 2, batch: 212, loss: 0.24622656404972076\n",
      "epoch: 2, batch: 213, loss: 0.13595964014530182\n",
      "epoch: 2, batch: 214, loss: 0.37250182032585144\n",
      "epoch: 2, batch: 215, loss: 0.26808515191078186\n",
      "epoch: 2, batch: 216, loss: 0.27877917885780334\n",
      "epoch: 2, batch: 217, loss: 0.3074020445346832\n",
      "epoch: 2, batch: 218, loss: 0.1171107068657875\n",
      "epoch: 2, batch: 219, loss: 0.18743358552455902\n",
      "epoch: 2, batch: 220, loss: 0.30622583627700806\n",
      "epoch: 2, batch: 221, loss: 0.23126840591430664\n",
      "epoch: 2, batch: 222, loss: 0.2532379925251007\n",
      "epoch: 2, batch: 223, loss: 0.36754366755485535\n",
      "epoch: 2, batch: 224, loss: 0.2653837502002716\n",
      "epoch: 2, batch: 225, loss: 0.33743777871131897\n",
      "epoch: 2, batch: 226, loss: 0.2372383326292038\n",
      "epoch: 2, batch: 227, loss: 0.3032032251358032\n",
      "epoch: 2, batch: 228, loss: 0.26823216676712036\n",
      "epoch: 2, batch: 229, loss: 0.3089897632598877\n",
      "epoch: 2, batch: 230, loss: 0.2202335149049759\n",
      "epoch: 2, batch: 231, loss: 0.22485153377056122\n",
      "epoch: 2, batch: 232, loss: 0.3567656874656677\n",
      "epoch: 2, batch: 233, loss: 0.2567124366760254\n",
      "epoch: 2, batch: 234, loss: 0.48248112201690674\n",
      "epoch: 2, batch: 235, loss: 0.262988418340683\n",
      "epoch: 2, batch: 236, loss: 0.39201363921165466\n",
      "epoch: 2, batch: 237, loss: 0.3354256749153137\n",
      "epoch: 2, batch: 238, loss: 0.27595755457878113\n",
      "epoch: 2, batch: 239, loss: 0.12054373323917389\n",
      "epoch: 2, batch: 240, loss: 0.304906964302063\n",
      "epoch: 2, batch: 241, loss: 0.22975438833236694\n",
      "epoch: 2, batch: 242, loss: 0.2855629622936249\n",
      "epoch: 2, batch: 243, loss: 0.19001366198062897\n",
      "epoch: 2, batch: 244, loss: 0.35091811418533325\n",
      "epoch: 2, batch: 245, loss: 0.24996742606163025\n",
      "epoch: 2, batch: 246, loss: 0.19254228472709656\n",
      "epoch: 2, batch: 247, loss: 0.4118967652320862\n",
      "epoch: 2, batch: 248, loss: 0.4155624210834503\n",
      "epoch: 2, batch: 249, loss: 0.2504968047142029\n",
      "epoch: 2, batch: 250, loss: 0.27585214376449585\n",
      "epoch: 2, batch: 251, loss: 0.2578633725643158\n",
      "epoch: 2, batch: 252, loss: 0.2407531440258026\n",
      "epoch: 2, batch: 253, loss: 0.3441905677318573\n",
      "epoch: 2, batch: 254, loss: 0.25091981887817383\n",
      "epoch: 2, batch: 255, loss: 0.3670578598976135\n",
      "epoch: 2, batch: 256, loss: 0.3263792097568512\n",
      "epoch: 2, batch: 257, loss: 0.46971213817596436\n",
      "epoch: 2, batch: 258, loss: 0.17146599292755127\n",
      "epoch: 2, batch: 259, loss: 0.47107529640197754\n",
      "epoch: 2, batch: 260, loss: 0.32467812299728394\n",
      "epoch: 2, batch: 261, loss: 0.2961927354335785\n",
      "epoch: 2, batch: 262, loss: 0.4600681662559509\n",
      "epoch: 2, batch: 263, loss: 0.2572086453437805\n",
      "epoch: 2, batch: 264, loss: 0.4865182340145111\n",
      "epoch: 2, batch: 265, loss: 0.27599555253982544\n",
      "epoch: 2, batch: 266, loss: 0.3765318989753723\n",
      "epoch: 2, batch: 267, loss: 0.47582268714904785\n",
      "epoch: 2, batch: 268, loss: 0.4537637233734131\n",
      "epoch: 2, batch: 269, loss: 0.24674862623214722\n",
      "epoch: 2, batch: 270, loss: 0.5287928581237793\n",
      "epoch: 2, batch: 271, loss: 0.2324550598859787\n",
      "epoch: 2, batch: 272, loss: 0.23809869587421417\n",
      "epoch: 2, batch: 273, loss: 0.26067012548446655\n",
      "epoch: 2, batch: 274, loss: 0.24269236624240875\n",
      "epoch: 2, batch: 275, loss: 0.3357692360877991\n",
      "epoch: 2, batch: 276, loss: 0.27055075764656067\n",
      "epoch: 2, batch: 277, loss: 0.3509475588798523\n",
      "epoch: 2, batch: 278, loss: 0.30892282724380493\n",
      "epoch: 2, batch: 279, loss: 0.2670188546180725\n",
      "epoch: 2, batch: 280, loss: 0.3101591467857361\n",
      "epoch: 2, batch: 281, loss: 0.34206366539001465\n",
      "epoch: 2, batch: 282, loss: 0.3114907145500183\n",
      "epoch: 2, batch: 283, loss: 0.5206855535507202\n",
      "epoch: 2, batch: 284, loss: 0.29944583773612976\n",
      "epoch: 2, batch: 285, loss: 0.5093433856964111\n",
      "epoch: 2, batch: 286, loss: 0.201849564909935\n",
      "epoch: 2, batch: 287, loss: 0.32694211602211\n",
      "epoch: 2, batch: 288, loss: 0.372149258852005\n",
      "epoch: 2, batch: 289, loss: 0.33812424540519714\n",
      "epoch: 2, batch: 290, loss: 0.28237831592559814\n",
      "epoch: 2, batch: 291, loss: 0.13253286480903625\n",
      "epoch: 2, batch: 292, loss: 0.2578727602958679\n",
      "epoch: 2, batch: 293, loss: 0.16674986481666565\n",
      "epoch: 2, batch: 294, loss: 0.45051679015159607\n",
      "epoch: 2, batch: 295, loss: 0.22100883722305298\n",
      "epoch: 2, batch: 296, loss: 0.3758033215999603\n",
      "epoch: 2, batch: 297, loss: 0.2578943967819214\n",
      "epoch: 2, batch: 298, loss: 0.42816847562789917\n",
      "epoch: 2, batch: 299, loss: 0.1353873759508133\n",
      "epoch: 2, batch: 300, loss: 0.3121047914028168\n",
      "epoch: 2, batch: 301, loss: 0.35381975769996643\n",
      "epoch: 2, batch: 302, loss: 0.19610874354839325\n",
      "epoch: 2, batch: 303, loss: 0.3664926588535309\n",
      "epoch: 2, batch: 304, loss: 0.3190520703792572\n",
      "epoch: 2, batch: 305, loss: 0.26003625988960266\n",
      "epoch: 2, batch: 306, loss: 0.38518717885017395\n",
      "epoch: 2, batch: 307, loss: 0.20788408815860748\n",
      "epoch: 2, batch: 308, loss: 0.2561511695384979\n",
      "epoch: 2, batch: 309, loss: 0.16698037087917328\n",
      "epoch: 2, batch: 310, loss: 0.27162063121795654\n",
      "epoch: 2, batch: 311, loss: 0.3552074432373047\n",
      "epoch: 2, batch: 312, loss: 0.215054452419281\n",
      "epoch: 2, batch: 313, loss: 0.29790860414505005\n",
      "epoch: 2, batch: 314, loss: 0.24550551176071167\n",
      "epoch: 2, batch: 315, loss: 0.3724479079246521\n",
      "epoch: 2, batch: 316, loss: 0.41520917415618896\n",
      "epoch: 2, batch: 317, loss: 0.29394233226776123\n",
      "epoch: 2, batch: 318, loss: 0.4964776635169983\n",
      "epoch: 2, batch: 319, loss: 0.43792077898979187\n",
      "epoch: 2, batch: 320, loss: 0.1189906895160675\n",
      "epoch: 2, batch: 321, loss: 0.4886791408061981\n",
      "epoch: 2, batch: 322, loss: 0.3001106083393097\n",
      "epoch: 2, batch: 323, loss: 0.1868368685245514\n",
      "epoch: 2, batch: 324, loss: 0.24612650275230408\n",
      "epoch: 2, batch: 325, loss: 0.37476128339767456\n",
      "epoch: 2, batch: 326, loss: 0.20248940587043762\n",
      "epoch: 2, batch: 327, loss: 0.2737804353237152\n",
      "epoch: 2, batch: 328, loss: 0.17167329788208008\n",
      "epoch: 2, batch: 329, loss: 0.2883909344673157\n",
      "epoch: 2, batch: 330, loss: 0.5018638968467712\n",
      "epoch: 2, batch: 331, loss: 0.3062698245048523\n",
      "epoch: 2, batch: 332, loss: 0.2240489274263382\n",
      "epoch: 2, batch: 333, loss: 0.25528159737586975\n",
      "epoch: 2, batch: 334, loss: 0.257996529340744\n",
      "epoch: 2, batch: 335, loss: 0.16146458685398102\n",
      "epoch: 2, batch: 336, loss: 0.286063551902771\n",
      "epoch: 2, batch: 337, loss: 0.22649268805980682\n",
      "epoch: 2, batch: 338, loss: 0.23408472537994385\n",
      "epoch: 2, batch: 339, loss: 0.22577638924121857\n",
      "epoch: 2, batch: 340, loss: 0.27075299620628357\n",
      "epoch: 2, batch: 341, loss: 0.3188098073005676\n",
      "epoch: 2, batch: 342, loss: 0.4742601215839386\n",
      "epoch: 2, batch: 343, loss: 0.41585686802864075\n",
      "epoch: 2, batch: 344, loss: 0.32117974758148193\n",
      "epoch: 2, batch: 345, loss: 0.2431391179561615\n",
      "epoch: 2, batch: 346, loss: 0.17988044023513794\n",
      "epoch: 2, batch: 347, loss: 0.2520049214363098\n",
      "epoch: 2, batch: 348, loss: 0.2569689154624939\n",
      "epoch: 2, batch: 349, loss: 0.2641834616661072\n",
      "epoch: 2, batch: 350, loss: 0.1958903819322586\n",
      "epoch: 2, batch: 351, loss: 0.2751169502735138\n",
      "epoch: 2, batch: 352, loss: 0.4149198830127716\n",
      "epoch: 2, batch: 353, loss: 0.4995362162590027\n",
      "epoch: 2, batch: 354, loss: 0.2713617980480194\n",
      "epoch: 2, batch: 355, loss: 0.2624412178993225\n",
      "epoch: 2, batch: 356, loss: 0.20949944853782654\n",
      "epoch: 2, batch: 357, loss: 0.20360322296619415\n",
      "epoch: 2, batch: 358, loss: 0.3278255760669708\n",
      "epoch: 2, batch: 359, loss: 0.3053460419178009\n",
      "epoch: 2, batch: 360, loss: 0.33734971284866333\n",
      "epoch: 2, batch: 361, loss: 0.32482805848121643\n",
      "epoch: 2, batch: 362, loss: 0.13901080191135406\n",
      "epoch: 2, batch: 363, loss: 0.30073705315589905\n",
      "epoch: 2, batch: 364, loss: 0.18208524584770203\n",
      "epoch: 2, batch: 365, loss: 0.20508980751037598\n",
      "epoch: 2, batch: 366, loss: 0.17081758379936218\n",
      "epoch: 2, batch: 367, loss: 0.28278303146362305\n",
      "epoch: 2, batch: 368, loss: 0.3650635778903961\n",
      "epoch: 2, batch: 369, loss: 0.3869350850582123\n",
      "epoch: 2, batch: 370, loss: 0.2112068086862564\n",
      "epoch: 2, batch: 371, loss: 0.4220865070819855\n",
      "epoch: 2, batch: 372, loss: 0.2037530541419983\n",
      "epoch: 2, batch: 373, loss: 0.1436776965856552\n",
      "epoch: 2, batch: 374, loss: 0.38064366579055786\n",
      "epoch: 2, batch: 375, loss: 0.472387433052063\n",
      "epoch: 2, batch: 376, loss: 0.3490009307861328\n",
      "epoch: 2, batch: 377, loss: 0.26001811027526855\n",
      "epoch: 2, batch: 378, loss: 0.3405468463897705\n",
      "epoch: 2, batch: 379, loss: 0.23462048172950745\n",
      "epoch: 2, batch: 380, loss: 0.3067624568939209\n",
      "epoch: 2, batch: 381, loss: 0.3440020978450775\n",
      "epoch: 2, batch: 382, loss: 0.27024272084236145\n",
      "epoch: 2, batch: 383, loss: 0.20199449360370636\n",
      "epoch: 2, batch: 384, loss: 0.31697994470596313\n",
      "epoch: 2, batch: 385, loss: 0.2390778511762619\n",
      "epoch: 2, batch: 386, loss: 0.2742709219455719\n",
      "epoch: 2, batch: 387, loss: 0.21142984926700592\n",
      "epoch: 2, batch: 388, loss: 0.28345322608947754\n",
      "epoch: 2, batch: 389, loss: 0.3526294529438019\n",
      "epoch: 2, batch: 390, loss: 0.40401220321655273\n",
      "epoch: 2, batch: 391, loss: 0.3089364767074585\n",
      "epoch: 2, batch: 392, loss: 0.30230584740638733\n",
      "epoch: 2, batch: 393, loss: 0.5010302662849426\n",
      "epoch: 2, batch: 394, loss: 0.2970081567764282\n",
      "epoch: 2, batch: 395, loss: 0.24602042138576508\n",
      "epoch: 2, batch: 396, loss: 0.17112776637077332\n",
      "epoch: 2, batch: 397, loss: 0.3558211028575897\n",
      "epoch: 2, batch: 398, loss: 0.2039124071598053\n",
      "epoch: 2, batch: 399, loss: 0.30587732791900635\n",
      "epoch: 2, batch: 400, loss: 0.26453065872192383\n",
      "epoch: 2, batch: 401, loss: 0.16805154085159302\n",
      "epoch: 2, batch: 402, loss: 0.14597024023532867\n",
      "epoch: 2, batch: 403, loss: 0.23775842785835266\n",
      "epoch: 2, batch: 404, loss: 0.18361663818359375\n",
      "epoch: 2, batch: 405, loss: 0.5416817665100098\n",
      "epoch: 2, batch: 406, loss: 0.2670370042324066\n",
      "epoch: 2, batch: 407, loss: 0.39880356192588806\n",
      "epoch: 2, batch: 408, loss: 0.3365575671195984\n",
      "epoch: 2, batch: 409, loss: 0.377573162317276\n",
      "epoch: 2, batch: 410, loss: 0.42818793654441833\n",
      "epoch: 2, batch: 411, loss: 0.30372655391693115\n",
      "epoch: 2, batch: 412, loss: 0.35316094756126404\n",
      "epoch: 2, batch: 413, loss: 0.20228421688079834\n",
      "epoch: 2, batch: 414, loss: 0.43906542658805847\n",
      "epoch: 2, batch: 415, loss: 0.4477556347846985\n",
      "epoch: 2, batch: 416, loss: 0.1995162069797516\n",
      "epoch: 2, batch: 417, loss: 0.23401996493339539\n",
      "epoch: 2, batch: 418, loss: 0.526639461517334\n",
      "epoch: 2, batch: 419, loss: 0.2599382698535919\n",
      "epoch: 2, batch: 420, loss: 0.23703545331954956\n",
      "epoch: 2, batch: 421, loss: 0.26583167910575867\n",
      "epoch: 2, batch: 422, loss: 0.5736358761787415\n",
      "epoch: 2, batch: 423, loss: 0.30750516057014465\n",
      "epoch: 2, batch: 424, loss: 0.2030884325504303\n",
      "epoch: 2, batch: 425, loss: 0.3466228246688843\n",
      "epoch: 2, batch: 426, loss: 0.3227264881134033\n",
      "epoch: 2, batch: 427, loss: 0.3504064977169037\n",
      "epoch: 2, batch: 428, loss: 0.14522820711135864\n",
      "epoch: 2, batch: 429, loss: 0.286663293838501\n",
      "epoch: 2, batch: 430, loss: 0.10727561265230179\n",
      "epoch: 2, batch: 431, loss: 0.17743027210235596\n",
      "epoch: 2, batch: 432, loss: 0.22851814329624176\n",
      "epoch: 2, batch: 433, loss: 0.45223236083984375\n",
      "epoch: 2, batch: 434, loss: 0.6558485627174377\n",
      "epoch: 2, batch: 435, loss: 0.25605469942092896\n",
      "epoch: 2, batch: 436, loss: 0.38987550139427185\n",
      "epoch: 2, batch: 437, loss: 0.2697175443172455\n",
      "epoch: 2, batch: 438, loss: 0.22135356068611145\n",
      "epoch: 2, batch: 439, loss: 0.17526821792125702\n",
      "epoch: 2, batch: 440, loss: 0.22560936212539673\n",
      "epoch: 2, batch: 441, loss: 0.5112918019294739\n",
      "epoch: 2, batch: 442, loss: 0.3418423533439636\n",
      "epoch: 2, batch: 443, loss: 0.5880900025367737\n",
      "epoch: 2, batch: 444, loss: 0.4336793124675751\n",
      "epoch: 2, batch: 445, loss: 0.16643790900707245\n",
      "epoch: 2, batch: 446, loss: 0.25608837604522705\n",
      "epoch: 2, batch: 447, loss: 0.3376554846763611\n",
      "epoch: 2, batch: 448, loss: 0.13268448412418365\n",
      "epoch: 2, batch: 449, loss: 0.1782567799091339\n",
      "epoch: 2, batch: 450, loss: 0.33381548523902893\n",
      "epoch: 2, batch: 451, loss: 0.12294274568557739\n",
      "epoch: 2, batch: 452, loss: 0.212044358253479\n",
      "epoch: 2, batch: 453, loss: 0.2517460286617279\n",
      "epoch: 2, batch: 454, loss: 0.1511756181716919\n",
      "epoch: 2, batch: 455, loss: 0.37250977754592896\n",
      "epoch: 2, batch: 456, loss: 0.3209202289581299\n",
      "epoch: 2, batch: 457, loss: 0.1504628211259842\n",
      "epoch: 2, batch: 458, loss: 0.3660334348678589\n",
      "epoch: 2, batch: 459, loss: 0.25607773661613464\n",
      "epoch: 2, batch: 460, loss: 0.3667062520980835\n",
      "epoch: 2, batch: 461, loss: 0.15977202355861664\n",
      "epoch: 2, batch: 462, loss: 0.5407726764678955\n",
      "epoch: 2, batch: 463, loss: 0.3410384953022003\n",
      "epoch: 2, batch: 464, loss: 0.22072568535804749\n",
      "epoch: 2, batch: 465, loss: 0.5238032937049866\n",
      "epoch: 2, batch: 466, loss: 0.4655804932117462\n",
      "epoch: 2, batch: 467, loss: 0.29346519708633423\n",
      "epoch: 2, batch: 468, loss: 0.3486346900463104\n",
      "epoch: 2, batch: 469, loss: 0.2629317045211792\n",
      "epoch: 2, batch: 470, loss: 0.2121831327676773\n",
      "epoch: 2, batch: 471, loss: 0.4245699346065521\n",
      "epoch: 2, batch: 472, loss: 0.30000314116477966\n",
      "epoch: 2, batch: 473, loss: 0.364810049533844\n",
      "epoch: 2, batch: 474, loss: 0.3541547358036041\n",
      "epoch: 2, batch: 475, loss: 0.11244963109493256\n",
      "epoch: 2, batch: 476, loss: 0.29900607466697693\n",
      "epoch: 2, batch: 477, loss: 0.1664237231016159\n",
      "epoch: 2, batch: 478, loss: 0.2592679262161255\n",
      "epoch: 2, batch: 479, loss: 0.3243452310562134\n",
      "epoch: 2, batch: 480, loss: 0.30434340238571167\n",
      "epoch: 2, batch: 481, loss: 0.26749423146247864\n",
      "epoch: 2, batch: 482, loss: 0.3239900469779968\n",
      "epoch: 2, batch: 483, loss: 0.32324284315109253\n",
      "epoch: 2, batch: 484, loss: 0.24443000555038452\n",
      "epoch: 2, batch: 485, loss: 0.20216475427150726\n",
      "epoch: 2, batch: 486, loss: 0.26966843008995056\n",
      "epoch: 2, batch: 487, loss: 0.2916642129421234\n",
      "epoch: 2, batch: 488, loss: 0.2993650734424591\n",
      "epoch: 2, batch: 489, loss: 0.4505119323730469\n",
      "epoch: 2, batch: 490, loss: 0.3473098576068878\n",
      "epoch: 2, batch: 491, loss: 0.3278334140777588\n",
      "epoch: 2, batch: 492, loss: 0.4077604115009308\n",
      "epoch: 2, batch: 493, loss: 0.1693645417690277\n",
      "epoch: 2, batch: 494, loss: 0.34920457005500793\n",
      "epoch: 2, batch: 495, loss: 0.17170563340187073\n",
      "epoch: 2, batch: 496, loss: 0.2935011684894562\n",
      "epoch: 2, batch: 497, loss: 0.27614349126815796\n",
      "epoch: 2, batch: 498, loss: 0.36549508571624756\n",
      "epoch: 2, batch: 499, loss: 0.40610459446907043\n",
      "epoch: 2, batch: 500, loss: 0.33427220582962036\n",
      "epoch: 2, batch: 501, loss: 0.44963720440864563\n",
      "epoch: 2, batch: 502, loss: 0.2866445779800415\n",
      "epoch: 2, batch: 503, loss: 0.3637239933013916\n",
      "epoch: 2, batch: 504, loss: 0.19481700658798218\n",
      "epoch: 2, batch: 505, loss: 0.29264208674430847\n",
      "epoch: 2, batch: 506, loss: 0.32212120294570923\n",
      "epoch: 2, batch: 507, loss: 0.40174612402915955\n",
      "epoch: 2, batch: 508, loss: 0.32691457867622375\n",
      "epoch: 2, batch: 509, loss: 0.5194188356399536\n",
      "epoch: 2, batch: 510, loss: 0.2147338092327118\n",
      "epoch: 2, batch: 511, loss: 0.46332165598869324\n",
      "epoch: 2, batch: 512, loss: 0.44306594133377075\n",
      "epoch: 2, batch: 513, loss: 0.26761695742607117\n",
      "epoch: 2, batch: 514, loss: 0.2885645925998688\n",
      "epoch: 2, batch: 515, loss: 0.23373398184776306\n",
      "epoch: 2, batch: 516, loss: 0.2092251032590866\n",
      "epoch: 2, batch: 517, loss: 0.20764218270778656\n",
      "epoch: 2, batch: 518, loss: 0.1543516218662262\n",
      "epoch: 2, batch: 519, loss: 0.1316450536251068\n",
      "epoch: 2, batch: 520, loss: 0.32272815704345703\n",
      "epoch: 2, batch: 521, loss: 0.2368931919336319\n",
      "epoch: 2, batch: 522, loss: 0.245929554104805\n",
      "epoch: 2, batch: 523, loss: 0.2431029975414276\n",
      "epoch: 2, batch: 524, loss: 0.3064710795879364\n",
      "epoch: 2, batch: 525, loss: 0.27399927377700806\n",
      "epoch: 2, batch: 526, loss: 0.20412814617156982\n",
      "epoch: 2, batch: 527, loss: 0.17159223556518555\n",
      "epoch: 2, batch: 528, loss: 0.31215712428092957\n",
      "epoch: 2, batch: 529, loss: 0.21326512098312378\n",
      "epoch: 2, batch: 530, loss: 0.19013087451457977\n",
      "epoch: 2, batch: 531, loss: 0.19609227776527405\n",
      "epoch: 2, batch: 532, loss: 0.36885711550712585\n",
      "epoch: 2, batch: 533, loss: 0.33606410026550293\n",
      "epoch: 2, batch: 534, loss: 0.3150971531867981\n",
      "epoch: 2, batch: 535, loss: 0.4604441523551941\n",
      "epoch: 2, batch: 536, loss: 0.2639135420322418\n",
      "epoch: 2, batch: 537, loss: 0.5413904190063477\n",
      "epoch: 2, batch: 538, loss: 0.3416919410228729\n",
      "epoch: 2, batch: 539, loss: 0.46634143590927124\n",
      "epoch: 2, batch: 540, loss: 0.15685920417308807\n",
      "epoch: 2, batch: 541, loss: 0.18951664865016937\n",
      "epoch: 2, batch: 542, loss: 0.21279405057430267\n",
      "epoch: 2, batch: 543, loss: 0.18531158566474915\n",
      "epoch: 2, batch: 544, loss: 0.2567891478538513\n",
      "epoch: 2, batch: 545, loss: 0.49910372495651245\n",
      "epoch: 2, batch: 546, loss: 0.37528276443481445\n",
      "epoch: 2, batch: 547, loss: 0.2531396746635437\n",
      "epoch: 2, batch: 548, loss: 0.16286860406398773\n",
      "epoch: 2, batch: 549, loss: 0.16579681634902954\n",
      "epoch: 2, batch: 550, loss: 0.1779431402683258\n",
      "epoch: 2, batch: 551, loss: 0.31082502007484436\n",
      "epoch: 2, batch: 552, loss: 0.203426793217659\n",
      "epoch: 2, batch: 553, loss: 0.3768405318260193\n",
      "epoch: 2, batch: 554, loss: 0.1420619636774063\n",
      "epoch: 2, batch: 555, loss: 0.4070867896080017\n",
      "epoch: 2, batch: 556, loss: 0.3079754710197449\n",
      "epoch: 2, batch: 557, loss: 0.31421470642089844\n",
      "epoch: 2, batch: 558, loss: 0.19368937611579895\n",
      "epoch: 2, batch: 559, loss: 0.2720145285129547\n",
      "epoch: 2, batch: 560, loss: 0.25482869148254395\n",
      "epoch: 2, batch: 561, loss: 0.3310129642486572\n",
      "epoch: 2, batch: 562, loss: 0.2057577520608902\n",
      "epoch: 2, batch: 563, loss: 0.22579661011695862\n",
      "epoch: 2, batch: 564, loss: 0.1670731008052826\n",
      "epoch: 2, batch: 565, loss: 0.23005692660808563\n",
      "epoch: 2, batch: 566, loss: 0.3936137855052948\n",
      "epoch: 2, batch: 567, loss: 0.5134803056716919\n",
      "epoch: 2, batch: 568, loss: 0.2049397975206375\n",
      "epoch: 2, batch: 569, loss: 0.3923458158969879\n",
      "epoch: 2, batch: 570, loss: 0.304207444190979\n",
      "epoch: 2, batch: 571, loss: 0.47542881965637207\n",
      "epoch: 2, batch: 572, loss: 0.326410710811615\n",
      "epoch: 2, batch: 573, loss: 0.4549805819988251\n",
      "epoch: 2, batch: 574, loss: 0.43695372343063354\n",
      "epoch: 2, batch: 575, loss: 0.41438841819763184\n",
      "epoch: 2, batch: 576, loss: 0.38330212235450745\n",
      "epoch: 2, batch: 577, loss: 0.14631208777427673\n",
      "epoch: 2, batch: 578, loss: 0.25188279151916504\n",
      "epoch: 2, batch: 579, loss: 0.4982726573944092\n",
      "epoch: 2, batch: 580, loss: 0.12186119705438614\n",
      "epoch: 2, batch: 581, loss: 0.31618961691856384\n",
      "epoch: 2, batch: 582, loss: 0.24145644903182983\n",
      "epoch: 2, batch: 583, loss: 0.23698413372039795\n",
      "epoch: 2, batch: 584, loss: 0.15394270420074463\n",
      "epoch: 2, batch: 585, loss: 0.24887573719024658\n",
      "epoch: 2, batch: 586, loss: 0.21830612421035767\n",
      "epoch: 2, batch: 587, loss: 0.3002399802207947\n",
      "epoch: 2, batch: 588, loss: 0.38040006160736084\n",
      "epoch: 2, batch: 589, loss: 0.33671000599861145\n",
      "epoch: 2, batch: 590, loss: 0.29568684101104736\n",
      "epoch: 2, batch: 591, loss: 0.23291483521461487\n",
      "epoch: 2, batch: 592, loss: 0.18405809998512268\n",
      "epoch: 2, batch: 593, loss: 0.3694205582141876\n",
      "epoch: 2, batch: 594, loss: 0.17729726433753967\n",
      "epoch: 2, batch: 595, loss: 0.2950260639190674\n",
      "epoch: 2, batch: 596, loss: 0.09236691147089005\n",
      "epoch: 2, batch: 597, loss: 0.16614127159118652\n",
      "epoch: 2, batch: 598, loss: 0.34934931993484497\n",
      "epoch: 2, batch: 599, loss: 0.13705341517925262\n",
      "epoch: 2, batch: 600, loss: 0.393198162317276\n",
      "epoch: 2, batch: 601, loss: 0.46242761611938477\n",
      "epoch: 2, batch: 602, loss: 0.2998664379119873\n",
      "epoch: 2, batch: 603, loss: 0.4414476752281189\n",
      "epoch: 2, batch: 604, loss: 0.453940212726593\n",
      "epoch: 2, batch: 605, loss: 0.369335800409317\n",
      "epoch: 2, batch: 606, loss: 0.3389086127281189\n",
      "epoch: 2, batch: 607, loss: 0.24173371493816376\n",
      "epoch: 2, batch: 608, loss: 0.3518927991390228\n",
      "epoch: 2, batch: 609, loss: 0.43641945719718933\n",
      "epoch: 2, batch: 610, loss: 0.1894521713256836\n",
      "epoch: 2, batch: 611, loss: 0.30205637216567993\n",
      "epoch: 2, batch: 612, loss: 0.2121409922838211\n",
      "epoch: 2, batch: 613, loss: 0.36186903715133667\n",
      "epoch: 2, batch: 614, loss: 0.47239741683006287\n",
      "epoch: 2, batch: 615, loss: 0.22350290417671204\n",
      "epoch: 2, batch: 616, loss: 0.454847514629364\n",
      "epoch: 2, batch: 617, loss: 0.41361257433891296\n",
      "epoch: 2, batch: 618, loss: 0.27303868532180786\n",
      "epoch: 2, batch: 619, loss: 0.36136746406555176\n",
      "epoch: 2, batch: 620, loss: 0.3979410231113434\n",
      "epoch: 2, batch: 621, loss: 0.3173038065433502\n",
      "epoch: 2, batch: 622, loss: 0.23105016350746155\n",
      "epoch: 2, batch: 623, loss: 0.14988155663013458\n",
      "epoch: 2, batch: 624, loss: 0.2704201638698578\n",
      "epoch: 2, batch: 625, loss: 0.42879804968833923\n",
      "epoch: 2, batch: 626, loss: 0.5045914053916931\n",
      "epoch: 2, batch: 627, loss: 0.31954532861709595\n",
      "epoch: 2, batch: 628, loss: 0.1815442144870758\n",
      "epoch: 2, batch: 629, loss: 0.19955801963806152\n",
      "epoch: 2, batch: 630, loss: 0.3123731017112732\n",
      "epoch: 2, batch: 631, loss: 0.33686375617980957\n",
      "epoch: 2, batch: 632, loss: 0.14343707263469696\n",
      "epoch: 2, batch: 633, loss: 0.36571505665779114\n",
      "epoch: 2, batch: 634, loss: 0.13513067364692688\n",
      "epoch: 2, batch: 635, loss: 0.08100531995296478\n",
      "epoch: 2, batch: 636, loss: 0.2846195101737976\n",
      "epoch: 2, batch: 637, loss: 0.145760178565979\n",
      "epoch: 2, batch: 638, loss: 0.19273464381694794\n",
      "epoch: 2, batch: 639, loss: 0.46298032999038696\n",
      "epoch: 2, batch: 640, loss: 0.30678442120552063\n",
      "epoch: 2, batch: 641, loss: 0.40054214000701904\n",
      "epoch: 2, batch: 642, loss: 0.21045531332492828\n",
      "epoch: 2, batch: 643, loss: 0.20735733211040497\n",
      "epoch: 2, batch: 644, loss: 0.26651567220687866\n",
      "epoch: 2, batch: 645, loss: 0.31771978735923767\n",
      "epoch: 2, batch: 646, loss: 0.14421527087688446\n",
      "epoch: 2, batch: 647, loss: 0.578108549118042\n",
      "epoch: 2, batch: 648, loss: 0.24368593096733093\n",
      "epoch: 2, batch: 649, loss: 0.25913700461387634\n",
      "epoch: 2, batch: 650, loss: 0.46504542231559753\n",
      "epoch: 2, batch: 651, loss: 0.23797212541103363\n",
      "epoch: 2, batch: 652, loss: 0.3370569050312042\n",
      "epoch: 2, batch: 653, loss: 0.19842170178890228\n",
      "epoch: 2, batch: 654, loss: 0.21788720786571503\n",
      "epoch: 2, batch: 655, loss: 0.3578258752822876\n",
      "epoch: 2, batch: 656, loss: 0.4298383295536041\n",
      "epoch: 2, batch: 657, loss: 0.1573316901922226\n",
      "epoch: 2, batch: 658, loss: 0.22395546734333038\n",
      "epoch: 2, batch: 659, loss: 0.24665634334087372\n",
      "epoch: 2, batch: 660, loss: 0.42205366492271423\n",
      "epoch: 2, batch: 661, loss: 0.2678387463092804\n",
      "epoch: 2, batch: 662, loss: 0.33630824089050293\n",
      "epoch: 2, batch: 663, loss: 0.24747994542121887\n",
      "epoch: 2, batch: 664, loss: 0.2413392812013626\n",
      "epoch: 2, batch: 665, loss: 0.3256857693195343\n",
      "epoch: 2, batch: 666, loss: 0.3092882037162781\n",
      "epoch: 2, batch: 667, loss: 0.4243887960910797\n",
      "epoch: 2, batch: 668, loss: 0.37947699427604675\n",
      "epoch: 2, batch: 669, loss: 0.2059333175420761\n",
      "epoch: 2, batch: 670, loss: 0.289562851190567\n",
      "epoch: 2, batch: 671, loss: 0.4460853338241577\n",
      "epoch: 2, batch: 672, loss: 0.307978093624115\n",
      "epoch: 2, batch: 673, loss: 0.27113935351371765\n",
      "epoch: 2, batch: 674, loss: 0.37628883123397827\n",
      "epoch: 2, batch: 675, loss: 0.22953341901302338\n",
      "epoch: 2, batch: 676, loss: 0.3198380768299103\n",
      "epoch: 2, batch: 677, loss: 0.1794164776802063\n",
      "epoch: 2, batch: 678, loss: 0.24469541013240814\n",
      "epoch: 2, batch: 679, loss: 0.30118638277053833\n",
      "epoch: 2, batch: 680, loss: 0.2835942506790161\n",
      "epoch: 2, batch: 681, loss: 0.18372684717178345\n",
      "epoch: 2, batch: 682, loss: 0.23835670948028564\n",
      "epoch: 2, batch: 683, loss: 0.0786963701248169\n",
      "epoch: 2, batch: 684, loss: 0.35800960659980774\n",
      "epoch: 2, batch: 685, loss: 0.13109050691127777\n",
      "epoch: 2, batch: 686, loss: 0.24410392343997955\n",
      "epoch: 2, batch: 687, loss: 0.16147062182426453\n",
      "epoch: 2, batch: 688, loss: 0.41271552443504333\n",
      "epoch: 2, batch: 689, loss: 0.34328773617744446\n",
      "epoch: 2, batch: 690, loss: 0.4123011827468872\n",
      "epoch: 2, batch: 691, loss: 0.2306743711233139\n",
      "epoch: 2, batch: 692, loss: 0.2946113646030426\n",
      "epoch: 2, batch: 693, loss: 0.19028586149215698\n",
      "epoch: 2, batch: 694, loss: 0.16176986694335938\n",
      "epoch: 2, batch: 695, loss: 0.25098082423210144\n",
      "epoch: 2, batch: 696, loss: 0.44404491782188416\n",
      "epoch: 2, batch: 697, loss: 0.1856117695569992\n",
      "epoch: 2, batch: 698, loss: 0.14465180039405823\n",
      "epoch: 2, batch: 699, loss: 0.3744712173938751\n",
      "epoch: 2, batch: 700, loss: 0.32582351565361023\n",
      "epoch: 2, batch: 701, loss: 0.19291171431541443\n",
      "epoch: 2, batch: 702, loss: 0.2682781517505646\n",
      "epoch: 2, batch: 703, loss: 0.3733952045440674\n",
      "epoch: 2, batch: 704, loss: 0.3260880410671234\n",
      "epoch: 2, batch: 705, loss: 0.2207905352115631\n",
      "epoch: 2, batch: 706, loss: 0.20578598976135254\n",
      "epoch: 2, batch: 707, loss: 0.21903643012046814\n",
      "epoch: 2, batch: 708, loss: 0.32137855887413025\n",
      "epoch: 2, batch: 709, loss: 0.3512718975543976\n",
      "epoch: 2, batch: 710, loss: 0.1699298769235611\n",
      "epoch: 2, batch: 711, loss: 0.19026115536689758\n",
      "epoch: 2, batch: 712, loss: 0.2574170231819153\n",
      "epoch: 2, batch: 713, loss: 0.31737226247787476\n",
      "epoch: 2, batch: 714, loss: 0.2698165476322174\n",
      "epoch: 2, batch: 715, loss: 0.23211608827114105\n",
      "epoch: 2, batch: 716, loss: 0.46205562353134155\n",
      "epoch: 2, batch: 717, loss: 0.521855890750885\n",
      "epoch: 2, batch: 718, loss: 0.2791939377784729\n",
      "epoch: 2, batch: 719, loss: 0.1286480575799942\n",
      "epoch: 2, batch: 720, loss: 0.22796110808849335\n",
      "epoch: 2, batch: 721, loss: 0.2414330095052719\n",
      "epoch: 2, batch: 722, loss: 0.4437180161476135\n",
      "epoch: 2, batch: 723, loss: 0.4741571247577667\n",
      "epoch: 2, batch: 724, loss: 0.23675765097141266\n",
      "epoch: 2, batch: 725, loss: 0.2501473128795624\n",
      "epoch: 2, batch: 726, loss: 0.30664896965026855\n",
      "epoch: 2, batch: 727, loss: 0.2215126007795334\n",
      "epoch: 2, batch: 728, loss: 0.247178316116333\n",
      "epoch: 2, batch: 729, loss: 0.15424062311649323\n",
      "epoch: 2, batch: 730, loss: 0.24876831471920013\n",
      "epoch: 2, batch: 731, loss: 0.3112128973007202\n",
      "epoch: 2, batch: 732, loss: 0.24233469367027283\n",
      "epoch: 2, batch: 733, loss: 0.21685856580734253\n",
      "epoch: 2, batch: 734, loss: 0.29110798239707947\n",
      "epoch: 2, batch: 735, loss: 0.4595192074775696\n",
      "epoch: 2, batch: 736, loss: 0.30052730441093445\n",
      "epoch: 2, batch: 737, loss: 0.428300678730011\n",
      "epoch: 2, batch: 738, loss: 0.44038650393486023\n",
      "epoch: 2, batch: 739, loss: 0.47235310077667236\n",
      "epoch: 2, batch: 740, loss: 0.18602804839611053\n",
      "epoch: 2, batch: 741, loss: 0.3530599772930145\n",
      "epoch: 2, batch: 742, loss: 0.3945867419242859\n",
      "epoch: 2, batch: 743, loss: 0.29472458362579346\n",
      "epoch: 2, batch: 744, loss: 0.38538697361946106\n",
      "epoch: 2, batch: 745, loss: 0.12193139642477036\n",
      "epoch: 2, batch: 746, loss: 0.20667652785778046\n",
      "epoch: 2, batch: 747, loss: 0.3253474831581116\n",
      "epoch: 2, batch: 748, loss: 0.1938246488571167\n",
      "epoch: 2, batch: 749, loss: 0.21623127162456512\n",
      "epoch: 2, batch: 750, loss: 0.19602669775485992\n",
      "epoch: 2, batch: 751, loss: 0.4800950884819031\n",
      "epoch: 2, batch: 752, loss: 0.24612556397914886\n",
      "epoch: 2, batch: 753, loss: 0.2514723539352417\n",
      "epoch: 2, batch: 754, loss: 0.249717116355896\n",
      "epoch: 2, batch: 755, loss: 0.2579507827758789\n",
      "epoch: 2, batch: 756, loss: 0.19676338136196136\n",
      "epoch: 2, batch: 757, loss: 0.4358615279197693\n",
      "epoch: 2, batch: 758, loss: 0.21379047632217407\n",
      "epoch: 2, batch: 759, loss: 0.3990616500377655\n",
      "epoch: 2, batch: 760, loss: 0.2358422577381134\n",
      "epoch: 2, batch: 761, loss: 0.22050821781158447\n",
      "epoch: 2, batch: 762, loss: 0.15543314814567566\n",
      "epoch: 2, batch: 763, loss: 0.16156341135501862\n",
      "epoch: 2, batch: 764, loss: 0.2682655453681946\n",
      "epoch: 2, batch: 765, loss: 0.18921878933906555\n",
      "epoch: 2, batch: 766, loss: 0.3662753701210022\n",
      "epoch: 2, batch: 767, loss: 0.22839120030403137\n",
      "epoch: 2, batch: 768, loss: 0.34686845541000366\n",
      "epoch: 2, batch: 769, loss: 0.2687372863292694\n",
      "epoch: 2, batch: 770, loss: 0.11881112307310104\n",
      "epoch: 2, batch: 771, loss: 0.22005526721477509\n",
      "epoch: 2, batch: 772, loss: 0.2727198600769043\n",
      "epoch: 2, batch: 773, loss: 0.31675878167152405\n",
      "epoch: 2, batch: 774, loss: 0.3387657403945923\n",
      "epoch: 2, batch: 775, loss: 0.23590686917304993\n",
      "epoch: 2, batch: 776, loss: 0.2689835727214813\n",
      "epoch: 2, batch: 777, loss: 0.5321117639541626\n",
      "epoch: 2, batch: 778, loss: 0.30558085441589355\n",
      "epoch: 2, batch: 779, loss: 0.3508673310279846\n",
      "epoch: 2, batch: 780, loss: 0.1761932522058487\n",
      "epoch: 2, batch: 781, loss: 0.06036376953125\n",
      "epoch: 2, batch: 782, loss: 0.40962326526641846\n",
      "epoch: 2, batch: 783, loss: 0.22527417540550232\n",
      "epoch: 2, batch: 784, loss: 0.33984214067459106\n",
      "epoch: 2, batch: 785, loss: 0.22263237833976746\n",
      "epoch: 2, batch: 786, loss: 0.18653151392936707\n",
      "epoch: 2, batch: 787, loss: 0.2931477427482605\n",
      "epoch: 2, batch: 788, loss: 0.2354479283094406\n",
      "epoch: 2, batch: 789, loss: 0.1907837986946106\n",
      "epoch: 2, batch: 790, loss: 0.2590889632701874\n",
      "epoch: 2, batch: 791, loss: 0.14286872744560242\n",
      "epoch: 2, batch: 792, loss: 0.19535240530967712\n",
      "epoch: 2, batch: 793, loss: 0.2504534125328064\n",
      "epoch: 2, batch: 794, loss: 0.39702510833740234\n",
      "epoch: 2, batch: 795, loss: 0.3280678689479828\n",
      "epoch: 2, batch: 796, loss: 0.21733161807060242\n",
      "epoch: 2, batch: 797, loss: 0.574742317199707\n",
      "epoch: 2, batch: 798, loss: 0.29758620262145996\n",
      "epoch: 2, batch: 799, loss: 0.12737715244293213\n",
      "epoch: 2, batch: 800, loss: 0.32814541459083557\n",
      "epoch: 2, batch: 801, loss: 0.3280821144580841\n",
      "epoch: 2, batch: 802, loss: 0.3615489602088928\n",
      "epoch: 2, batch: 803, loss: 0.20909671485424042\n",
      "epoch: 2, batch: 804, loss: 0.3494519591331482\n",
      "epoch: 2, batch: 805, loss: 0.38944417238235474\n",
      "epoch: 2, batch: 806, loss: 0.19649849832057953\n",
      "epoch: 2, batch: 807, loss: 0.24301671981811523\n",
      "epoch: 2, batch: 808, loss: 0.5449562668800354\n",
      "epoch: 2, batch: 809, loss: 0.23313021659851074\n",
      "epoch: 2, batch: 810, loss: 0.29445749521255493\n",
      "epoch: 2, batch: 811, loss: 0.38565966486930847\n",
      "epoch: 2, batch: 812, loss: 0.3005613386631012\n",
      "epoch: 2, batch: 813, loss: 0.41878870129585266\n",
      "epoch: 2, batch: 814, loss: 0.23826895654201508\n",
      "epoch: 2, batch: 815, loss: 0.24271835386753082\n",
      "epoch: 2, batch: 816, loss: 0.43955129384994507\n",
      "epoch: 2, batch: 817, loss: 0.23064766824245453\n",
      "epoch: 2, batch: 818, loss: 0.29422715306282043\n",
      "epoch: 2, batch: 819, loss: 0.2485094666481018\n",
      "epoch: 2, batch: 820, loss: 0.5623216032981873\n",
      "epoch: 2, batch: 821, loss: 0.1480959951877594\n",
      "epoch: 2, batch: 822, loss: 0.2425360381603241\n",
      "epoch: 2, batch: 823, loss: 0.26922523975372314\n",
      "epoch: 2, batch: 824, loss: 0.365702360868454\n",
      "epoch: 2, batch: 825, loss: 0.23254159092903137\n",
      "epoch: 2, batch: 826, loss: 0.17457064986228943\n",
      "epoch: 2, batch: 827, loss: 0.24803023040294647\n",
      "epoch: 2, batch: 828, loss: 0.3394429683685303\n",
      "epoch: 2, batch: 829, loss: 0.27046632766723633\n",
      "epoch: 2, batch: 830, loss: 0.22968414425849915\n",
      "epoch: 2, batch: 831, loss: 0.1242426410317421\n",
      "epoch: 2, batch: 832, loss: 0.22099776566028595\n",
      "epoch: 2, batch: 833, loss: 0.20634570717811584\n",
      "epoch: 2, batch: 834, loss: 0.32538357377052307\n",
      "epoch: 2, batch: 835, loss: 0.22986119985580444\n",
      "epoch: 2, batch: 836, loss: 0.19618582725524902\n",
      "epoch: 2, batch: 837, loss: 0.11393695324659348\n",
      "epoch: 2, batch: 838, loss: 0.13385450839996338\n",
      "epoch: 2, batch: 839, loss: 0.3774219751358032\n",
      "epoch: 2, batch: 840, loss: 0.21940168738365173\n",
      "epoch: 2, batch: 841, loss: 0.1801416277885437\n",
      "epoch: 2, batch: 842, loss: 0.38743075728416443\n",
      "epoch: 2, batch: 843, loss: 0.17027388513088226\n",
      "epoch: 2, batch: 844, loss: 0.44095784425735474\n",
      "epoch: 2, batch: 845, loss: 0.3267790973186493\n",
      "epoch: 2, batch: 846, loss: 0.2924449145793915\n",
      "epoch: 2, batch: 847, loss: 0.290263831615448\n",
      "epoch: 2, batch: 848, loss: 0.15086254477500916\n",
      "epoch: 2, batch: 849, loss: 0.266071081161499\n",
      "epoch: 2, batch: 850, loss: 0.30947715044021606\n",
      "epoch: 2, batch: 851, loss: 0.44904521107673645\n",
      "epoch: 2, batch: 852, loss: 0.2312801033258438\n",
      "epoch: 2, batch: 853, loss: 0.32870906591415405\n",
      "epoch: 2, batch: 854, loss: 0.3009137511253357\n",
      "epoch: 2, batch: 855, loss: 0.09250439703464508\n",
      "epoch: 2, batch: 856, loss: 0.2866489887237549\n",
      "epoch: 2, batch: 857, loss: 0.3506952226161957\n",
      "epoch: 2, batch: 858, loss: 0.11407389491796494\n",
      "epoch: 2, batch: 859, loss: 0.2820347249507904\n",
      "epoch: 2, batch: 860, loss: 0.47285085916519165\n",
      "epoch: 2, batch: 861, loss: 0.36700934171676636\n",
      "epoch: 2, batch: 862, loss: 0.38315603137016296\n",
      "epoch: 2, batch: 863, loss: 0.2693007290363312\n",
      "epoch: 2, batch: 864, loss: 0.35754358768463135\n",
      "epoch: 2, batch: 865, loss: 0.16380366683006287\n",
      "epoch: 2, batch: 866, loss: 0.468473881483078\n",
      "epoch: 2, batch: 867, loss: 0.19141045212745667\n",
      "epoch: 2, batch: 868, loss: 0.39467987418174744\n",
      "epoch: 2, batch: 869, loss: 0.16765408217906952\n",
      "epoch: 2, batch: 870, loss: 0.1521359086036682\n",
      "epoch: 2, batch: 871, loss: 0.13629543781280518\n",
      "epoch: 2, batch: 872, loss: 0.19049414992332458\n",
      "epoch: 2, batch: 873, loss: 0.2734938859939575\n",
      "epoch: 2, batch: 874, loss: 0.1666434109210968\n",
      "epoch: 2, batch: 875, loss: 0.31932640075683594\n",
      "epoch: 2, batch: 876, loss: 0.44174280762672424\n",
      "epoch: 2, batch: 877, loss: 0.21991200745105743\n",
      "epoch: 2, batch: 878, loss: 0.2796715497970581\n",
      "epoch: 2, batch: 879, loss: 0.2915073037147522\n",
      "epoch: 2, batch: 880, loss: 0.32976803183555603\n",
      "epoch: 2, batch: 881, loss: 0.1733403503894806\n",
      "epoch: 2, batch: 882, loss: 0.3099666237831116\n",
      "epoch: 2, batch: 883, loss: 0.23073866963386536\n",
      "epoch: 2, batch: 884, loss: 0.4217767119407654\n",
      "epoch: 2, batch: 885, loss: 0.28027045726776123\n",
      "epoch: 2, batch: 886, loss: 0.27055519819259644\n",
      "epoch: 2, batch: 887, loss: 0.327116459608078\n",
      "epoch: 2, batch: 888, loss: 0.20565150678157806\n",
      "epoch: 2, batch: 889, loss: 0.26599588990211487\n",
      "epoch: 2, batch: 890, loss: 0.30927395820617676\n",
      "epoch: 2, batch: 891, loss: 0.31836938858032227\n",
      "epoch: 2, batch: 892, loss: 0.41103845834732056\n",
      "epoch: 2, batch: 893, loss: 0.17614081501960754\n",
      "epoch: 2, batch: 894, loss: 0.08790067583322525\n",
      "epoch: 2, batch: 895, loss: 0.39172056317329407\n",
      "epoch: 2, batch: 896, loss: 0.16008491814136505\n",
      "epoch: 2, batch: 897, loss: 0.5570505857467651\n",
      "epoch: 2, batch: 898, loss: 0.23411992192268372\n",
      "epoch: 2, batch: 899, loss: 0.41804730892181396\n",
      "epoch: 2, batch: 900, loss: 0.35468780994415283\n",
      "epoch: 2, batch: 901, loss: 0.23079319298267365\n",
      "epoch: 2, batch: 902, loss: 0.17358282208442688\n",
      "epoch: 2, batch: 903, loss: 0.3074245750904083\n",
      "epoch: 2, batch: 904, loss: 0.377236008644104\n",
      "epoch: 2, batch: 905, loss: 0.20915059745311737\n",
      "epoch: 2, batch: 906, loss: 0.23827365040779114\n",
      "epoch: 2, batch: 907, loss: 0.22919601202011108\n",
      "epoch: 2, batch: 908, loss: 0.3056813180446625\n",
      "epoch: 2, batch: 909, loss: 0.3165571987628937\n",
      "epoch: 2, batch: 910, loss: 0.20626820623874664\n",
      "epoch: 2, batch: 911, loss: 0.2520183026790619\n",
      "epoch: 2, batch: 912, loss: 0.224856436252594\n",
      "epoch: 2, batch: 913, loss: 0.3006836473941803\n",
      "epoch: 2, batch: 914, loss: 0.25042468309402466\n",
      "epoch: 2, batch: 915, loss: 0.3121527135372162\n",
      "epoch: 2, batch: 916, loss: 0.17913202941417694\n",
      "epoch: 2, batch: 917, loss: 0.31035229563713074\n",
      "epoch: 2, batch: 918, loss: 0.11578209698200226\n",
      "epoch: 2, batch: 919, loss: 0.48447471857070923\n",
      "epoch: 2, batch: 920, loss: 0.23736554384231567\n",
      "epoch: 2, batch: 921, loss: 0.10534362494945526\n",
      "epoch: 2, batch: 922, loss: 0.19384361803531647\n",
      "epoch: 2, batch: 923, loss: 0.26918718218803406\n",
      "epoch: 2, batch: 924, loss: 0.13477642834186554\n",
      "epoch: 2, batch: 925, loss: 0.2061450481414795\n",
      "epoch: 2, batch: 926, loss: 0.06689763814210892\n",
      "epoch: 2, batch: 927, loss: 0.3787252902984619\n",
      "epoch: 2, batch: 928, loss: 0.27657175064086914\n",
      "epoch: 2, batch: 929, loss: 0.22444681823253632\n",
      "epoch: 2, batch: 930, loss: 0.11459766328334808\n",
      "epoch: 2, batch: 931, loss: 0.5183135271072388\n",
      "epoch: 2, batch: 932, loss: 0.40510502457618713\n",
      "epoch: 2, batch: 933, loss: 0.1478503793478012\n",
      "epoch: 2, batch: 934, loss: 0.2771667540073395\n",
      "epoch: 2, batch: 935, loss: 0.2449166625738144\n",
      "epoch: 2, batch: 936, loss: 0.16279862821102142\n",
      "epoch: 2, batch: 937, loss: 0.14597809314727783\n",
      "epoch: 3, batch: 0, loss: 0.3308775722980499\n",
      "epoch: 3, batch: 1, loss: 0.34896451234817505\n",
      "epoch: 3, batch: 2, loss: 0.1363665759563446\n",
      "epoch: 3, batch: 3, loss: 0.171006977558136\n",
      "epoch: 3, batch: 4, loss: 0.2986043095588684\n",
      "epoch: 3, batch: 5, loss: 0.4347866475582123\n",
      "epoch: 3, batch: 6, loss: 0.28639331459999084\n",
      "epoch: 3, batch: 7, loss: 0.16453461349010468\n",
      "epoch: 3, batch: 8, loss: 0.2047184556722641\n",
      "epoch: 3, batch: 9, loss: 0.12812882661819458\n",
      "epoch: 3, batch: 10, loss: 0.2067800909280777\n",
      "epoch: 3, batch: 11, loss: 0.20680363476276398\n",
      "epoch: 3, batch: 12, loss: 0.18384245038032532\n",
      "epoch: 3, batch: 13, loss: 0.3572879731655121\n",
      "epoch: 3, batch: 14, loss: 0.44643062353134155\n",
      "epoch: 3, batch: 15, loss: 0.2724730372428894\n",
      "epoch: 3, batch: 16, loss: 0.2533760368824005\n",
      "epoch: 3, batch: 17, loss: 0.317880779504776\n",
      "epoch: 3, batch: 18, loss: 0.16540087759494781\n",
      "epoch: 3, batch: 19, loss: 0.17324228584766388\n",
      "epoch: 3, batch: 20, loss: 0.20499438047409058\n",
      "epoch: 3, batch: 21, loss: 0.3030347228050232\n",
      "epoch: 3, batch: 22, loss: 0.33371424674987793\n",
      "epoch: 3, batch: 23, loss: 0.2460240125656128\n",
      "epoch: 3, batch: 24, loss: 0.3498274087905884\n",
      "epoch: 3, batch: 25, loss: 0.2206612527370453\n",
      "epoch: 3, batch: 26, loss: 0.16893140971660614\n",
      "epoch: 3, batch: 27, loss: 0.25018322467803955\n",
      "epoch: 3, batch: 28, loss: 0.1704302430152893\n",
      "epoch: 3, batch: 29, loss: 0.1829972118139267\n",
      "epoch: 3, batch: 30, loss: 0.33963578939437866\n",
      "epoch: 3, batch: 31, loss: 0.21096891164779663\n",
      "epoch: 3, batch: 32, loss: 0.24477477371692657\n",
      "epoch: 3, batch: 33, loss: 0.31351691484451294\n",
      "epoch: 3, batch: 34, loss: 0.463520884513855\n",
      "epoch: 3, batch: 35, loss: 0.4813539385795593\n",
      "epoch: 3, batch: 36, loss: 0.35329651832580566\n",
      "epoch: 3, batch: 37, loss: 0.19484572112560272\n",
      "epoch: 3, batch: 38, loss: 0.2816668152809143\n",
      "epoch: 3, batch: 39, loss: 0.17323099076747894\n",
      "epoch: 3, batch: 40, loss: 0.4912450313568115\n",
      "epoch: 3, batch: 41, loss: 0.19680073857307434\n",
      "epoch: 3, batch: 42, loss: 0.09519685804843903\n",
      "epoch: 3, batch: 43, loss: 0.40714502334594727\n",
      "epoch: 3, batch: 44, loss: 0.12119535356760025\n",
      "epoch: 3, batch: 45, loss: 0.46533825993537903\n",
      "epoch: 3, batch: 46, loss: 0.17454415559768677\n",
      "epoch: 3, batch: 47, loss: 0.22843186557292938\n",
      "epoch: 3, batch: 48, loss: 0.1957448273897171\n",
      "epoch: 3, batch: 49, loss: 0.13519234955310822\n",
      "epoch: 3, batch: 50, loss: 0.31776389479637146\n",
      "epoch: 3, batch: 51, loss: 0.424502968788147\n",
      "epoch: 3, batch: 52, loss: 0.12027369439601898\n",
      "epoch: 3, batch: 53, loss: 0.1924707442522049\n",
      "epoch: 3, batch: 54, loss: 0.2997908592224121\n",
      "epoch: 3, batch: 55, loss: 0.22936561703681946\n",
      "epoch: 3, batch: 56, loss: 0.3429040312767029\n",
      "epoch: 3, batch: 57, loss: 0.1219460591673851\n",
      "epoch: 3, batch: 58, loss: 0.2315381020307541\n",
      "epoch: 3, batch: 59, loss: 0.15175673365592957\n",
      "epoch: 3, batch: 60, loss: 0.16692528128623962\n",
      "epoch: 3, batch: 61, loss: 0.37661808729171753\n",
      "epoch: 3, batch: 62, loss: 0.21977487206459045\n",
      "epoch: 3, batch: 63, loss: 0.1267596334218979\n",
      "epoch: 3, batch: 64, loss: 0.23963001370429993\n",
      "epoch: 3, batch: 65, loss: 0.21114884316921234\n",
      "epoch: 3, batch: 66, loss: 0.2765595018863678\n",
      "epoch: 3, batch: 67, loss: 0.1662210077047348\n",
      "epoch: 3, batch: 68, loss: 0.18451406061649323\n",
      "epoch: 3, batch: 69, loss: 0.21225953102111816\n",
      "epoch: 3, batch: 70, loss: 0.3089018762111664\n",
      "epoch: 3, batch: 71, loss: 0.35012543201446533\n",
      "epoch: 3, batch: 72, loss: 0.39639848470687866\n",
      "epoch: 3, batch: 73, loss: 0.32156920433044434\n",
      "epoch: 3, batch: 74, loss: 0.1802898794412613\n",
      "epoch: 3, batch: 75, loss: 0.25552505254745483\n",
      "epoch: 3, batch: 76, loss: 0.5791237950325012\n",
      "epoch: 3, batch: 77, loss: 0.16891704499721527\n",
      "epoch: 3, batch: 78, loss: 0.36568641662597656\n",
      "epoch: 3, batch: 79, loss: 0.24679523706436157\n",
      "epoch: 3, batch: 80, loss: 0.17304669320583344\n",
      "epoch: 3, batch: 81, loss: 0.2766614854335785\n",
      "epoch: 3, batch: 82, loss: 0.15662844479084015\n",
      "epoch: 3, batch: 83, loss: 0.12937119603157043\n",
      "epoch: 3, batch: 84, loss: 0.23374804854393005\n",
      "epoch: 3, batch: 85, loss: 0.3624190390110016\n",
      "epoch: 3, batch: 86, loss: 0.36382436752319336\n",
      "epoch: 3, batch: 87, loss: 0.21488772332668304\n",
      "epoch: 3, batch: 88, loss: 0.19974523782730103\n",
      "epoch: 3, batch: 89, loss: 0.31452417373657227\n",
      "epoch: 3, batch: 90, loss: 0.22628042101860046\n",
      "epoch: 3, batch: 91, loss: 0.21253392100334167\n",
      "epoch: 3, batch: 92, loss: 0.36905863881111145\n",
      "epoch: 3, batch: 93, loss: 0.12062258273363113\n",
      "epoch: 3, batch: 94, loss: 0.26421260833740234\n",
      "epoch: 3, batch: 95, loss: 0.30208683013916016\n",
      "epoch: 3, batch: 96, loss: 0.2701910734176636\n",
      "epoch: 3, batch: 97, loss: 0.0977494940161705\n",
      "epoch: 3, batch: 98, loss: 0.30723944306373596\n",
      "epoch: 3, batch: 99, loss: 0.271379292011261\n",
      "epoch: 3, batch: 100, loss: 0.16201019287109375\n",
      "epoch: 3, batch: 101, loss: 0.2519912123680115\n",
      "epoch: 3, batch: 102, loss: 0.4514421820640564\n",
      "epoch: 3, batch: 103, loss: 0.4322282373905182\n",
      "epoch: 3, batch: 104, loss: 0.20753690600395203\n",
      "epoch: 3, batch: 105, loss: 0.25961625576019287\n",
      "epoch: 3, batch: 106, loss: 0.20645593106746674\n",
      "epoch: 3, batch: 107, loss: 0.3281090259552002\n",
      "epoch: 3, batch: 108, loss: 0.13860690593719482\n",
      "epoch: 3, batch: 109, loss: 0.24090251326560974\n",
      "epoch: 3, batch: 110, loss: 0.29330912232398987\n",
      "epoch: 3, batch: 111, loss: 0.37498900294303894\n",
      "epoch: 3, batch: 112, loss: 0.47748109698295593\n",
      "epoch: 3, batch: 113, loss: 0.10567037761211395\n",
      "epoch: 3, batch: 114, loss: 0.3478851020336151\n",
      "epoch: 3, batch: 115, loss: 0.3274388313293457\n",
      "epoch: 3, batch: 116, loss: 0.5436381101608276\n",
      "epoch: 3, batch: 117, loss: 0.17772190272808075\n",
      "epoch: 3, batch: 118, loss: 0.2123633772134781\n",
      "epoch: 3, batch: 119, loss: 0.1699245721101761\n",
      "epoch: 3, batch: 120, loss: 0.365781307220459\n",
      "epoch: 3, batch: 121, loss: 0.08229701966047287\n",
      "epoch: 3, batch: 122, loss: 0.2725220024585724\n",
      "epoch: 3, batch: 123, loss: 0.3907099962234497\n",
      "epoch: 3, batch: 124, loss: 0.3722241222858429\n",
      "epoch: 3, batch: 125, loss: 0.24981564283370972\n",
      "epoch: 3, batch: 126, loss: 0.4114533066749573\n",
      "epoch: 3, batch: 127, loss: 0.2467208057641983\n",
      "epoch: 3, batch: 128, loss: 0.3453513979911804\n",
      "epoch: 3, batch: 129, loss: 0.1618129462003708\n",
      "epoch: 3, batch: 130, loss: 0.28448352217674255\n",
      "epoch: 3, batch: 131, loss: 0.20135243237018585\n",
      "epoch: 3, batch: 132, loss: 0.22674255073070526\n",
      "epoch: 3, batch: 133, loss: 0.3784375786781311\n",
      "epoch: 3, batch: 134, loss: 0.32177063822746277\n",
      "epoch: 3, batch: 135, loss: 0.2807941436767578\n",
      "epoch: 3, batch: 136, loss: 0.19333302974700928\n",
      "epoch: 3, batch: 137, loss: 0.2764815092086792\n",
      "epoch: 3, batch: 138, loss: 0.14020487666130066\n",
      "epoch: 3, batch: 139, loss: 0.2645138204097748\n",
      "epoch: 3, batch: 140, loss: 0.2525968551635742\n",
      "epoch: 3, batch: 141, loss: 0.39136213064193726\n",
      "epoch: 3, batch: 142, loss: 0.17938773334026337\n",
      "epoch: 3, batch: 143, loss: 0.26385098695755005\n",
      "epoch: 3, batch: 144, loss: 0.24574531614780426\n",
      "epoch: 3, batch: 145, loss: 0.1691356897354126\n",
      "epoch: 3, batch: 146, loss: 0.13549213111400604\n",
      "epoch: 3, batch: 147, loss: 0.3496568202972412\n",
      "epoch: 3, batch: 148, loss: 0.17331945896148682\n",
      "epoch: 3, batch: 149, loss: 0.1696665734052658\n",
      "epoch: 3, batch: 150, loss: 0.3411196768283844\n",
      "epoch: 3, batch: 151, loss: 0.32068029046058655\n",
      "epoch: 3, batch: 152, loss: 0.2385580986738205\n",
      "epoch: 3, batch: 153, loss: 0.27684012055397034\n",
      "epoch: 3, batch: 154, loss: 0.2677449584007263\n",
      "epoch: 3, batch: 155, loss: 0.2375827431678772\n",
      "epoch: 3, batch: 156, loss: 0.1256532073020935\n",
      "epoch: 3, batch: 157, loss: 0.2944604158401489\n",
      "epoch: 3, batch: 158, loss: 0.14096206426620483\n",
      "epoch: 3, batch: 159, loss: 0.31552478671073914\n",
      "epoch: 3, batch: 160, loss: 0.145535409450531\n",
      "epoch: 3, batch: 161, loss: 0.2526046335697174\n",
      "epoch: 3, batch: 162, loss: 0.2283920794725418\n",
      "epoch: 3, batch: 163, loss: 0.35345736145973206\n",
      "epoch: 3, batch: 164, loss: 0.22404633462429047\n",
      "epoch: 3, batch: 165, loss: 0.40001624822616577\n",
      "epoch: 3, batch: 166, loss: 0.3891180157661438\n",
      "epoch: 3, batch: 167, loss: 0.6166473627090454\n",
      "epoch: 3, batch: 168, loss: 0.3809332549571991\n",
      "epoch: 3, batch: 169, loss: 0.4490942656993866\n",
      "epoch: 3, batch: 170, loss: 0.16810186207294464\n",
      "epoch: 3, batch: 171, loss: 0.24311071634292603\n",
      "epoch: 3, batch: 172, loss: 0.15526603162288666\n",
      "epoch: 3, batch: 173, loss: 0.4017861783504486\n",
      "epoch: 3, batch: 174, loss: 0.24942605197429657\n",
      "epoch: 3, batch: 175, loss: 0.25826504826545715\n",
      "epoch: 3, batch: 176, loss: 0.372084379196167\n",
      "epoch: 3, batch: 177, loss: 0.17753954231739044\n",
      "epoch: 3, batch: 178, loss: 0.29859209060668945\n",
      "epoch: 3, batch: 179, loss: 0.15750263631343842\n",
      "epoch: 3, batch: 180, loss: 0.30969586968421936\n",
      "epoch: 3, batch: 181, loss: 0.36807578802108765\n",
      "epoch: 3, batch: 182, loss: 0.24147629737854004\n",
      "epoch: 3, batch: 183, loss: 0.1882854700088501\n",
      "epoch: 3, batch: 184, loss: 0.1667390912771225\n",
      "epoch: 3, batch: 185, loss: 0.17254674434661865\n",
      "epoch: 3, batch: 186, loss: 0.1582360565662384\n",
      "epoch: 3, batch: 187, loss: 0.2651117146015167\n",
      "epoch: 3, batch: 188, loss: 0.20625831186771393\n",
      "epoch: 3, batch: 189, loss: 0.33726876974105835\n",
      "epoch: 3, batch: 190, loss: 0.2503608465194702\n",
      "epoch: 3, batch: 191, loss: 0.19127725064754486\n",
      "epoch: 3, batch: 192, loss: 0.26276344060897827\n",
      "epoch: 3, batch: 193, loss: 0.2523539066314697\n",
      "epoch: 3, batch: 194, loss: 0.17397652566432953\n",
      "epoch: 3, batch: 195, loss: 0.2278003841638565\n",
      "epoch: 3, batch: 196, loss: 0.16310761868953705\n",
      "epoch: 3, batch: 197, loss: 0.37900087237358093\n",
      "epoch: 3, batch: 198, loss: 0.19049793481826782\n",
      "epoch: 3, batch: 199, loss: 0.10363849252462387\n",
      "epoch: 3, batch: 200, loss: 0.2208065241575241\n",
      "epoch: 3, batch: 201, loss: 0.21726365387439728\n",
      "epoch: 3, batch: 202, loss: 0.23168405890464783\n",
      "epoch: 3, batch: 203, loss: 0.26169678568840027\n",
      "epoch: 3, batch: 204, loss: 0.3593781590461731\n",
      "epoch: 3, batch: 205, loss: 0.2916112244129181\n",
      "epoch: 3, batch: 206, loss: 0.3519403040409088\n",
      "epoch: 3, batch: 207, loss: 0.21672983467578888\n",
      "epoch: 3, batch: 208, loss: 0.06604405492544174\n",
      "epoch: 3, batch: 209, loss: 0.23728112876415253\n",
      "epoch: 3, batch: 210, loss: 0.25654006004333496\n",
      "epoch: 3, batch: 211, loss: 0.2507067918777466\n",
      "epoch: 3, batch: 212, loss: 0.39227065443992615\n",
      "epoch: 3, batch: 213, loss: 0.3245115578174591\n",
      "epoch: 3, batch: 214, loss: 0.12724760174751282\n",
      "epoch: 3, batch: 215, loss: 0.15058031678199768\n",
      "epoch: 3, batch: 216, loss: 0.26829344034194946\n",
      "epoch: 3, batch: 217, loss: 0.20807483792304993\n",
      "epoch: 3, batch: 218, loss: 0.29548075795173645\n",
      "epoch: 3, batch: 219, loss: 0.16953077912330627\n",
      "epoch: 3, batch: 220, loss: 0.3903873562812805\n",
      "epoch: 3, batch: 221, loss: 0.2404029369354248\n",
      "epoch: 3, batch: 222, loss: 0.2205311357975006\n",
      "epoch: 3, batch: 223, loss: 0.4108562171459198\n",
      "epoch: 3, batch: 224, loss: 0.20955480635166168\n",
      "epoch: 3, batch: 225, loss: 0.3785587549209595\n",
      "epoch: 3, batch: 226, loss: 0.2837269604206085\n",
      "epoch: 3, batch: 227, loss: 0.22299142181873322\n",
      "epoch: 3, batch: 228, loss: 0.12230534851551056\n",
      "epoch: 3, batch: 229, loss: 0.31440311670303345\n",
      "epoch: 3, batch: 230, loss: 0.353884756565094\n",
      "epoch: 3, batch: 231, loss: 0.1483694463968277\n",
      "epoch: 3, batch: 232, loss: 0.39938098192214966\n",
      "epoch: 3, batch: 233, loss: 0.3545795977115631\n",
      "epoch: 3, batch: 234, loss: 0.2507801055908203\n",
      "epoch: 3, batch: 235, loss: 0.29171183705329895\n",
      "epoch: 3, batch: 236, loss: 0.2261025309562683\n",
      "epoch: 3, batch: 237, loss: 0.3774147927761078\n",
      "epoch: 3, batch: 238, loss: 0.186919167637825\n",
      "epoch: 3, batch: 239, loss: 0.19233126938343048\n",
      "epoch: 3, batch: 240, loss: 0.15555228292942047\n",
      "epoch: 3, batch: 241, loss: 0.34690171480178833\n",
      "epoch: 3, batch: 242, loss: 0.16832906007766724\n",
      "epoch: 3, batch: 243, loss: 0.24883529543876648\n",
      "epoch: 3, batch: 244, loss: 0.21999427676200867\n",
      "epoch: 3, batch: 245, loss: 0.2892276644706726\n",
      "epoch: 3, batch: 246, loss: 0.17296087741851807\n",
      "epoch: 3, batch: 247, loss: 0.13121125102043152\n",
      "epoch: 3, batch: 248, loss: 0.17396610975265503\n",
      "epoch: 3, batch: 249, loss: 0.22584570944309235\n",
      "epoch: 3, batch: 250, loss: 0.22602565586566925\n",
      "epoch: 3, batch: 251, loss: 0.13739536702632904\n",
      "epoch: 3, batch: 252, loss: 0.2488383650779724\n",
      "epoch: 3, batch: 253, loss: 0.21378105878829956\n",
      "epoch: 3, batch: 254, loss: 0.4753718376159668\n",
      "epoch: 3, batch: 255, loss: 0.24185603857040405\n",
      "epoch: 3, batch: 256, loss: 0.2731058895587921\n",
      "epoch: 3, batch: 257, loss: 0.25072529911994934\n",
      "epoch: 3, batch: 258, loss: 0.169500932097435\n",
      "epoch: 3, batch: 259, loss: 0.23233622312545776\n",
      "epoch: 3, batch: 260, loss: 0.26667866110801697\n",
      "epoch: 3, batch: 261, loss: 0.1496007740497589\n",
      "epoch: 3, batch: 262, loss: 0.2016606330871582\n",
      "epoch: 3, batch: 263, loss: 0.3169199228286743\n",
      "epoch: 3, batch: 264, loss: 0.36093196272850037\n",
      "epoch: 3, batch: 265, loss: 0.4544478952884674\n",
      "epoch: 3, batch: 266, loss: 0.3316684663295746\n",
      "epoch: 3, batch: 267, loss: 0.2668243646621704\n",
      "epoch: 3, batch: 268, loss: 0.08957314491271973\n",
      "epoch: 3, batch: 269, loss: 0.47521206736564636\n",
      "epoch: 3, batch: 270, loss: 0.33703380823135376\n",
      "epoch: 3, batch: 271, loss: 0.08230975270271301\n",
      "epoch: 3, batch: 272, loss: 0.17685934901237488\n",
      "epoch: 3, batch: 273, loss: 0.23891180753707886\n",
      "epoch: 3, batch: 274, loss: 0.2459261417388916\n",
      "epoch: 3, batch: 275, loss: 0.25458967685699463\n",
      "epoch: 3, batch: 276, loss: 0.44827011227607727\n",
      "epoch: 3, batch: 277, loss: 0.32024717330932617\n",
      "epoch: 3, batch: 278, loss: 0.1449577659368515\n",
      "epoch: 3, batch: 279, loss: 0.4032078981399536\n",
      "epoch: 3, batch: 280, loss: 0.27234116196632385\n",
      "epoch: 3, batch: 281, loss: 0.3091619908809662\n",
      "epoch: 3, batch: 282, loss: 0.1353645920753479\n",
      "epoch: 3, batch: 283, loss: 0.447597861289978\n",
      "epoch: 3, batch: 284, loss: 0.21335597336292267\n",
      "epoch: 3, batch: 285, loss: 0.1679292917251587\n",
      "epoch: 3, batch: 286, loss: 0.18243294954299927\n",
      "epoch: 3, batch: 287, loss: 0.15846052765846252\n",
      "epoch: 3, batch: 288, loss: 0.2019014209508896\n",
      "epoch: 3, batch: 289, loss: 0.19816753268241882\n",
      "epoch: 3, batch: 290, loss: 0.31536340713500977\n",
      "epoch: 3, batch: 291, loss: 0.26910150051116943\n",
      "epoch: 3, batch: 292, loss: 0.1600053757429123\n",
      "epoch: 3, batch: 293, loss: 0.09171295166015625\n",
      "epoch: 3, batch: 294, loss: 0.20758193731307983\n",
      "epoch: 3, batch: 295, loss: 0.27149778604507446\n",
      "epoch: 3, batch: 296, loss: 0.3692953884601593\n",
      "epoch: 3, batch: 297, loss: 0.1658879667520523\n",
      "epoch: 3, batch: 298, loss: 0.07965860515832901\n",
      "epoch: 3, batch: 299, loss: 0.17132920026779175\n",
      "epoch: 3, batch: 300, loss: 0.40807950496673584\n",
      "epoch: 3, batch: 301, loss: 0.21758796274662018\n",
      "epoch: 3, batch: 302, loss: 0.09826838970184326\n",
      "epoch: 3, batch: 303, loss: 0.2715587615966797\n",
      "epoch: 3, batch: 304, loss: 0.16565550863742828\n",
      "epoch: 3, batch: 305, loss: 0.4855979084968567\n",
      "epoch: 3, batch: 306, loss: 0.197807177901268\n",
      "epoch: 3, batch: 307, loss: 0.19953498244285583\n",
      "epoch: 3, batch: 308, loss: 0.1892635077238083\n",
      "epoch: 3, batch: 309, loss: 0.26519057154655457\n",
      "epoch: 3, batch: 310, loss: 0.3693307638168335\n",
      "epoch: 3, batch: 311, loss: 0.19257311522960663\n",
      "epoch: 3, batch: 312, loss: 0.2738410532474518\n",
      "epoch: 3, batch: 313, loss: 0.256817102432251\n",
      "epoch: 3, batch: 314, loss: 0.21205443143844604\n",
      "epoch: 3, batch: 315, loss: 0.2565617859363556\n",
      "epoch: 3, batch: 316, loss: 0.41075849533081055\n",
      "epoch: 3, batch: 317, loss: 0.32166096568107605\n",
      "epoch: 3, batch: 318, loss: 0.1766463816165924\n",
      "epoch: 3, batch: 319, loss: 0.18509823083877563\n",
      "epoch: 3, batch: 320, loss: 0.19436988234519958\n",
      "epoch: 3, batch: 321, loss: 0.3986071050167084\n",
      "epoch: 3, batch: 322, loss: 0.3037310838699341\n",
      "epoch: 3, batch: 323, loss: 0.189237579703331\n",
      "epoch: 3, batch: 324, loss: 0.3225870430469513\n",
      "epoch: 3, batch: 325, loss: 0.16038374602794647\n",
      "epoch: 3, batch: 326, loss: 0.12437150627374649\n",
      "epoch: 3, batch: 327, loss: 0.314205527305603\n",
      "epoch: 3, batch: 328, loss: 0.1209336370229721\n",
      "epoch: 3, batch: 329, loss: 0.2191842645406723\n",
      "epoch: 3, batch: 330, loss: 0.1616673469543457\n",
      "epoch: 3, batch: 331, loss: 0.2501521706581116\n",
      "epoch: 3, batch: 332, loss: 0.2481405884027481\n",
      "epoch: 3, batch: 333, loss: 0.40270471572875977\n",
      "epoch: 3, batch: 334, loss: 0.30547717213630676\n",
      "epoch: 3, batch: 335, loss: 0.16558684408664703\n",
      "epoch: 3, batch: 336, loss: 0.3736979365348816\n",
      "epoch: 3, batch: 337, loss: 0.2469690442085266\n",
      "epoch: 3, batch: 338, loss: 0.22468747198581696\n",
      "epoch: 3, batch: 339, loss: 0.2815217971801758\n",
      "epoch: 3, batch: 340, loss: 0.1372588574886322\n",
      "epoch: 3, batch: 341, loss: 0.3064230680465698\n",
      "epoch: 3, batch: 342, loss: 0.2652444839477539\n",
      "epoch: 3, batch: 343, loss: 0.2744918763637543\n",
      "epoch: 3, batch: 344, loss: 0.28451693058013916\n",
      "epoch: 3, batch: 345, loss: 0.372071772813797\n",
      "epoch: 3, batch: 346, loss: 0.31468501687049866\n",
      "epoch: 3, batch: 347, loss: 0.38941481709480286\n",
      "epoch: 3, batch: 348, loss: 0.2562113404273987\n",
      "epoch: 3, batch: 349, loss: 0.23765972256660461\n",
      "epoch: 3, batch: 350, loss: 0.2715708315372467\n",
      "epoch: 3, batch: 351, loss: 0.38991108536720276\n",
      "epoch: 3, batch: 352, loss: 0.23490440845489502\n",
      "epoch: 3, batch: 353, loss: 0.15346412360668182\n",
      "epoch: 3, batch: 354, loss: 0.15440542995929718\n",
      "epoch: 3, batch: 355, loss: 0.28513652086257935\n",
      "epoch: 3, batch: 356, loss: 0.3621460497379303\n",
      "epoch: 3, batch: 357, loss: 0.3089751899242401\n",
      "epoch: 3, batch: 358, loss: 0.15692493319511414\n",
      "epoch: 3, batch: 359, loss: 0.20332665741443634\n",
      "epoch: 3, batch: 360, loss: 0.21832965314388275\n",
      "epoch: 3, batch: 361, loss: 0.2743503451347351\n",
      "epoch: 3, batch: 362, loss: 0.310018926858902\n",
      "epoch: 3, batch: 363, loss: 0.16665112972259521\n",
      "epoch: 3, batch: 364, loss: 0.17615588009357452\n",
      "epoch: 3, batch: 365, loss: 0.2889106273651123\n",
      "epoch: 3, batch: 366, loss: 0.3402736485004425\n",
      "epoch: 3, batch: 367, loss: 0.112313412129879\n",
      "epoch: 3, batch: 368, loss: 0.3366938531398773\n",
      "epoch: 3, batch: 369, loss: 0.1600310057401657\n",
      "epoch: 3, batch: 370, loss: 0.13755303621292114\n",
      "epoch: 3, batch: 371, loss: 0.33530929684638977\n",
      "epoch: 3, batch: 372, loss: 0.23065753281116486\n",
      "epoch: 3, batch: 373, loss: 0.2646414041519165\n",
      "epoch: 3, batch: 374, loss: 0.1758134365081787\n",
      "epoch: 3, batch: 375, loss: 0.3860919773578644\n",
      "epoch: 3, batch: 376, loss: 0.17259271442890167\n",
      "epoch: 3, batch: 377, loss: 0.1757429987192154\n",
      "epoch: 3, batch: 378, loss: 0.12489401549100876\n",
      "epoch: 3, batch: 379, loss: 0.13882356882095337\n",
      "epoch: 3, batch: 380, loss: 0.18490362167358398\n",
      "epoch: 3, batch: 381, loss: 0.35393020510673523\n",
      "epoch: 3, batch: 382, loss: 0.31803345680236816\n",
      "epoch: 3, batch: 383, loss: 0.15881069004535675\n",
      "epoch: 3, batch: 384, loss: 0.28478139638900757\n",
      "epoch: 3, batch: 385, loss: 0.24869994819164276\n",
      "epoch: 3, batch: 386, loss: 0.32005542516708374\n",
      "epoch: 3, batch: 387, loss: 0.16753479838371277\n",
      "epoch: 3, batch: 388, loss: 0.24189789593219757\n",
      "epoch: 3, batch: 389, loss: 0.17602978646755219\n",
      "epoch: 3, batch: 390, loss: 0.16434435546398163\n",
      "epoch: 3, batch: 391, loss: 0.12895964086055756\n",
      "epoch: 3, batch: 392, loss: 0.1526165008544922\n",
      "epoch: 3, batch: 393, loss: 0.19416071474552155\n",
      "epoch: 3, batch: 394, loss: 0.3197436034679413\n",
      "epoch: 3, batch: 395, loss: 0.4403603672981262\n",
      "epoch: 3, batch: 396, loss: 0.1508624404668808\n",
      "epoch: 3, batch: 397, loss: 0.25675496459007263\n",
      "epoch: 3, batch: 398, loss: 0.23991496860980988\n",
      "epoch: 3, batch: 399, loss: 0.3055170178413391\n",
      "epoch: 3, batch: 400, loss: 0.28338006138801575\n",
      "epoch: 3, batch: 401, loss: 0.2795761227607727\n",
      "epoch: 3, batch: 402, loss: 0.3886151611804962\n",
      "epoch: 3, batch: 403, loss: 0.08537006378173828\n",
      "epoch: 3, batch: 404, loss: 0.2302348017692566\n",
      "epoch: 3, batch: 405, loss: 0.24533534049987793\n",
      "epoch: 3, batch: 406, loss: 0.08795483410358429\n",
      "epoch: 3, batch: 407, loss: 0.34891483187675476\n",
      "epoch: 3, batch: 408, loss: 0.28491148352622986\n",
      "epoch: 3, batch: 409, loss: 0.3200641870498657\n",
      "epoch: 3, batch: 410, loss: 0.1210024282336235\n",
      "epoch: 3, batch: 411, loss: 0.12545576691627502\n",
      "epoch: 3, batch: 412, loss: 0.3099823296070099\n",
      "epoch: 3, batch: 413, loss: 0.2005164921283722\n",
      "epoch: 3, batch: 414, loss: 0.23044763505458832\n",
      "epoch: 3, batch: 415, loss: 0.1638944298028946\n",
      "epoch: 3, batch: 416, loss: 0.37334731221199036\n",
      "epoch: 3, batch: 417, loss: 0.11228685826063156\n",
      "epoch: 3, batch: 418, loss: 0.1238269954919815\n",
      "epoch: 3, batch: 419, loss: 0.21637551486492157\n",
      "epoch: 3, batch: 420, loss: 0.12044380605220795\n",
      "epoch: 3, batch: 421, loss: 0.20627467334270477\n",
      "epoch: 3, batch: 422, loss: 0.2521686851978302\n",
      "epoch: 3, batch: 423, loss: 0.26018545031547546\n",
      "epoch: 3, batch: 424, loss: 0.2266799360513687\n",
      "epoch: 3, batch: 425, loss: 0.17075414955615997\n",
      "epoch: 3, batch: 426, loss: 0.275177925825119\n",
      "epoch: 3, batch: 427, loss: 0.384818971157074\n",
      "epoch: 3, batch: 428, loss: 0.3428703248500824\n",
      "epoch: 3, batch: 429, loss: 0.45725569128990173\n",
      "epoch: 3, batch: 430, loss: 0.2796829342842102\n",
      "epoch: 3, batch: 431, loss: 0.13846035301685333\n",
      "epoch: 3, batch: 432, loss: 0.2901782691478729\n",
      "epoch: 3, batch: 433, loss: 0.2888646721839905\n",
      "epoch: 3, batch: 434, loss: 0.2846299707889557\n",
      "epoch: 3, batch: 435, loss: 0.22233755886554718\n",
      "epoch: 3, batch: 436, loss: 0.2936307489871979\n",
      "epoch: 3, batch: 437, loss: 0.1599053144454956\n",
      "epoch: 3, batch: 438, loss: 0.21921883523464203\n",
      "epoch: 3, batch: 439, loss: 0.20620979368686676\n",
      "epoch: 3, batch: 440, loss: 0.2315913289785385\n",
      "epoch: 3, batch: 441, loss: 0.19467578828334808\n",
      "epoch: 3, batch: 442, loss: 0.32683682441711426\n",
      "epoch: 3, batch: 443, loss: 0.13756869733333588\n",
      "epoch: 3, batch: 444, loss: 0.2291639894247055\n",
      "epoch: 3, batch: 445, loss: 0.29385802149772644\n",
      "epoch: 3, batch: 446, loss: 0.23301151394844055\n",
      "epoch: 3, batch: 447, loss: 0.150248184800148\n",
      "epoch: 3, batch: 448, loss: 0.21799057722091675\n",
      "epoch: 3, batch: 449, loss: 0.11621332168579102\n",
      "epoch: 3, batch: 450, loss: 0.1309274286031723\n",
      "epoch: 3, batch: 451, loss: 0.2509106397628784\n",
      "epoch: 3, batch: 452, loss: 0.28751140832901\n",
      "epoch: 3, batch: 453, loss: 0.2756563723087311\n",
      "epoch: 3, batch: 454, loss: 0.11799059063196182\n",
      "epoch: 3, batch: 455, loss: 0.31044134497642517\n",
      "epoch: 3, batch: 456, loss: 0.26936447620391846\n",
      "epoch: 3, batch: 457, loss: 0.32541102170944214\n",
      "epoch: 3, batch: 458, loss: 0.4133758842945099\n",
      "epoch: 3, batch: 459, loss: 0.25293058156967163\n",
      "epoch: 3, batch: 460, loss: 0.3533519506454468\n",
      "epoch: 3, batch: 461, loss: 0.14073361456394196\n",
      "epoch: 3, batch: 462, loss: 0.26673105359077454\n",
      "epoch: 3, batch: 463, loss: 0.12217482179403305\n",
      "epoch: 3, batch: 464, loss: 0.21727313101291656\n",
      "epoch: 3, batch: 465, loss: 0.13587632775306702\n",
      "epoch: 3, batch: 466, loss: 0.24672925472259521\n",
      "epoch: 3, batch: 467, loss: 0.23396389186382294\n",
      "epoch: 3, batch: 468, loss: 0.11587986350059509\n",
      "epoch: 3, batch: 469, loss: 0.20288771390914917\n",
      "epoch: 3, batch: 470, loss: 0.21292653679847717\n",
      "epoch: 3, batch: 471, loss: 0.12739044427871704\n",
      "epoch: 3, batch: 472, loss: 0.3247283697128296\n",
      "epoch: 3, batch: 473, loss: 0.14607463777065277\n",
      "epoch: 3, batch: 474, loss: 0.2133391946554184\n",
      "epoch: 3, batch: 475, loss: 0.1990898847579956\n",
      "epoch: 3, batch: 476, loss: 0.3334764838218689\n",
      "epoch: 3, batch: 477, loss: 0.11763899773359299\n",
      "epoch: 3, batch: 478, loss: 0.11904816329479218\n",
      "epoch: 3, batch: 479, loss: 0.3276696503162384\n",
      "epoch: 3, batch: 480, loss: 0.1338960975408554\n",
      "epoch: 3, batch: 481, loss: 0.26977840065956116\n",
      "epoch: 3, batch: 482, loss: 0.16424041986465454\n",
      "epoch: 3, batch: 483, loss: 0.16263875365257263\n",
      "epoch: 3, batch: 484, loss: 0.3268354833126068\n",
      "epoch: 3, batch: 485, loss: 0.19255009293556213\n",
      "epoch: 3, batch: 486, loss: 0.28053465485572815\n",
      "epoch: 3, batch: 487, loss: 0.0922127291560173\n",
      "epoch: 3, batch: 488, loss: 0.34645718336105347\n",
      "epoch: 3, batch: 489, loss: 0.0729360356926918\n",
      "epoch: 3, batch: 490, loss: 0.10275405645370483\n",
      "epoch: 3, batch: 491, loss: 0.24584008753299713\n",
      "epoch: 3, batch: 492, loss: 0.1879376471042633\n",
      "epoch: 3, batch: 493, loss: 0.1776961088180542\n",
      "epoch: 3, batch: 494, loss: 0.3052263855934143\n",
      "epoch: 3, batch: 495, loss: 0.29767221212387085\n",
      "epoch: 3, batch: 496, loss: 0.15237019956111908\n",
      "epoch: 3, batch: 497, loss: 0.06175345927476883\n",
      "epoch: 3, batch: 498, loss: 0.340077668428421\n",
      "epoch: 3, batch: 499, loss: 0.13403941690921783\n",
      "epoch: 3, batch: 500, loss: 0.279620885848999\n",
      "epoch: 3, batch: 501, loss: 0.16738055646419525\n",
      "epoch: 3, batch: 502, loss: 0.48173728585243225\n",
      "epoch: 3, batch: 503, loss: 0.1814933717250824\n",
      "epoch: 3, batch: 504, loss: 0.4500274062156677\n",
      "epoch: 3, batch: 505, loss: 0.31309399008750916\n",
      "epoch: 3, batch: 506, loss: 0.15420681238174438\n",
      "epoch: 3, batch: 507, loss: 0.2515796720981598\n",
      "epoch: 3, batch: 508, loss: 0.48182767629623413\n",
      "epoch: 3, batch: 509, loss: 0.3211216926574707\n",
      "epoch: 3, batch: 510, loss: 0.29114845395088196\n",
      "epoch: 3, batch: 511, loss: 0.2775162160396576\n",
      "epoch: 3, batch: 512, loss: 0.2797253727912903\n",
      "epoch: 3, batch: 513, loss: 0.13330800831317902\n",
      "epoch: 3, batch: 514, loss: 0.3553502857685089\n",
      "epoch: 3, batch: 515, loss: 0.4508034586906433\n",
      "epoch: 3, batch: 516, loss: 0.1504397988319397\n",
      "epoch: 3, batch: 517, loss: 0.41923394799232483\n",
      "epoch: 3, batch: 518, loss: 0.310106486082077\n",
      "epoch: 3, batch: 519, loss: 0.1618761420249939\n",
      "epoch: 3, batch: 520, loss: 0.2898949980735779\n",
      "epoch: 3, batch: 521, loss: 0.3399522006511688\n",
      "epoch: 3, batch: 522, loss: 0.15667909383773804\n",
      "epoch: 3, batch: 523, loss: 0.1797245889902115\n",
      "epoch: 3, batch: 524, loss: 0.1510273814201355\n",
      "epoch: 3, batch: 525, loss: 0.21330054104328156\n",
      "epoch: 3, batch: 526, loss: 0.32368549704551697\n",
      "epoch: 3, batch: 527, loss: 0.21816012263298035\n",
      "epoch: 3, batch: 528, loss: 0.24407561123371124\n",
      "epoch: 3, batch: 529, loss: 0.20670095086097717\n",
      "epoch: 3, batch: 530, loss: 0.16873140633106232\n",
      "epoch: 3, batch: 531, loss: 0.17760208249092102\n",
      "epoch: 3, batch: 532, loss: 0.21272937953472137\n",
      "epoch: 3, batch: 533, loss: 0.3054341971874237\n",
      "epoch: 3, batch: 534, loss: 0.2780781686306\n",
      "epoch: 3, batch: 535, loss: 0.07728618383407593\n",
      "epoch: 3, batch: 536, loss: 0.16886021196842194\n",
      "epoch: 3, batch: 537, loss: 0.15748625993728638\n",
      "epoch: 3, batch: 538, loss: 0.11658592522144318\n",
      "epoch: 3, batch: 539, loss: 0.2753787636756897\n",
      "epoch: 3, batch: 540, loss: 0.21556508541107178\n",
      "epoch: 3, batch: 541, loss: 0.12004247307777405\n",
      "epoch: 3, batch: 542, loss: 0.1398213654756546\n",
      "epoch: 3, batch: 543, loss: 0.18473513424396515\n",
      "epoch: 3, batch: 544, loss: 0.2058430165052414\n",
      "epoch: 3, batch: 545, loss: 0.2919667959213257\n",
      "epoch: 3, batch: 546, loss: 0.19081518054008484\n",
      "epoch: 3, batch: 547, loss: 0.2818621098995209\n",
      "epoch: 3, batch: 548, loss: 0.24185150861740112\n",
      "epoch: 3, batch: 549, loss: 0.19957582652568817\n",
      "epoch: 3, batch: 550, loss: 0.27038708329200745\n",
      "epoch: 3, batch: 551, loss: 0.2169649749994278\n",
      "epoch: 3, batch: 552, loss: 0.07616735994815826\n",
      "epoch: 3, batch: 553, loss: 0.10309427976608276\n",
      "epoch: 3, batch: 554, loss: 0.18911390006542206\n",
      "epoch: 3, batch: 555, loss: 0.2315962165594101\n",
      "epoch: 3, batch: 556, loss: 0.3698570430278778\n",
      "epoch: 3, batch: 557, loss: 0.2493126094341278\n",
      "epoch: 3, batch: 558, loss: 0.16955240070819855\n",
      "epoch: 3, batch: 559, loss: 0.2633729577064514\n",
      "epoch: 3, batch: 560, loss: 0.2209920734167099\n",
      "epoch: 3, batch: 561, loss: 0.32161974906921387\n",
      "epoch: 3, batch: 562, loss: 0.11242802441120148\n",
      "epoch: 3, batch: 563, loss: 0.0844344049692154\n",
      "epoch: 3, batch: 564, loss: 0.1938130259513855\n",
      "epoch: 3, batch: 565, loss: 0.1555168479681015\n",
      "epoch: 3, batch: 566, loss: 0.1908101737499237\n",
      "epoch: 3, batch: 567, loss: 0.20112474262714386\n",
      "epoch: 3, batch: 568, loss: 0.08173341304063797\n",
      "epoch: 3, batch: 569, loss: 0.2750844359397888\n",
      "epoch: 3, batch: 570, loss: 0.26818084716796875\n",
      "epoch: 3, batch: 571, loss: 0.22743694484233856\n",
      "epoch: 3, batch: 572, loss: 0.2047964483499527\n",
      "epoch: 3, batch: 573, loss: 0.26893535256385803\n",
      "epoch: 3, batch: 574, loss: 0.3213679790496826\n",
      "epoch: 3, batch: 575, loss: 0.10844402015209198\n",
      "epoch: 3, batch: 576, loss: 0.43149659037590027\n",
      "epoch: 3, batch: 577, loss: 0.32208240032196045\n",
      "epoch: 3, batch: 578, loss: 0.33421802520751953\n",
      "epoch: 3, batch: 579, loss: 0.2074575424194336\n",
      "epoch: 3, batch: 580, loss: 0.2499660700559616\n",
      "epoch: 3, batch: 581, loss: 0.270636647939682\n",
      "epoch: 3, batch: 582, loss: 0.14619499444961548\n",
      "epoch: 3, batch: 583, loss: 0.1594264954328537\n",
      "epoch: 3, batch: 584, loss: 0.13010165095329285\n",
      "epoch: 3, batch: 585, loss: 0.37064552307128906\n",
      "epoch: 3, batch: 586, loss: 0.16525281965732574\n",
      "epoch: 3, batch: 587, loss: 0.22780852019786835\n",
      "epoch: 3, batch: 588, loss: 0.3927576243877411\n",
      "epoch: 3, batch: 589, loss: 0.19610220193862915\n",
      "epoch: 3, batch: 590, loss: 0.0724756270647049\n",
      "epoch: 3, batch: 591, loss: 0.2119547575712204\n",
      "epoch: 3, batch: 592, loss: 0.2243470847606659\n",
      "epoch: 3, batch: 593, loss: 0.23546060919761658\n",
      "epoch: 3, batch: 594, loss: 0.1561340093612671\n",
      "epoch: 3, batch: 595, loss: 0.18260458111763\n",
      "epoch: 3, batch: 596, loss: 0.18047091364860535\n",
      "epoch: 3, batch: 597, loss: 0.20668303966522217\n",
      "epoch: 3, batch: 598, loss: 0.2067742645740509\n",
      "epoch: 3, batch: 599, loss: 0.19157849252223969\n",
      "epoch: 3, batch: 600, loss: 0.4285215139389038\n",
      "epoch: 3, batch: 601, loss: 0.2860158681869507\n",
      "epoch: 3, batch: 602, loss: 0.29224395751953125\n",
      "epoch: 3, batch: 603, loss: 0.2856997847557068\n",
      "epoch: 3, batch: 604, loss: 0.12911280989646912\n",
      "epoch: 3, batch: 605, loss: 0.22722958028316498\n",
      "epoch: 3, batch: 606, loss: 0.15119145810604095\n",
      "epoch: 3, batch: 607, loss: 0.10902390629053116\n",
      "epoch: 3, batch: 608, loss: 0.2080221176147461\n",
      "epoch: 3, batch: 609, loss: 0.11469841003417969\n",
      "epoch: 3, batch: 610, loss: 0.5650844573974609\n",
      "epoch: 3, batch: 611, loss: 0.08587634563446045\n",
      "epoch: 3, batch: 612, loss: 0.34627583622932434\n",
      "epoch: 3, batch: 613, loss: 0.2445821762084961\n",
      "epoch: 3, batch: 614, loss: 0.18577931821346283\n",
      "epoch: 3, batch: 615, loss: 0.2995157539844513\n",
      "epoch: 3, batch: 616, loss: 0.2640419602394104\n",
      "epoch: 3, batch: 617, loss: 0.17285758256912231\n",
      "epoch: 3, batch: 618, loss: 0.2509136497974396\n",
      "epoch: 3, batch: 619, loss: 0.3462713956832886\n",
      "epoch: 3, batch: 620, loss: 0.2219175398349762\n",
      "epoch: 3, batch: 621, loss: 0.21352991461753845\n",
      "epoch: 3, batch: 622, loss: 0.2024117261171341\n",
      "epoch: 3, batch: 623, loss: 0.15332239866256714\n",
      "epoch: 3, batch: 624, loss: 0.3798407018184662\n",
      "epoch: 3, batch: 625, loss: 0.09422720223665237\n",
      "epoch: 3, batch: 626, loss: 0.243853360414505\n",
      "epoch: 3, batch: 627, loss: 0.3919215202331543\n",
      "epoch: 3, batch: 628, loss: 0.16697220504283905\n",
      "epoch: 3, batch: 629, loss: 0.30249476432800293\n",
      "epoch: 3, batch: 630, loss: 0.2675294578075409\n",
      "epoch: 3, batch: 631, loss: 0.27005326747894287\n",
      "epoch: 3, batch: 632, loss: 0.3758143484592438\n",
      "epoch: 3, batch: 633, loss: 0.35886314511299133\n",
      "epoch: 3, batch: 634, loss: 0.13883212208747864\n",
      "epoch: 3, batch: 635, loss: 0.26266855001449585\n",
      "epoch: 3, batch: 636, loss: 0.181266650557518\n",
      "epoch: 3, batch: 637, loss: 0.22732335329055786\n",
      "epoch: 3, batch: 638, loss: 0.1788373440504074\n",
      "epoch: 3, batch: 639, loss: 0.11255346238613129\n",
      "epoch: 3, batch: 640, loss: 0.2138785719871521\n",
      "epoch: 3, batch: 641, loss: 0.2045956552028656\n",
      "epoch: 3, batch: 642, loss: 0.3019588589668274\n",
      "epoch: 3, batch: 643, loss: 0.41643527150154114\n",
      "epoch: 3, batch: 644, loss: 0.28597602248191833\n",
      "epoch: 3, batch: 645, loss: 0.16003939509391785\n",
      "epoch: 3, batch: 646, loss: 0.2600853741168976\n",
      "epoch: 3, batch: 647, loss: 0.27236828207969666\n",
      "epoch: 3, batch: 648, loss: 0.26009541749954224\n",
      "epoch: 3, batch: 649, loss: 0.2505233883857727\n",
      "epoch: 3, batch: 650, loss: 0.15202535688877106\n",
      "epoch: 3, batch: 651, loss: 0.18799370527267456\n",
      "epoch: 3, batch: 652, loss: 0.23761573433876038\n",
      "epoch: 3, batch: 653, loss: 0.2713165283203125\n",
      "epoch: 3, batch: 654, loss: 0.1919594705104828\n",
      "epoch: 3, batch: 655, loss: 0.17069624364376068\n",
      "epoch: 3, batch: 656, loss: 0.3052487373352051\n",
      "epoch: 3, batch: 657, loss: 0.2526223659515381\n",
      "epoch: 3, batch: 658, loss: 0.19523556530475616\n",
      "epoch: 3, batch: 659, loss: 0.2059350162744522\n",
      "epoch: 3, batch: 660, loss: 0.1694076955318451\n",
      "epoch: 3, batch: 661, loss: 0.20828965306282043\n",
      "epoch: 3, batch: 662, loss: 0.2890135645866394\n",
      "epoch: 3, batch: 663, loss: 0.11865227669477463\n",
      "epoch: 3, batch: 664, loss: 0.1955452263355255\n",
      "epoch: 3, batch: 665, loss: 0.3097509443759918\n",
      "epoch: 3, batch: 666, loss: 0.3907182812690735\n",
      "epoch: 3, batch: 667, loss: 0.18141157925128937\n",
      "epoch: 3, batch: 668, loss: 0.21772681176662445\n",
      "epoch: 3, batch: 669, loss: 0.31452465057373047\n",
      "epoch: 3, batch: 670, loss: 0.27885377407073975\n",
      "epoch: 3, batch: 671, loss: 0.536393404006958\n",
      "epoch: 3, batch: 672, loss: 0.1758427917957306\n",
      "epoch: 3, batch: 673, loss: 0.11706399917602539\n",
      "epoch: 3, batch: 674, loss: 0.07979460060596466\n",
      "epoch: 3, batch: 675, loss: 0.32567697763442993\n",
      "epoch: 3, batch: 676, loss: 0.1505456417798996\n",
      "epoch: 3, batch: 677, loss: 0.2241593599319458\n",
      "epoch: 3, batch: 678, loss: 0.16963250935077667\n",
      "epoch: 3, batch: 679, loss: 0.19477935135364532\n",
      "epoch: 3, batch: 680, loss: 0.34658998250961304\n",
      "epoch: 3, batch: 681, loss: 0.38674071431159973\n",
      "epoch: 3, batch: 682, loss: 0.32004544138908386\n",
      "epoch: 3, batch: 683, loss: 0.17736634612083435\n",
      "epoch: 3, batch: 684, loss: 0.39391636848449707\n",
      "epoch: 3, batch: 685, loss: 0.18151241540908813\n",
      "epoch: 3, batch: 686, loss: 0.23275011777877808\n",
      "epoch: 3, batch: 687, loss: 0.2100427746772766\n",
      "epoch: 3, batch: 688, loss: 0.20022359490394592\n",
      "epoch: 3, batch: 689, loss: 0.3485325574874878\n",
      "epoch: 3, batch: 690, loss: 0.2380080670118332\n",
      "epoch: 3, batch: 691, loss: 0.12290926277637482\n",
      "epoch: 3, batch: 692, loss: 0.21370576322078705\n",
      "epoch: 3, batch: 693, loss: 0.07441762089729309\n",
      "epoch: 3, batch: 694, loss: 0.2801603078842163\n",
      "epoch: 3, batch: 695, loss: 0.31584975123405457\n",
      "epoch: 3, batch: 696, loss: 0.22634828090667725\n",
      "epoch: 3, batch: 697, loss: 0.3863128423690796\n",
      "epoch: 3, batch: 698, loss: 0.15052273869514465\n",
      "epoch: 3, batch: 699, loss: 0.06731683760881424\n",
      "epoch: 3, batch: 700, loss: 0.3141571879386902\n",
      "epoch: 3, batch: 701, loss: 0.4124988913536072\n",
      "epoch: 3, batch: 702, loss: 0.18246404826641083\n",
      "epoch: 3, batch: 703, loss: 0.3957611322402954\n",
      "epoch: 3, batch: 704, loss: 0.41595378518104553\n",
      "epoch: 3, batch: 705, loss: 0.2238832265138626\n",
      "epoch: 3, batch: 706, loss: 0.23262906074523926\n",
      "epoch: 3, batch: 707, loss: 0.24343305826187134\n",
      "epoch: 3, batch: 708, loss: 0.2591012716293335\n",
      "epoch: 3, batch: 709, loss: 0.22808058559894562\n",
      "epoch: 3, batch: 710, loss: 0.3714854121208191\n",
      "epoch: 3, batch: 711, loss: 0.25413456559181213\n",
      "epoch: 3, batch: 712, loss: 0.1421041339635849\n",
      "epoch: 3, batch: 713, loss: 0.17128652334213257\n",
      "epoch: 3, batch: 714, loss: 0.30055761337280273\n",
      "epoch: 3, batch: 715, loss: 0.23356971144676208\n",
      "epoch: 3, batch: 716, loss: 0.2787552773952484\n",
      "epoch: 3, batch: 717, loss: 0.3535107672214508\n",
      "epoch: 3, batch: 718, loss: 0.24432972073554993\n",
      "epoch: 3, batch: 719, loss: 0.1755066066980362\n",
      "epoch: 3, batch: 720, loss: 0.3840332329273224\n",
      "epoch: 3, batch: 721, loss: 0.4092707633972168\n",
      "epoch: 3, batch: 722, loss: 0.24831272661685944\n",
      "epoch: 3, batch: 723, loss: 0.20268449187278748\n",
      "epoch: 3, batch: 724, loss: 0.32295793294906616\n",
      "epoch: 3, batch: 725, loss: 0.3092275559902191\n",
      "epoch: 3, batch: 726, loss: 0.21890327334403992\n",
      "epoch: 3, batch: 727, loss: 0.3834846317768097\n",
      "epoch: 3, batch: 728, loss: 0.3139277696609497\n",
      "epoch: 3, batch: 729, loss: 0.13866595923900604\n",
      "epoch: 3, batch: 730, loss: 0.2523237466812134\n",
      "epoch: 3, batch: 731, loss: 0.2429141253232956\n",
      "epoch: 3, batch: 732, loss: 0.23025692999362946\n",
      "epoch: 3, batch: 733, loss: 0.2509133815765381\n",
      "epoch: 3, batch: 734, loss: 0.2667975127696991\n",
      "epoch: 3, batch: 735, loss: 0.21850979328155518\n",
      "epoch: 3, batch: 736, loss: 0.3203936219215393\n",
      "epoch: 3, batch: 737, loss: 0.30388736724853516\n",
      "epoch: 3, batch: 738, loss: 0.27550217509269714\n",
      "epoch: 3, batch: 739, loss: 0.5282608270645142\n",
      "epoch: 3, batch: 740, loss: 0.2509557008743286\n",
      "epoch: 3, batch: 741, loss: 0.1006387323141098\n",
      "epoch: 3, batch: 742, loss: 0.3814743161201477\n",
      "epoch: 3, batch: 743, loss: 0.38695400953292847\n",
      "epoch: 3, batch: 744, loss: 0.1713295727968216\n",
      "epoch: 3, batch: 745, loss: 0.23322531580924988\n",
      "epoch: 3, batch: 746, loss: 0.18110880255699158\n",
      "epoch: 3, batch: 747, loss: 0.19989274442195892\n",
      "epoch: 3, batch: 748, loss: 0.18027758598327637\n",
      "epoch: 3, batch: 749, loss: 0.1755504161119461\n",
      "epoch: 3, batch: 750, loss: 0.17578572034835815\n",
      "epoch: 3, batch: 751, loss: 0.2536025941371918\n",
      "epoch: 3, batch: 752, loss: 0.17696070671081543\n",
      "epoch: 3, batch: 753, loss: 0.2715531587600708\n",
      "epoch: 3, batch: 754, loss: 0.325376957654953\n",
      "epoch: 3, batch: 755, loss: 0.28330475091934204\n",
      "epoch: 3, batch: 756, loss: 0.3001570701599121\n",
      "epoch: 3, batch: 757, loss: 0.12210871279239655\n",
      "epoch: 3, batch: 758, loss: 0.12676814198493958\n",
      "epoch: 3, batch: 759, loss: 0.29664337635040283\n",
      "epoch: 3, batch: 760, loss: 0.3281121551990509\n",
      "epoch: 3, batch: 761, loss: 0.22141507267951965\n",
      "epoch: 3, batch: 762, loss: 0.24702481925487518\n",
      "epoch: 3, batch: 763, loss: 0.13012568652629852\n",
      "epoch: 3, batch: 764, loss: 0.2963353991508484\n",
      "epoch: 3, batch: 765, loss: 0.28680387139320374\n",
      "epoch: 3, batch: 766, loss: 0.4029886722564697\n",
      "epoch: 3, batch: 767, loss: 0.1669834554195404\n",
      "epoch: 3, batch: 768, loss: 0.38388335704803467\n",
      "epoch: 3, batch: 769, loss: 0.14759717881679535\n",
      "epoch: 3, batch: 770, loss: 0.2895578444004059\n",
      "epoch: 3, batch: 771, loss: 0.1974652260541916\n",
      "epoch: 3, batch: 772, loss: 0.1944160908460617\n",
      "epoch: 3, batch: 773, loss: 0.1677514761686325\n",
      "epoch: 3, batch: 774, loss: 0.2446102499961853\n",
      "epoch: 3, batch: 775, loss: 0.34362179040908813\n",
      "epoch: 3, batch: 776, loss: 0.21499845385551453\n",
      "epoch: 3, batch: 777, loss: 0.3346017897129059\n",
      "epoch: 3, batch: 778, loss: 0.1828414797782898\n",
      "epoch: 3, batch: 779, loss: 0.10537957400083542\n",
      "epoch: 3, batch: 780, loss: 0.1959429234266281\n",
      "epoch: 3, batch: 781, loss: 0.22633351385593414\n",
      "epoch: 3, batch: 782, loss: 0.35449227690696716\n",
      "epoch: 3, batch: 783, loss: 0.231473907828331\n",
      "epoch: 3, batch: 784, loss: 0.08606299012899399\n",
      "epoch: 3, batch: 785, loss: 0.28303298354148865\n",
      "epoch: 3, batch: 786, loss: 0.2202831506729126\n",
      "epoch: 3, batch: 787, loss: 0.15097881853580475\n",
      "epoch: 3, batch: 788, loss: 0.13484269380569458\n",
      "epoch: 3, batch: 789, loss: 0.28164491057395935\n",
      "epoch: 3, batch: 790, loss: 0.15610353648662567\n",
      "epoch: 3, batch: 791, loss: 0.19692343473434448\n",
      "epoch: 3, batch: 792, loss: 0.19940780103206635\n",
      "epoch: 3, batch: 793, loss: 0.14332830905914307\n",
      "epoch: 3, batch: 794, loss: 0.529542088508606\n",
      "epoch: 3, batch: 795, loss: 0.4428066611289978\n",
      "epoch: 3, batch: 796, loss: 0.2765866816043854\n",
      "epoch: 3, batch: 797, loss: 0.20477482676506042\n",
      "epoch: 3, batch: 798, loss: 0.33402886986732483\n",
      "epoch: 3, batch: 799, loss: 0.2562122642993927\n",
      "epoch: 3, batch: 800, loss: 0.23762482404708862\n",
      "epoch: 3, batch: 801, loss: 0.21600602567195892\n",
      "epoch: 3, batch: 802, loss: 0.22587288916110992\n",
      "epoch: 3, batch: 803, loss: 0.11747817695140839\n",
      "epoch: 3, batch: 804, loss: 0.10950066149234772\n",
      "epoch: 3, batch: 805, loss: 0.3573894202709198\n",
      "epoch: 3, batch: 806, loss: 0.43753379583358765\n",
      "epoch: 3, batch: 807, loss: 0.2263244092464447\n",
      "epoch: 3, batch: 808, loss: 0.2342016100883484\n",
      "epoch: 3, batch: 809, loss: 0.0673021748661995\n",
      "epoch: 3, batch: 810, loss: 0.23789717257022858\n",
      "epoch: 3, batch: 811, loss: 0.07908988744020462\n",
      "epoch: 3, batch: 812, loss: 0.13663941621780396\n",
      "epoch: 3, batch: 813, loss: 0.20937514305114746\n",
      "epoch: 3, batch: 814, loss: 0.18587233126163483\n",
      "epoch: 3, batch: 815, loss: 0.16408908367156982\n",
      "epoch: 3, batch: 816, loss: 0.11336001008749008\n",
      "epoch: 3, batch: 817, loss: 0.1417853683233261\n",
      "epoch: 3, batch: 818, loss: 0.21062451601028442\n",
      "epoch: 3, batch: 819, loss: 0.2317916750907898\n",
      "epoch: 3, batch: 820, loss: 0.35896509885787964\n",
      "epoch: 3, batch: 821, loss: 0.3368641138076782\n",
      "epoch: 3, batch: 822, loss: 0.19700942933559418\n",
      "epoch: 3, batch: 823, loss: 0.1779201328754425\n",
      "epoch: 3, batch: 824, loss: 0.2055993378162384\n",
      "epoch: 3, batch: 825, loss: 0.12505033612251282\n",
      "epoch: 3, batch: 826, loss: 0.17102180421352386\n",
      "epoch: 3, batch: 827, loss: 0.1460481435060501\n",
      "epoch: 3, batch: 828, loss: 0.17772714793682098\n",
      "epoch: 3, batch: 829, loss: 0.26463231444358826\n",
      "epoch: 3, batch: 830, loss: 0.23686707019805908\n",
      "epoch: 3, batch: 831, loss: 0.10301848500967026\n",
      "epoch: 3, batch: 832, loss: 0.19711478054523468\n",
      "epoch: 3, batch: 833, loss: 0.14944326877593994\n",
      "epoch: 3, batch: 834, loss: 0.6065875291824341\n",
      "epoch: 3, batch: 835, loss: 0.24321763217449188\n",
      "epoch: 3, batch: 836, loss: 0.2582741379737854\n",
      "epoch: 3, batch: 837, loss: 0.15839627385139465\n",
      "epoch: 3, batch: 838, loss: 0.2717815637588501\n",
      "epoch: 3, batch: 839, loss: 0.2733980417251587\n",
      "epoch: 3, batch: 840, loss: 0.15669938921928406\n",
      "epoch: 3, batch: 841, loss: 0.24755118787288666\n",
      "epoch: 3, batch: 842, loss: 0.28201529383659363\n",
      "epoch: 3, batch: 843, loss: 0.39561694860458374\n",
      "epoch: 3, batch: 844, loss: 0.13420088589191437\n",
      "epoch: 3, batch: 845, loss: 0.22712063789367676\n",
      "epoch: 3, batch: 846, loss: 0.27452346682548523\n",
      "epoch: 3, batch: 847, loss: 0.1596316248178482\n",
      "epoch: 3, batch: 848, loss: 0.26152220368385315\n",
      "epoch: 3, batch: 849, loss: 0.2585092782974243\n",
      "epoch: 3, batch: 850, loss: 0.20779789984226227\n",
      "epoch: 3, batch: 851, loss: 0.1814098060131073\n",
      "epoch: 3, batch: 852, loss: 0.24178123474121094\n",
      "epoch: 3, batch: 853, loss: 0.14034782350063324\n",
      "epoch: 3, batch: 854, loss: 0.1418466866016388\n",
      "epoch: 3, batch: 855, loss: 0.2161027193069458\n",
      "epoch: 3, batch: 856, loss: 0.16001921892166138\n",
      "epoch: 3, batch: 857, loss: 0.17661890387535095\n",
      "epoch: 3, batch: 858, loss: 0.13480742275714874\n",
      "epoch: 3, batch: 859, loss: 0.18264928460121155\n",
      "epoch: 3, batch: 860, loss: 0.2852150797843933\n",
      "epoch: 3, batch: 861, loss: 0.19476638734340668\n",
      "epoch: 3, batch: 862, loss: 0.312060683965683\n",
      "epoch: 3, batch: 863, loss: 0.13137206435203552\n",
      "epoch: 3, batch: 864, loss: 0.3569222390651703\n",
      "epoch: 3, batch: 865, loss: 0.20119708776474\n",
      "epoch: 3, batch: 866, loss: 0.5399255752563477\n",
      "epoch: 3, batch: 867, loss: 0.3643430173397064\n",
      "epoch: 3, batch: 868, loss: 0.21277083456516266\n",
      "epoch: 3, batch: 869, loss: 0.4241628050804138\n",
      "epoch: 3, batch: 870, loss: 0.3041766583919525\n",
      "epoch: 3, batch: 871, loss: 0.10901348292827606\n",
      "epoch: 3, batch: 872, loss: 0.305859237909317\n",
      "epoch: 3, batch: 873, loss: 0.12101352959871292\n",
      "epoch: 3, batch: 874, loss: 0.23720072209835052\n",
      "epoch: 3, batch: 875, loss: 0.23441661894321442\n",
      "epoch: 3, batch: 876, loss: 0.1467430591583252\n",
      "epoch: 3, batch: 877, loss: 0.28541404008865356\n",
      "epoch: 3, batch: 878, loss: 0.20066069066524506\n",
      "epoch: 3, batch: 879, loss: 0.21983183920383453\n",
      "epoch: 3, batch: 880, loss: 0.30371707677841187\n",
      "epoch: 3, batch: 881, loss: 0.35231438279151917\n",
      "epoch: 3, batch: 882, loss: 0.10487242043018341\n",
      "epoch: 3, batch: 883, loss: 0.1701917201280594\n",
      "epoch: 3, batch: 884, loss: 0.14929139614105225\n",
      "epoch: 3, batch: 885, loss: 0.1867889016866684\n",
      "epoch: 3, batch: 886, loss: 0.19163203239440918\n",
      "epoch: 3, batch: 887, loss: 0.221497043967247\n",
      "epoch: 3, batch: 888, loss: 0.33437734842300415\n",
      "epoch: 3, batch: 889, loss: 0.2796655297279358\n",
      "epoch: 3, batch: 890, loss: 0.11482662707567215\n",
      "epoch: 3, batch: 891, loss: 0.18727251887321472\n",
      "epoch: 3, batch: 892, loss: 0.4750715494155884\n",
      "epoch: 3, batch: 893, loss: 0.4404461085796356\n",
      "epoch: 3, batch: 894, loss: 0.2684331238269806\n",
      "epoch: 3, batch: 895, loss: 0.30202117562294006\n",
      "epoch: 3, batch: 896, loss: 0.22817936539649963\n",
      "epoch: 3, batch: 897, loss: 0.20338822901248932\n",
      "epoch: 3, batch: 898, loss: 0.20950834453105927\n",
      "epoch: 3, batch: 899, loss: 0.16810844838619232\n",
      "epoch: 3, batch: 900, loss: 0.12192890048027039\n",
      "epoch: 3, batch: 901, loss: 0.261540025472641\n",
      "epoch: 3, batch: 902, loss: 0.22232931852340698\n",
      "epoch: 3, batch: 903, loss: 0.30980801582336426\n",
      "epoch: 3, batch: 904, loss: 0.17974162101745605\n",
      "epoch: 3, batch: 905, loss: 0.25050991773605347\n",
      "epoch: 3, batch: 906, loss: 0.3159148395061493\n",
      "epoch: 3, batch: 907, loss: 0.27826860547065735\n",
      "epoch: 3, batch: 908, loss: 0.12232787907123566\n",
      "epoch: 3, batch: 909, loss: 0.18433649837970734\n",
      "epoch: 3, batch: 910, loss: 0.24008461833000183\n",
      "epoch: 3, batch: 911, loss: 0.14405278861522675\n",
      "epoch: 3, batch: 912, loss: 0.2517924904823303\n",
      "epoch: 3, batch: 913, loss: 0.34328845143318176\n",
      "epoch: 3, batch: 914, loss: 0.11000818759202957\n",
      "epoch: 3, batch: 915, loss: 0.29818451404571533\n",
      "epoch: 3, batch: 916, loss: 0.23052166402339935\n",
      "epoch: 3, batch: 917, loss: 0.37156111001968384\n",
      "epoch: 3, batch: 918, loss: 0.3634835183620453\n",
      "epoch: 3, batch: 919, loss: 0.36672747135162354\n",
      "epoch: 3, batch: 920, loss: 0.19783100485801697\n",
      "epoch: 3, batch: 921, loss: 0.13734856247901917\n",
      "epoch: 3, batch: 922, loss: 0.3029143214225769\n",
      "epoch: 3, batch: 923, loss: 0.17246533930301666\n",
      "epoch: 3, batch: 924, loss: 0.3043105900287628\n",
      "epoch: 3, batch: 925, loss: 0.12150204181671143\n",
      "epoch: 3, batch: 926, loss: 0.19472332298755646\n",
      "epoch: 3, batch: 927, loss: 0.17949600517749786\n",
      "epoch: 3, batch: 928, loss: 0.43549197912216187\n",
      "epoch: 3, batch: 929, loss: 0.13395138084888458\n",
      "epoch: 3, batch: 930, loss: 0.2030533254146576\n",
      "epoch: 3, batch: 931, loss: 0.16195619106292725\n",
      "epoch: 3, batch: 932, loss: 0.5427576899528503\n",
      "epoch: 3, batch: 933, loss: 0.19594603776931763\n",
      "epoch: 3, batch: 934, loss: 0.17225468158721924\n",
      "epoch: 3, batch: 935, loss: 0.1480090618133545\n",
      "epoch: 3, batch: 936, loss: 0.22671560943126678\n",
      "epoch: 3, batch: 937, loss: 0.2130213975906372\n",
      "epoch: 4, batch: 0, loss: 0.44215086102485657\n",
      "epoch: 4, batch: 1, loss: 0.16602082550525665\n",
      "epoch: 4, batch: 2, loss: 0.17588959634304047\n",
      "epoch: 4, batch: 3, loss: 0.23825910687446594\n",
      "epoch: 4, batch: 4, loss: 0.32475101947784424\n",
      "epoch: 4, batch: 5, loss: 0.2718169689178467\n",
      "epoch: 4, batch: 6, loss: 0.08124473690986633\n",
      "epoch: 4, batch: 7, loss: 0.19841113686561584\n",
      "epoch: 4, batch: 8, loss: 0.13285398483276367\n",
      "epoch: 4, batch: 9, loss: 0.12818488478660583\n",
      "epoch: 4, batch: 10, loss: 0.09827111661434174\n",
      "epoch: 4, batch: 11, loss: 0.20322944223880768\n",
      "epoch: 4, batch: 12, loss: 0.18430043756961823\n",
      "epoch: 4, batch: 13, loss: 0.2558872699737549\n",
      "epoch: 4, batch: 14, loss: 0.14651164412498474\n",
      "epoch: 4, batch: 15, loss: 0.2571480870246887\n",
      "epoch: 4, batch: 16, loss: 0.19014844298362732\n",
      "epoch: 4, batch: 17, loss: 0.29413461685180664\n",
      "epoch: 4, batch: 18, loss: 0.25629982352256775\n",
      "epoch: 4, batch: 19, loss: 0.3265701234340668\n",
      "epoch: 4, batch: 20, loss: 0.23260055482387543\n",
      "epoch: 4, batch: 21, loss: 0.15250399708747864\n",
      "epoch: 4, batch: 22, loss: 0.1858285516500473\n",
      "epoch: 4, batch: 23, loss: 0.2813856303691864\n",
      "epoch: 4, batch: 24, loss: 0.195868119597435\n",
      "epoch: 4, batch: 25, loss: 0.2721692621707916\n",
      "epoch: 4, batch: 26, loss: 0.23673848807811737\n",
      "epoch: 4, batch: 27, loss: 0.10703979432582855\n",
      "epoch: 4, batch: 28, loss: 0.22335152328014374\n",
      "epoch: 4, batch: 29, loss: 0.3053371012210846\n",
      "epoch: 4, batch: 30, loss: 0.3279206156730652\n",
      "epoch: 4, batch: 31, loss: 0.20815755426883698\n",
      "epoch: 4, batch: 32, loss: 0.14961807429790497\n",
      "epoch: 4, batch: 33, loss: 0.09075557440519333\n",
      "epoch: 4, batch: 34, loss: 0.28532713651657104\n",
      "epoch: 4, batch: 35, loss: 0.17353180050849915\n",
      "epoch: 4, batch: 36, loss: 0.2171637862920761\n",
      "epoch: 4, batch: 37, loss: 0.16189774870872498\n",
      "epoch: 4, batch: 38, loss: 0.3975832462310791\n",
      "epoch: 4, batch: 39, loss: 0.1818142533302307\n",
      "epoch: 4, batch: 40, loss: 0.30760928988456726\n",
      "epoch: 4, batch: 41, loss: 0.12389176338911057\n",
      "epoch: 4, batch: 42, loss: 0.17198453843593597\n",
      "epoch: 4, batch: 43, loss: 0.30974873900413513\n",
      "epoch: 4, batch: 44, loss: 0.23848217725753784\n",
      "epoch: 4, batch: 45, loss: 0.3206884264945984\n",
      "epoch: 4, batch: 46, loss: 0.2953754663467407\n",
      "epoch: 4, batch: 47, loss: 0.2591908276081085\n",
      "epoch: 4, batch: 48, loss: 0.1361003816127777\n",
      "epoch: 4, batch: 49, loss: 0.18437431752681732\n",
      "epoch: 4, batch: 50, loss: 0.1840124875307083\n",
      "epoch: 4, batch: 51, loss: 0.27400341629981995\n",
      "epoch: 4, batch: 52, loss: 0.3400987386703491\n",
      "epoch: 4, batch: 53, loss: 0.16635097563266754\n",
      "epoch: 4, batch: 54, loss: 0.25267088413238525\n",
      "epoch: 4, batch: 55, loss: 0.18961334228515625\n",
      "epoch: 4, batch: 56, loss: 0.12151487171649933\n",
      "epoch: 4, batch: 57, loss: 0.08767640590667725\n",
      "epoch: 4, batch: 58, loss: 0.12529350817203522\n",
      "epoch: 4, batch: 59, loss: 0.12469112128019333\n",
      "epoch: 4, batch: 60, loss: 0.36510875821113586\n",
      "epoch: 4, batch: 61, loss: 0.17881466448307037\n",
      "epoch: 4, batch: 62, loss: 0.12302479147911072\n",
      "epoch: 4, batch: 63, loss: 0.08797048032283783\n",
      "epoch: 4, batch: 64, loss: 0.22263777256011963\n",
      "epoch: 4, batch: 65, loss: 0.2067500203847885\n",
      "epoch: 4, batch: 66, loss: 0.2662090063095093\n",
      "epoch: 4, batch: 67, loss: 0.30268430709838867\n",
      "epoch: 4, batch: 68, loss: 0.19830036163330078\n",
      "epoch: 4, batch: 69, loss: 0.2377033829689026\n",
      "epoch: 4, batch: 70, loss: 0.10897505283355713\n",
      "epoch: 4, batch: 71, loss: 0.1637040078639984\n",
      "epoch: 4, batch: 72, loss: 0.18001247942447662\n",
      "epoch: 4, batch: 73, loss: 0.26722514629364014\n",
      "epoch: 4, batch: 74, loss: 0.20245541632175446\n",
      "epoch: 4, batch: 75, loss: 0.12002526968717575\n",
      "epoch: 4, batch: 76, loss: 0.16061212122440338\n",
      "epoch: 4, batch: 77, loss: 0.25135910511016846\n",
      "epoch: 4, batch: 78, loss: 0.15903885662555695\n",
      "epoch: 4, batch: 79, loss: 0.2273995578289032\n",
      "epoch: 4, batch: 80, loss: 0.05593651905655861\n",
      "epoch: 4, batch: 81, loss: 0.32199081778526306\n",
      "epoch: 4, batch: 82, loss: 0.1699923425912857\n",
      "epoch: 4, batch: 83, loss: 0.3141315281391144\n",
      "epoch: 4, batch: 84, loss: 0.16924582421779633\n",
      "epoch: 4, batch: 85, loss: 0.2621784806251526\n",
      "epoch: 4, batch: 86, loss: 0.13629503548145294\n",
      "epoch: 4, batch: 87, loss: 0.18991824984550476\n",
      "epoch: 4, batch: 88, loss: 0.1869814097881317\n",
      "epoch: 4, batch: 89, loss: 0.08839517831802368\n",
      "epoch: 4, batch: 90, loss: 0.1740495264530182\n",
      "epoch: 4, batch: 91, loss: 0.2312048226594925\n",
      "epoch: 4, batch: 92, loss: 0.5205754637718201\n",
      "epoch: 4, batch: 93, loss: 0.3153073489665985\n",
      "epoch: 4, batch: 94, loss: 0.14241966605186462\n",
      "epoch: 4, batch: 95, loss: 0.3684186637401581\n",
      "epoch: 4, batch: 96, loss: 0.2027241736650467\n",
      "epoch: 4, batch: 97, loss: 0.27702975273132324\n",
      "epoch: 4, batch: 98, loss: 0.2189517766237259\n",
      "epoch: 4, batch: 99, loss: 0.393114298582077\n",
      "epoch: 4, batch: 100, loss: 0.21961574256420135\n",
      "epoch: 4, batch: 101, loss: 0.34617677330970764\n",
      "epoch: 4, batch: 102, loss: 0.2125328779220581\n",
      "epoch: 4, batch: 103, loss: 0.33624494075775146\n",
      "epoch: 4, batch: 104, loss: 0.14709711074829102\n",
      "epoch: 4, batch: 105, loss: 0.34565186500549316\n",
      "epoch: 4, batch: 106, loss: 0.2494199275970459\n",
      "epoch: 4, batch: 107, loss: 0.208225280046463\n",
      "epoch: 4, batch: 108, loss: 0.4191679060459137\n",
      "epoch: 4, batch: 109, loss: 0.1500307321548462\n",
      "epoch: 4, batch: 110, loss: 0.3986721336841583\n",
      "epoch: 4, batch: 111, loss: 0.2229682356119156\n",
      "epoch: 4, batch: 112, loss: 0.2750091254711151\n",
      "epoch: 4, batch: 113, loss: 0.12073425203561783\n",
      "epoch: 4, batch: 114, loss: 0.3311718702316284\n",
      "epoch: 4, batch: 115, loss: 0.26053565740585327\n",
      "epoch: 4, batch: 116, loss: 0.16232633590698242\n",
      "epoch: 4, batch: 117, loss: 0.26249802112579346\n",
      "epoch: 4, batch: 118, loss: 0.14618152379989624\n",
      "epoch: 4, batch: 119, loss: 0.2084871232509613\n",
      "epoch: 4, batch: 120, loss: 0.3106551468372345\n",
      "epoch: 4, batch: 121, loss: 0.10623031854629517\n",
      "epoch: 4, batch: 122, loss: 0.21851426362991333\n",
      "epoch: 4, batch: 123, loss: 0.098973348736763\n",
      "epoch: 4, batch: 124, loss: 0.30947116017341614\n",
      "epoch: 4, batch: 125, loss: 0.11461181938648224\n",
      "epoch: 4, batch: 126, loss: 0.16259466111660004\n",
      "epoch: 4, batch: 127, loss: 0.15182246267795563\n",
      "epoch: 4, batch: 128, loss: 0.18576891720294952\n",
      "epoch: 4, batch: 129, loss: 0.14789775013923645\n",
      "epoch: 4, batch: 130, loss: 0.34531015157699585\n",
      "epoch: 4, batch: 131, loss: 0.2734526991844177\n",
      "epoch: 4, batch: 132, loss: 0.13794156908988953\n",
      "epoch: 4, batch: 133, loss: 0.13750293850898743\n",
      "epoch: 4, batch: 134, loss: 0.12497813254594803\n",
      "epoch: 4, batch: 135, loss: 0.1813480406999588\n",
      "epoch: 4, batch: 136, loss: 0.29951211810112\n",
      "epoch: 4, batch: 137, loss: 0.13909851014614105\n",
      "epoch: 4, batch: 138, loss: 0.3143475353717804\n",
      "epoch: 4, batch: 139, loss: 0.5053079724311829\n",
      "epoch: 4, batch: 140, loss: 0.14997264742851257\n",
      "epoch: 4, batch: 141, loss: 0.18166178464889526\n",
      "epoch: 4, batch: 142, loss: 0.10653799772262573\n",
      "epoch: 4, batch: 143, loss: 0.16870401799678802\n",
      "epoch: 4, batch: 144, loss: 0.28176456689834595\n",
      "epoch: 4, batch: 145, loss: 0.2985209822654724\n",
      "epoch: 4, batch: 146, loss: 0.48423755168914795\n",
      "epoch: 4, batch: 147, loss: 0.23015503585338593\n",
      "epoch: 4, batch: 148, loss: 0.1428166776895523\n",
      "epoch: 4, batch: 149, loss: 0.2997238039970398\n",
      "epoch: 4, batch: 150, loss: 0.17051924765110016\n",
      "epoch: 4, batch: 151, loss: 0.4134211242198944\n",
      "epoch: 4, batch: 152, loss: 0.1659858375787735\n",
      "epoch: 4, batch: 153, loss: 0.22105224430561066\n",
      "epoch: 4, batch: 154, loss: 0.21379636228084564\n",
      "epoch: 4, batch: 155, loss: 0.16550621390342712\n",
      "epoch: 4, batch: 156, loss: 0.16770383715629578\n",
      "epoch: 4, batch: 157, loss: 0.10190566629171371\n",
      "epoch: 4, batch: 158, loss: 0.21402409672737122\n",
      "epoch: 4, batch: 159, loss: 0.19476759433746338\n",
      "epoch: 4, batch: 160, loss: 0.3048153221607208\n",
      "epoch: 4, batch: 161, loss: 0.13057519495487213\n",
      "epoch: 4, batch: 162, loss: 0.21381862461566925\n",
      "epoch: 4, batch: 163, loss: 0.22299213707447052\n",
      "epoch: 4, batch: 164, loss: 0.2718556821346283\n",
      "epoch: 4, batch: 165, loss: 0.3573382496833801\n",
      "epoch: 4, batch: 166, loss: 0.21783043444156647\n",
      "epoch: 4, batch: 167, loss: 0.1781444102525711\n",
      "epoch: 4, batch: 168, loss: 0.25065019726753235\n",
      "epoch: 4, batch: 169, loss: 0.32745370268821716\n",
      "epoch: 4, batch: 170, loss: 0.14646530151367188\n",
      "epoch: 4, batch: 171, loss: 0.27830424904823303\n",
      "epoch: 4, batch: 172, loss: 0.13802294433116913\n",
      "epoch: 4, batch: 173, loss: 0.24089449644088745\n",
      "epoch: 4, batch: 174, loss: 0.3666820824146271\n",
      "epoch: 4, batch: 175, loss: 0.11573503911495209\n",
      "epoch: 4, batch: 176, loss: 0.31060346961021423\n",
      "epoch: 4, batch: 177, loss: 0.28617146611213684\n",
      "epoch: 4, batch: 178, loss: 0.2588377594947815\n",
      "epoch: 4, batch: 179, loss: 0.11892568320035934\n",
      "epoch: 4, batch: 180, loss: 0.2250458151102066\n",
      "epoch: 4, batch: 181, loss: 0.32232704758644104\n",
      "epoch: 4, batch: 182, loss: 0.46528950333595276\n",
      "epoch: 4, batch: 183, loss: 0.27629807591438293\n",
      "epoch: 4, batch: 184, loss: 0.4763888418674469\n",
      "epoch: 4, batch: 185, loss: 0.1975821554660797\n",
      "epoch: 4, batch: 186, loss: 0.314157634973526\n",
      "epoch: 4, batch: 187, loss: 0.15977197885513306\n",
      "epoch: 4, batch: 188, loss: 0.18226312100887299\n",
      "epoch: 4, batch: 189, loss: 0.33060455322265625\n",
      "epoch: 4, batch: 190, loss: 0.12788352370262146\n",
      "epoch: 4, batch: 191, loss: 0.25246667861938477\n",
      "epoch: 4, batch: 192, loss: 0.18729300796985626\n",
      "epoch: 4, batch: 193, loss: 0.22692042589187622\n",
      "epoch: 4, batch: 194, loss: 0.17431217432022095\n",
      "epoch: 4, batch: 195, loss: 0.1766049712896347\n",
      "epoch: 4, batch: 196, loss: 0.1749824583530426\n",
      "epoch: 4, batch: 197, loss: 0.1977890431880951\n",
      "epoch: 4, batch: 198, loss: 0.0954555869102478\n",
      "epoch: 4, batch: 199, loss: 0.2553808093070984\n",
      "epoch: 4, batch: 200, loss: 0.25401628017425537\n",
      "epoch: 4, batch: 201, loss: 0.2054169625043869\n",
      "epoch: 4, batch: 202, loss: 0.13891273736953735\n",
      "epoch: 4, batch: 203, loss: 0.18581314384937286\n",
      "epoch: 4, batch: 204, loss: 0.19622598588466644\n",
      "epoch: 4, batch: 205, loss: 0.11126792430877686\n",
      "epoch: 4, batch: 206, loss: 0.25365808606147766\n",
      "epoch: 4, batch: 207, loss: 0.1519506573677063\n",
      "epoch: 4, batch: 208, loss: 0.2734644412994385\n",
      "epoch: 4, batch: 209, loss: 0.184193417429924\n",
      "epoch: 4, batch: 210, loss: 0.40620195865631104\n",
      "epoch: 4, batch: 211, loss: 0.2637602686882019\n",
      "epoch: 4, batch: 212, loss: 0.19108068943023682\n",
      "epoch: 4, batch: 213, loss: 0.31124404072761536\n",
      "epoch: 4, batch: 214, loss: 0.36414265632629395\n",
      "epoch: 4, batch: 215, loss: 0.12728853523731232\n",
      "epoch: 4, batch: 216, loss: 0.14870098233222961\n",
      "epoch: 4, batch: 217, loss: 0.11601186543703079\n",
      "epoch: 4, batch: 218, loss: 0.2676880359649658\n",
      "epoch: 4, batch: 219, loss: 0.17417779564857483\n",
      "epoch: 4, batch: 220, loss: 0.19406795501708984\n",
      "epoch: 4, batch: 221, loss: 0.11256659775972366\n",
      "epoch: 4, batch: 222, loss: 0.1943318247795105\n",
      "epoch: 4, batch: 223, loss: 0.1580352783203125\n",
      "epoch: 4, batch: 224, loss: 0.2683771848678589\n",
      "epoch: 4, batch: 225, loss: 0.32376253604888916\n",
      "epoch: 4, batch: 226, loss: 0.056573253124952316\n",
      "epoch: 4, batch: 227, loss: 0.1628892570734024\n",
      "epoch: 4, batch: 228, loss: 0.1478777676820755\n",
      "epoch: 4, batch: 229, loss: 0.2946875989437103\n",
      "epoch: 4, batch: 230, loss: 0.05981873348355293\n",
      "epoch: 4, batch: 231, loss: 0.29196175932884216\n",
      "epoch: 4, batch: 232, loss: 0.2073289304971695\n",
      "epoch: 4, batch: 233, loss: 0.20480987429618835\n",
      "epoch: 4, batch: 234, loss: 0.15200768411159515\n",
      "epoch: 4, batch: 235, loss: 0.05426652356982231\n",
      "epoch: 4, batch: 236, loss: 0.33700501918792725\n",
      "epoch: 4, batch: 237, loss: 0.17879706621170044\n",
      "epoch: 4, batch: 238, loss: 0.2581801116466522\n",
      "epoch: 4, batch: 239, loss: 0.214724600315094\n",
      "epoch: 4, batch: 240, loss: 0.14398664236068726\n",
      "epoch: 4, batch: 241, loss: 0.27706581354141235\n",
      "epoch: 4, batch: 242, loss: 0.09353266656398773\n",
      "epoch: 4, batch: 243, loss: 0.18616245687007904\n",
      "epoch: 4, batch: 244, loss: 0.2553935945034027\n",
      "epoch: 4, batch: 245, loss: 0.09785723686218262\n",
      "epoch: 4, batch: 246, loss: 0.24516765773296356\n",
      "epoch: 4, batch: 247, loss: 0.2978886365890503\n",
      "epoch: 4, batch: 248, loss: 0.2525588572025299\n",
      "epoch: 4, batch: 249, loss: 0.22170113027095795\n",
      "epoch: 4, batch: 250, loss: 0.13073097169399261\n",
      "epoch: 4, batch: 251, loss: 0.28007057309150696\n",
      "epoch: 4, batch: 252, loss: 0.17783799767494202\n",
      "epoch: 4, batch: 253, loss: 0.2571133077144623\n",
      "epoch: 4, batch: 254, loss: 0.11198926717042923\n",
      "epoch: 4, batch: 255, loss: 0.13372477889060974\n",
      "epoch: 4, batch: 256, loss: 0.34898272156715393\n",
      "epoch: 4, batch: 257, loss: 0.2342267781496048\n",
      "epoch: 4, batch: 258, loss: 0.06316570192575455\n",
      "epoch: 4, batch: 259, loss: 0.1485336273908615\n",
      "epoch: 4, batch: 260, loss: 0.1597577929496765\n",
      "epoch: 4, batch: 261, loss: 0.4304366111755371\n",
      "epoch: 4, batch: 262, loss: 0.2503005564212799\n",
      "epoch: 4, batch: 263, loss: 0.3734285235404968\n",
      "epoch: 4, batch: 264, loss: 0.2004270702600479\n",
      "epoch: 4, batch: 265, loss: 0.2697685658931732\n",
      "epoch: 4, batch: 266, loss: 0.08493034541606903\n",
      "epoch: 4, batch: 267, loss: 0.26343050599098206\n",
      "epoch: 4, batch: 268, loss: 0.13833807408809662\n",
      "epoch: 4, batch: 269, loss: 0.12553201615810394\n",
      "epoch: 4, batch: 270, loss: 0.4280615746974945\n",
      "epoch: 4, batch: 271, loss: 0.24653971195220947\n",
      "epoch: 4, batch: 272, loss: 0.23690739274024963\n",
      "epoch: 4, batch: 273, loss: 0.14671427011489868\n",
      "epoch: 4, batch: 274, loss: 0.334548681974411\n",
      "epoch: 4, batch: 275, loss: 0.15713806450366974\n",
      "epoch: 4, batch: 276, loss: 0.306185245513916\n",
      "epoch: 4, batch: 277, loss: 0.25945767760276794\n",
      "epoch: 4, batch: 278, loss: 0.15826064348220825\n",
      "epoch: 4, batch: 279, loss: 0.1570090651512146\n",
      "epoch: 4, batch: 280, loss: 0.25213146209716797\n",
      "epoch: 4, batch: 281, loss: 0.21044720709323883\n",
      "epoch: 4, batch: 282, loss: 0.12485440075397491\n",
      "epoch: 4, batch: 283, loss: 0.22736135125160217\n",
      "epoch: 4, batch: 284, loss: 0.09193442016839981\n",
      "epoch: 4, batch: 285, loss: 0.22418224811553955\n",
      "epoch: 4, batch: 286, loss: 0.2168167531490326\n",
      "epoch: 4, batch: 287, loss: 0.46083685755729675\n",
      "epoch: 4, batch: 288, loss: 0.19215577840805054\n",
      "epoch: 4, batch: 289, loss: 0.2550913393497467\n",
      "epoch: 4, batch: 290, loss: 0.14666321873664856\n",
      "epoch: 4, batch: 291, loss: 0.08108632266521454\n",
      "epoch: 4, batch: 292, loss: 0.18575771152973175\n",
      "epoch: 4, batch: 293, loss: 0.221080020070076\n",
      "epoch: 4, batch: 294, loss: 0.06908532977104187\n",
      "epoch: 4, batch: 295, loss: 0.2519124150276184\n",
      "epoch: 4, batch: 296, loss: 0.21310488879680634\n",
      "epoch: 4, batch: 297, loss: 0.3016032874584198\n",
      "epoch: 4, batch: 298, loss: 0.11534765362739563\n",
      "epoch: 4, batch: 299, loss: 0.22466135025024414\n",
      "epoch: 4, batch: 300, loss: 0.1707724630832672\n",
      "epoch: 4, batch: 301, loss: 0.24633295834064484\n",
      "epoch: 4, batch: 302, loss: 0.3621009588241577\n",
      "epoch: 4, batch: 303, loss: 0.16384784877300262\n",
      "epoch: 4, batch: 304, loss: 0.21454407274723053\n",
      "epoch: 4, batch: 305, loss: 0.1410522758960724\n",
      "epoch: 4, batch: 306, loss: 0.3921144902706146\n",
      "epoch: 4, batch: 307, loss: 0.05501997843384743\n",
      "epoch: 4, batch: 308, loss: 0.2518656253814697\n",
      "epoch: 4, batch: 309, loss: 0.11529818922281265\n",
      "epoch: 4, batch: 310, loss: 0.1631058007478714\n",
      "epoch: 4, batch: 311, loss: 0.11766176670789719\n",
      "epoch: 4, batch: 312, loss: 0.1304868906736374\n",
      "epoch: 4, batch: 313, loss: 0.4692372977733612\n",
      "epoch: 4, batch: 314, loss: 0.1945601999759674\n",
      "epoch: 4, batch: 315, loss: 0.14882947504520416\n",
      "epoch: 4, batch: 316, loss: 0.12990416586399078\n",
      "epoch: 4, batch: 317, loss: 0.16376009583473206\n",
      "epoch: 4, batch: 318, loss: 0.3299504816532135\n",
      "epoch: 4, batch: 319, loss: 0.26849281787872314\n",
      "epoch: 4, batch: 320, loss: 0.41425827145576477\n",
      "epoch: 4, batch: 321, loss: 0.08609341830015182\n",
      "epoch: 4, batch: 322, loss: 0.17260603606700897\n",
      "epoch: 4, batch: 323, loss: 0.2603561282157898\n",
      "epoch: 4, batch: 324, loss: 0.18021178245544434\n",
      "epoch: 4, batch: 325, loss: 0.08961620181798935\n",
      "epoch: 4, batch: 326, loss: 0.17041920125484467\n",
      "epoch: 4, batch: 327, loss: 0.24206970632076263\n",
      "epoch: 4, batch: 328, loss: 0.170353502035141\n",
      "epoch: 4, batch: 329, loss: 0.2344951182603836\n",
      "epoch: 4, batch: 330, loss: 0.10716267675161362\n",
      "epoch: 4, batch: 331, loss: 0.5442700982093811\n",
      "epoch: 4, batch: 332, loss: 0.08611031621694565\n",
      "epoch: 4, batch: 333, loss: 0.18299095332622528\n",
      "epoch: 4, batch: 334, loss: 0.04895429313182831\n",
      "epoch: 4, batch: 335, loss: 0.11025968939065933\n",
      "epoch: 4, batch: 336, loss: 0.16704849898815155\n",
      "epoch: 4, batch: 337, loss: 0.17384065687656403\n",
      "epoch: 4, batch: 338, loss: 0.1545199155807495\n",
      "epoch: 4, batch: 339, loss: 0.19625963270664215\n",
      "epoch: 4, batch: 340, loss: 0.10983126610517502\n",
      "epoch: 4, batch: 341, loss: 0.2345169633626938\n",
      "epoch: 4, batch: 342, loss: 0.223196342587471\n",
      "epoch: 4, batch: 343, loss: 0.25274357199668884\n",
      "epoch: 4, batch: 344, loss: 0.17253148555755615\n",
      "epoch: 4, batch: 345, loss: 0.13289868831634521\n",
      "epoch: 4, batch: 346, loss: 0.3733232319355011\n",
      "epoch: 4, batch: 347, loss: 0.28660446405410767\n",
      "epoch: 4, batch: 348, loss: 0.5042088627815247\n",
      "epoch: 4, batch: 349, loss: 0.15146580338478088\n",
      "epoch: 4, batch: 350, loss: 0.1470513641834259\n",
      "epoch: 4, batch: 351, loss: 0.20692096650600433\n",
      "epoch: 4, batch: 352, loss: 0.1429831087589264\n",
      "epoch: 4, batch: 353, loss: 0.34918028116226196\n",
      "epoch: 4, batch: 354, loss: 0.06848961114883423\n",
      "epoch: 4, batch: 355, loss: 0.20008759200572968\n",
      "epoch: 4, batch: 356, loss: 0.2478482574224472\n",
      "epoch: 4, batch: 357, loss: 0.2050725221633911\n",
      "epoch: 4, batch: 358, loss: 0.20162346959114075\n",
      "epoch: 4, batch: 359, loss: 0.2692204713821411\n",
      "epoch: 4, batch: 360, loss: 0.19950361549854279\n",
      "epoch: 4, batch: 361, loss: 0.31560173630714417\n",
      "epoch: 4, batch: 362, loss: 0.13538965582847595\n",
      "epoch: 4, batch: 363, loss: 0.13918925821781158\n",
      "epoch: 4, batch: 364, loss: 0.22511787712574005\n",
      "epoch: 4, batch: 365, loss: 0.10433656722307205\n",
      "epoch: 4, batch: 366, loss: 0.2496236115694046\n",
      "epoch: 4, batch: 367, loss: 0.27596163749694824\n",
      "epoch: 4, batch: 368, loss: 0.16502606868743896\n",
      "epoch: 4, batch: 369, loss: 0.3208805024623871\n",
      "epoch: 4, batch: 370, loss: 0.2510817050933838\n",
      "epoch: 4, batch: 371, loss: 0.22078180313110352\n",
      "epoch: 4, batch: 372, loss: 0.13391084969043732\n",
      "epoch: 4, batch: 373, loss: 0.20402716100215912\n",
      "epoch: 4, batch: 374, loss: 0.17465892434120178\n",
      "epoch: 4, batch: 375, loss: 0.12621590495109558\n",
      "epoch: 4, batch: 376, loss: 0.16664983332157135\n",
      "epoch: 4, batch: 377, loss: 0.2201841026544571\n",
      "epoch: 4, batch: 378, loss: 0.3490210771560669\n",
      "epoch: 4, batch: 379, loss: 0.15701411664485931\n",
      "epoch: 4, batch: 380, loss: 0.10265108197927475\n",
      "epoch: 4, batch: 381, loss: 0.2449668049812317\n",
      "epoch: 4, batch: 382, loss: 0.08117427676916122\n",
      "epoch: 4, batch: 383, loss: 0.15723764896392822\n",
      "epoch: 4, batch: 384, loss: 0.17766015231609344\n",
      "epoch: 4, batch: 385, loss: 0.4214590787887573\n",
      "epoch: 4, batch: 386, loss: 0.19528119266033173\n",
      "epoch: 4, batch: 387, loss: 0.2137322723865509\n",
      "epoch: 4, batch: 388, loss: 0.2843084931373596\n",
      "epoch: 4, batch: 389, loss: 0.1552048921585083\n",
      "epoch: 4, batch: 390, loss: 0.22054678201675415\n",
      "epoch: 4, batch: 391, loss: 0.1091514602303505\n",
      "epoch: 4, batch: 392, loss: 0.2589360177516937\n",
      "epoch: 4, batch: 393, loss: 0.1845805048942566\n",
      "epoch: 4, batch: 394, loss: 0.17898476123809814\n",
      "epoch: 4, batch: 395, loss: 0.13404245674610138\n",
      "epoch: 4, batch: 396, loss: 0.16609248518943787\n",
      "epoch: 4, batch: 397, loss: 0.20768766105175018\n",
      "epoch: 4, batch: 398, loss: 0.17258426547050476\n",
      "epoch: 4, batch: 399, loss: 0.11178645491600037\n",
      "epoch: 4, batch: 400, loss: 0.2800425887107849\n",
      "epoch: 4, batch: 401, loss: 0.1819150596857071\n",
      "epoch: 4, batch: 402, loss: 0.16174539923667908\n",
      "epoch: 4, batch: 403, loss: 0.1977590024471283\n",
      "epoch: 4, batch: 404, loss: 0.4442031979560852\n",
      "epoch: 4, batch: 405, loss: 0.07788874953985214\n",
      "epoch: 4, batch: 406, loss: 0.12721234560012817\n",
      "epoch: 4, batch: 407, loss: 0.14585533738136292\n",
      "epoch: 4, batch: 408, loss: 0.2673971951007843\n",
      "epoch: 4, batch: 409, loss: 0.13346150517463684\n",
      "epoch: 4, batch: 410, loss: 0.10444175451993942\n",
      "epoch: 4, batch: 411, loss: 0.1716475635766983\n",
      "epoch: 4, batch: 412, loss: 0.1801571398973465\n",
      "epoch: 4, batch: 413, loss: 0.23797248303890228\n",
      "epoch: 4, batch: 414, loss: 0.27401676774024963\n",
      "epoch: 4, batch: 415, loss: 0.14832909405231476\n",
      "epoch: 4, batch: 416, loss: 0.10813669860363007\n",
      "epoch: 4, batch: 417, loss: 0.2016487419605255\n",
      "epoch: 4, batch: 418, loss: 0.2212054431438446\n",
      "epoch: 4, batch: 419, loss: 0.20513048768043518\n",
      "epoch: 4, batch: 420, loss: 0.19915135204792023\n",
      "epoch: 4, batch: 421, loss: 0.24693529307842255\n",
      "epoch: 4, batch: 422, loss: 0.13174396753311157\n",
      "epoch: 4, batch: 423, loss: 0.37667569518089294\n",
      "epoch: 4, batch: 424, loss: 0.14807790517807007\n",
      "epoch: 4, batch: 425, loss: 0.3308202028274536\n",
      "epoch: 4, batch: 426, loss: 0.30001264810562134\n",
      "epoch: 4, batch: 427, loss: 0.26927927136421204\n",
      "epoch: 4, batch: 428, loss: 0.18210504949092865\n",
      "epoch: 4, batch: 429, loss: 0.06351159512996674\n",
      "epoch: 4, batch: 430, loss: 0.38075968623161316\n",
      "epoch: 4, batch: 431, loss: 0.22310423851013184\n",
      "epoch: 4, batch: 432, loss: 0.3465735614299774\n",
      "epoch: 4, batch: 433, loss: 0.14162878692150116\n",
      "epoch: 4, batch: 434, loss: 0.3659161925315857\n",
      "epoch: 4, batch: 435, loss: 0.1397494375705719\n",
      "epoch: 4, batch: 436, loss: 0.17930598556995392\n",
      "epoch: 4, batch: 437, loss: 0.31289538741111755\n",
      "epoch: 4, batch: 438, loss: 0.14416755735874176\n",
      "epoch: 4, batch: 439, loss: 0.3967961370944977\n",
      "epoch: 4, batch: 440, loss: 0.24383236467838287\n",
      "epoch: 4, batch: 441, loss: 0.22347836196422577\n",
      "epoch: 4, batch: 442, loss: 0.22093161940574646\n",
      "epoch: 4, batch: 443, loss: 0.28707560896873474\n",
      "epoch: 4, batch: 444, loss: 0.1638825535774231\n",
      "epoch: 4, batch: 445, loss: 0.24392753839492798\n",
      "epoch: 4, batch: 446, loss: 0.12047912925481796\n",
      "epoch: 4, batch: 447, loss: 0.2999891936779022\n",
      "epoch: 4, batch: 448, loss: 0.2224372923374176\n",
      "epoch: 4, batch: 449, loss: 0.2377808839082718\n",
      "epoch: 4, batch: 450, loss: 0.2573404312133789\n",
      "epoch: 4, batch: 451, loss: 0.1649760752916336\n",
      "epoch: 4, batch: 452, loss: 0.0951102152466774\n",
      "epoch: 4, batch: 453, loss: 0.2668408751487732\n",
      "epoch: 4, batch: 454, loss: 0.33747631311416626\n",
      "epoch: 4, batch: 455, loss: 0.24027147889137268\n",
      "epoch: 4, batch: 456, loss: 0.13800044357776642\n",
      "epoch: 4, batch: 457, loss: 0.14672113955020905\n",
      "epoch: 4, batch: 458, loss: 0.1873065084218979\n",
      "epoch: 4, batch: 459, loss: 0.20472504198551178\n",
      "epoch: 4, batch: 460, loss: 0.24874752759933472\n",
      "epoch: 4, batch: 461, loss: 0.16692714393138885\n",
      "epoch: 4, batch: 462, loss: 0.1427161544561386\n",
      "epoch: 4, batch: 463, loss: 0.24386365711688995\n",
      "epoch: 4, batch: 464, loss: 0.16314545273780823\n",
      "epoch: 4, batch: 465, loss: 0.25488707423210144\n",
      "epoch: 4, batch: 466, loss: 0.17725150287151337\n",
      "epoch: 4, batch: 467, loss: 0.4551455080509186\n",
      "epoch: 4, batch: 468, loss: 0.25988465547561646\n",
      "epoch: 4, batch: 469, loss: 0.1960204541683197\n",
      "epoch: 4, batch: 470, loss: 0.18730215728282928\n",
      "epoch: 4, batch: 471, loss: 0.19198863208293915\n",
      "epoch: 4, batch: 472, loss: 0.29194188117980957\n",
      "epoch: 4, batch: 473, loss: 0.26791054010391235\n",
      "epoch: 4, batch: 474, loss: 0.1779499351978302\n",
      "epoch: 4, batch: 475, loss: 0.1510612666606903\n",
      "epoch: 4, batch: 476, loss: 0.2780933678150177\n",
      "epoch: 4, batch: 477, loss: 0.205608069896698\n",
      "epoch: 4, batch: 478, loss: 0.1979900747537613\n",
      "epoch: 4, batch: 479, loss: 0.20012103021144867\n",
      "epoch: 4, batch: 480, loss: 0.10847990959882736\n",
      "epoch: 4, batch: 481, loss: 0.2773078382015228\n",
      "epoch: 4, batch: 482, loss: 0.15890270471572876\n",
      "epoch: 4, batch: 483, loss: 0.2928585708141327\n",
      "epoch: 4, batch: 484, loss: 0.26997706294059753\n",
      "epoch: 4, batch: 485, loss: 0.26085543632507324\n",
      "epoch: 4, batch: 486, loss: 0.15552085638046265\n",
      "epoch: 4, batch: 487, loss: 0.3705506920814514\n",
      "epoch: 4, batch: 488, loss: 0.346881240606308\n",
      "epoch: 4, batch: 489, loss: 0.13966986536979675\n",
      "epoch: 4, batch: 490, loss: 0.5806988477706909\n",
      "epoch: 4, batch: 491, loss: 0.4329119920730591\n",
      "epoch: 4, batch: 492, loss: 0.33442816138267517\n",
      "epoch: 4, batch: 493, loss: 0.3980047404766083\n",
      "epoch: 4, batch: 494, loss: 0.31393522024154663\n",
      "epoch: 4, batch: 495, loss: 0.28093892335891724\n",
      "epoch: 4, batch: 496, loss: 0.42594215273857117\n",
      "epoch: 4, batch: 497, loss: 0.13057729601860046\n",
      "epoch: 4, batch: 498, loss: 0.23092477023601532\n",
      "epoch: 4, batch: 499, loss: 0.16242311894893646\n",
      "epoch: 4, batch: 500, loss: 0.16891780495643616\n",
      "epoch: 4, batch: 501, loss: 0.20923829078674316\n",
      "epoch: 4, batch: 502, loss: 0.145988330245018\n",
      "epoch: 4, batch: 503, loss: 0.2160130888223648\n",
      "epoch: 4, batch: 504, loss: 0.3019588887691498\n",
      "epoch: 4, batch: 505, loss: 0.1067010685801506\n",
      "epoch: 4, batch: 506, loss: 0.22495347261428833\n",
      "epoch: 4, batch: 507, loss: 0.1949775367975235\n",
      "epoch: 4, batch: 508, loss: 0.18642112612724304\n",
      "epoch: 4, batch: 509, loss: 0.08805657923221588\n",
      "epoch: 4, batch: 510, loss: 0.09337019175291061\n",
      "epoch: 4, batch: 511, loss: 0.2297305464744568\n",
      "epoch: 4, batch: 512, loss: 0.21220365166664124\n",
      "epoch: 4, batch: 513, loss: 0.22117508947849274\n",
      "epoch: 4, batch: 514, loss: 0.2511446177959442\n",
      "epoch: 4, batch: 515, loss: 0.1754349321126938\n",
      "epoch: 4, batch: 516, loss: 0.23109056055545807\n",
      "epoch: 4, batch: 517, loss: 0.20462200045585632\n",
      "epoch: 4, batch: 518, loss: 0.262959748506546\n",
      "epoch: 4, batch: 519, loss: 0.13722853362560272\n",
      "epoch: 4, batch: 520, loss: 0.07129405438899994\n",
      "epoch: 4, batch: 521, loss: 0.32457417249679565\n",
      "epoch: 4, batch: 522, loss: 0.2250213921070099\n",
      "epoch: 4, batch: 523, loss: 0.12576830387115479\n",
      "epoch: 4, batch: 524, loss: 0.23674745857715607\n",
      "epoch: 4, batch: 525, loss: 0.17112596333026886\n",
      "epoch: 4, batch: 526, loss: 0.09685917943716049\n",
      "epoch: 4, batch: 527, loss: 0.15722334384918213\n",
      "epoch: 4, batch: 528, loss: 0.17731179296970367\n",
      "epoch: 4, batch: 529, loss: 0.15314655005931854\n",
      "epoch: 4, batch: 530, loss: 0.0915374755859375\n",
      "epoch: 4, batch: 531, loss: 0.13137200474739075\n",
      "epoch: 4, batch: 532, loss: 0.1145128607749939\n",
      "epoch: 4, batch: 533, loss: 0.44685813784599304\n",
      "epoch: 4, batch: 534, loss: 0.08917320519685745\n",
      "epoch: 4, batch: 535, loss: 0.19573038816452026\n",
      "epoch: 4, batch: 536, loss: 0.13942036032676697\n",
      "epoch: 4, batch: 537, loss: 0.10599906742572784\n",
      "epoch: 4, batch: 538, loss: 0.059908024966716766\n",
      "epoch: 4, batch: 539, loss: 0.27427244186401367\n",
      "epoch: 4, batch: 540, loss: 0.10983455181121826\n",
      "epoch: 4, batch: 541, loss: 0.32648542523384094\n",
      "epoch: 4, batch: 542, loss: 0.22532348334789276\n",
      "epoch: 4, batch: 543, loss: 0.13834315538406372\n",
      "epoch: 4, batch: 544, loss: 0.4473510682582855\n",
      "epoch: 4, batch: 545, loss: 0.288135290145874\n",
      "epoch: 4, batch: 546, loss: 0.31651031970977783\n",
      "epoch: 4, batch: 547, loss: 0.3462999761104584\n",
      "epoch: 4, batch: 548, loss: 0.12793803215026855\n",
      "epoch: 4, batch: 549, loss: 0.4111427366733551\n",
      "epoch: 4, batch: 550, loss: 0.11743562668561935\n",
      "epoch: 4, batch: 551, loss: 0.2398424744606018\n",
      "epoch: 4, batch: 552, loss: 0.31126388907432556\n",
      "epoch: 4, batch: 553, loss: 0.2789433002471924\n",
      "epoch: 4, batch: 554, loss: 0.14905138313770294\n",
      "epoch: 4, batch: 555, loss: 0.27619925141334534\n",
      "epoch: 4, batch: 556, loss: 0.17765136063098907\n",
      "epoch: 4, batch: 557, loss: 0.2105766087770462\n",
      "epoch: 4, batch: 558, loss: 0.1905447244644165\n",
      "epoch: 4, batch: 559, loss: 0.09111616015434265\n",
      "epoch: 4, batch: 560, loss: 0.29799890518188477\n",
      "epoch: 4, batch: 561, loss: 0.3253166079521179\n",
      "epoch: 4, batch: 562, loss: 0.12200108915567398\n",
      "epoch: 4, batch: 563, loss: 0.21039488911628723\n",
      "epoch: 4, batch: 564, loss: 0.30877044796943665\n",
      "epoch: 4, batch: 565, loss: 0.0803518295288086\n",
      "epoch: 4, batch: 566, loss: 0.22328083217144012\n",
      "epoch: 4, batch: 567, loss: 0.10070091485977173\n",
      "epoch: 4, batch: 568, loss: 0.20126789808273315\n",
      "epoch: 4, batch: 569, loss: 0.17037126421928406\n",
      "epoch: 4, batch: 570, loss: 0.3185921311378479\n",
      "epoch: 4, batch: 571, loss: 0.18871955573558807\n",
      "epoch: 4, batch: 572, loss: 0.22480683028697968\n",
      "epoch: 4, batch: 573, loss: 0.37272971868515015\n",
      "epoch: 4, batch: 574, loss: 0.1920490860939026\n",
      "epoch: 4, batch: 575, loss: 0.18920162320137024\n",
      "epoch: 4, batch: 576, loss: 0.14816293120384216\n",
      "epoch: 4, batch: 577, loss: 0.2815866768360138\n",
      "epoch: 4, batch: 578, loss: 0.2376450151205063\n",
      "epoch: 4, batch: 579, loss: 0.2576146125793457\n",
      "epoch: 4, batch: 580, loss: 0.20898544788360596\n",
      "epoch: 4, batch: 581, loss: 0.17475396394729614\n",
      "epoch: 4, batch: 582, loss: 0.1574677675962448\n",
      "epoch: 4, batch: 583, loss: 0.26892581582069397\n",
      "epoch: 4, batch: 584, loss: 0.3570878505706787\n",
      "epoch: 4, batch: 585, loss: 0.09717727452516556\n",
      "epoch: 4, batch: 586, loss: 0.2301344871520996\n",
      "epoch: 4, batch: 587, loss: 0.22014616429805756\n",
      "epoch: 4, batch: 588, loss: 0.20265381038188934\n",
      "epoch: 4, batch: 589, loss: 0.18804019689559937\n",
      "epoch: 4, batch: 590, loss: 0.16272494196891785\n",
      "epoch: 4, batch: 591, loss: 0.10039642453193665\n",
      "epoch: 4, batch: 592, loss: 0.1885509341955185\n",
      "epoch: 4, batch: 593, loss: 0.27108386158943176\n",
      "epoch: 4, batch: 594, loss: 0.3058813810348511\n",
      "epoch: 4, batch: 595, loss: 0.09681183099746704\n",
      "epoch: 4, batch: 596, loss: 0.27455461025238037\n",
      "epoch: 4, batch: 597, loss: 0.2003970742225647\n",
      "epoch: 4, batch: 598, loss: 0.06800376623868942\n",
      "epoch: 4, batch: 599, loss: 0.2845016121864319\n",
      "epoch: 4, batch: 600, loss: 0.1657971292734146\n",
      "epoch: 4, batch: 601, loss: 0.16373798251152039\n",
      "epoch: 4, batch: 602, loss: 0.26060643792152405\n",
      "epoch: 4, batch: 603, loss: 0.30931568145751953\n",
      "epoch: 4, batch: 604, loss: 0.2757081985473633\n",
      "epoch: 4, batch: 605, loss: 0.25808942317962646\n",
      "epoch: 4, batch: 606, loss: 0.15942250192165375\n",
      "epoch: 4, batch: 607, loss: 0.23633255064487457\n",
      "epoch: 4, batch: 608, loss: 0.17586416006088257\n",
      "epoch: 4, batch: 609, loss: 0.19232366979122162\n",
      "epoch: 4, batch: 610, loss: 0.23178227245807648\n",
      "epoch: 4, batch: 611, loss: 0.20833411812782288\n",
      "epoch: 4, batch: 612, loss: 0.32514724135398865\n",
      "epoch: 4, batch: 613, loss: 0.18239040672779083\n",
      "epoch: 4, batch: 614, loss: 0.3953561782836914\n",
      "epoch: 4, batch: 615, loss: 0.1037505492568016\n",
      "epoch: 4, batch: 616, loss: 0.22042685747146606\n",
      "epoch: 4, batch: 617, loss: 0.39362090826034546\n",
      "epoch: 4, batch: 618, loss: 0.4383959174156189\n",
      "epoch: 4, batch: 619, loss: 0.08022851496934891\n",
      "epoch: 4, batch: 620, loss: 0.12861496210098267\n",
      "epoch: 4, batch: 621, loss: 0.11688510328531265\n",
      "epoch: 4, batch: 622, loss: 0.07676704227924347\n",
      "epoch: 4, batch: 623, loss: 0.18460071086883545\n",
      "epoch: 4, batch: 624, loss: 0.2681354880332947\n",
      "epoch: 4, batch: 625, loss: 0.39109522104263306\n",
      "epoch: 4, batch: 626, loss: 0.2964710295200348\n",
      "epoch: 4, batch: 627, loss: 0.14192883670330048\n",
      "epoch: 4, batch: 628, loss: 0.3594123423099518\n",
      "epoch: 4, batch: 629, loss: 0.28904208540916443\n",
      "epoch: 4, batch: 630, loss: 0.25456157326698303\n",
      "epoch: 4, batch: 631, loss: 0.2190311998128891\n",
      "epoch: 4, batch: 632, loss: 0.1918635368347168\n",
      "epoch: 4, batch: 633, loss: 0.3815844655036926\n",
      "epoch: 4, batch: 634, loss: 0.26948702335357666\n",
      "epoch: 4, batch: 635, loss: 0.18984295427799225\n",
      "epoch: 4, batch: 636, loss: 0.1875835657119751\n",
      "epoch: 4, batch: 637, loss: 0.22492529451847076\n",
      "epoch: 4, batch: 638, loss: 0.14690174162387848\n",
      "epoch: 4, batch: 639, loss: 0.1943163126707077\n",
      "epoch: 4, batch: 640, loss: 0.25732043385505676\n",
      "epoch: 4, batch: 641, loss: 0.24136744439601898\n",
      "epoch: 4, batch: 642, loss: 0.18612167239189148\n",
      "epoch: 4, batch: 643, loss: 0.18134967982769012\n",
      "epoch: 4, batch: 644, loss: 0.23719818890094757\n",
      "epoch: 4, batch: 645, loss: 0.22825223207473755\n",
      "epoch: 4, batch: 646, loss: 0.14496628940105438\n",
      "epoch: 4, batch: 647, loss: 0.22658735513687134\n",
      "epoch: 4, batch: 648, loss: 0.2254541665315628\n",
      "epoch: 4, batch: 649, loss: 0.20152586698532104\n",
      "epoch: 4, batch: 650, loss: 0.07336893677711487\n",
      "epoch: 4, batch: 651, loss: 0.19157998263835907\n",
      "epoch: 4, batch: 652, loss: 0.16685830056667328\n",
      "epoch: 4, batch: 653, loss: 0.13978946208953857\n",
      "epoch: 4, batch: 654, loss: 0.130446657538414\n",
      "epoch: 4, batch: 655, loss: 0.12558016180992126\n",
      "epoch: 4, batch: 656, loss: 0.25664767622947693\n",
      "epoch: 4, batch: 657, loss: 0.34148165583610535\n",
      "epoch: 4, batch: 658, loss: 0.2441735863685608\n",
      "epoch: 4, batch: 659, loss: 0.1885402500629425\n",
      "epoch: 4, batch: 660, loss: 0.12130436301231384\n",
      "epoch: 4, batch: 661, loss: 0.27237778902053833\n",
      "epoch: 4, batch: 662, loss: 0.06781501322984695\n",
      "epoch: 4, batch: 663, loss: 0.4698977470397949\n",
      "epoch: 4, batch: 664, loss: 0.24861805140972137\n",
      "epoch: 4, batch: 665, loss: 0.3148590326309204\n",
      "epoch: 4, batch: 666, loss: 0.18187259137630463\n",
      "epoch: 4, batch: 667, loss: 0.13789990544319153\n",
      "epoch: 4, batch: 668, loss: 0.32037729024887085\n",
      "epoch: 4, batch: 669, loss: 0.308919221162796\n",
      "epoch: 4, batch: 670, loss: 0.1186077743768692\n",
      "epoch: 4, batch: 671, loss: 0.24094431102275848\n",
      "epoch: 4, batch: 672, loss: 0.36645689606666565\n",
      "epoch: 4, batch: 673, loss: 0.22404596209526062\n",
      "epoch: 4, batch: 674, loss: 0.1860680878162384\n",
      "epoch: 4, batch: 675, loss: 0.41643014550209045\n",
      "epoch: 4, batch: 676, loss: 0.20049838721752167\n",
      "epoch: 4, batch: 677, loss: 0.29481518268585205\n",
      "epoch: 4, batch: 678, loss: 0.1403600126504898\n",
      "epoch: 4, batch: 679, loss: 0.07886219769716263\n",
      "epoch: 4, batch: 680, loss: 0.12797223031520844\n",
      "epoch: 4, batch: 681, loss: 0.20148274302482605\n",
      "epoch: 4, batch: 682, loss: 0.11951901763677597\n",
      "epoch: 4, batch: 683, loss: 0.18113121390342712\n",
      "epoch: 4, batch: 684, loss: 0.09949926286935806\n",
      "epoch: 4, batch: 685, loss: 0.16536076366901398\n",
      "epoch: 4, batch: 686, loss: 0.16173532605171204\n",
      "epoch: 4, batch: 687, loss: 0.23822005093097687\n",
      "epoch: 4, batch: 688, loss: 0.10325814038515091\n",
      "epoch: 4, batch: 689, loss: 0.3942547142505646\n",
      "epoch: 4, batch: 690, loss: 0.14620016515254974\n",
      "epoch: 4, batch: 691, loss: 0.08700451254844666\n",
      "epoch: 4, batch: 692, loss: 0.18480752408504486\n",
      "epoch: 4, batch: 693, loss: 0.22895929217338562\n",
      "epoch: 4, batch: 694, loss: 0.07893773168325424\n",
      "epoch: 4, batch: 695, loss: 0.13906988501548767\n",
      "epoch: 4, batch: 696, loss: 0.14290542900562286\n",
      "epoch: 4, batch: 697, loss: 0.12475550919771194\n",
      "epoch: 4, batch: 698, loss: 0.23636944591999054\n",
      "epoch: 4, batch: 699, loss: 0.2461780160665512\n",
      "epoch: 4, batch: 700, loss: 0.17605698108673096\n",
      "epoch: 4, batch: 701, loss: 0.1484859138727188\n",
      "epoch: 4, batch: 702, loss: 0.24085573852062225\n",
      "epoch: 4, batch: 703, loss: 0.25583207607269287\n",
      "epoch: 4, batch: 704, loss: 0.33804866671562195\n",
      "epoch: 4, batch: 705, loss: 0.4147856533527374\n",
      "epoch: 4, batch: 706, loss: 0.20493078231811523\n",
      "epoch: 4, batch: 707, loss: 0.10005197674036026\n",
      "epoch: 4, batch: 708, loss: 0.22941480576992035\n",
      "epoch: 4, batch: 709, loss: 0.10601301491260529\n",
      "epoch: 4, batch: 710, loss: 0.34835731983184814\n",
      "epoch: 4, batch: 711, loss: 0.16808976233005524\n",
      "epoch: 4, batch: 712, loss: 0.15450458228588104\n",
      "epoch: 4, batch: 713, loss: 0.10945723950862885\n",
      "epoch: 4, batch: 714, loss: 0.0903237834572792\n",
      "epoch: 4, batch: 715, loss: 0.5041165947914124\n",
      "epoch: 4, batch: 716, loss: 0.14507374167442322\n",
      "epoch: 4, batch: 717, loss: 0.18443207442760468\n",
      "epoch: 4, batch: 718, loss: 0.19757989048957825\n",
      "epoch: 4, batch: 719, loss: 0.1422727406024933\n",
      "epoch: 4, batch: 720, loss: 0.1881788820028305\n",
      "epoch: 4, batch: 721, loss: 0.125112384557724\n",
      "epoch: 4, batch: 722, loss: 0.15683546662330627\n",
      "epoch: 4, batch: 723, loss: 0.18317179381847382\n",
      "epoch: 4, batch: 724, loss: 0.13897013664245605\n",
      "epoch: 4, batch: 725, loss: 0.1556420624256134\n",
      "epoch: 4, batch: 726, loss: 0.2412157952785492\n",
      "epoch: 4, batch: 727, loss: 0.2009495198726654\n",
      "epoch: 4, batch: 728, loss: 0.0713425949215889\n",
      "epoch: 4, batch: 729, loss: 0.11303568631410599\n",
      "epoch: 4, batch: 730, loss: 0.404156357049942\n",
      "epoch: 4, batch: 731, loss: 0.09646394848823547\n",
      "epoch: 4, batch: 732, loss: 0.1941281110048294\n",
      "epoch: 4, batch: 733, loss: 0.2221355438232422\n",
      "epoch: 4, batch: 734, loss: 0.0751371756196022\n",
      "epoch: 4, batch: 735, loss: 0.18797197937965393\n",
      "epoch: 4, batch: 736, loss: 0.16517041623592377\n",
      "epoch: 4, batch: 737, loss: 0.06277503818273544\n",
      "epoch: 4, batch: 738, loss: 0.2509510815143585\n",
      "epoch: 4, batch: 739, loss: 0.09792780131101608\n",
      "epoch: 4, batch: 740, loss: 0.17109788954257965\n",
      "epoch: 4, batch: 741, loss: 0.2891078591346741\n",
      "epoch: 4, batch: 742, loss: 0.34187597036361694\n",
      "epoch: 4, batch: 743, loss: 0.10834260284900665\n",
      "epoch: 4, batch: 744, loss: 0.13676901161670685\n",
      "epoch: 4, batch: 745, loss: 0.3642580211162567\n",
      "epoch: 4, batch: 746, loss: 0.1188332661986351\n",
      "epoch: 4, batch: 747, loss: 0.178087055683136\n",
      "epoch: 4, batch: 748, loss: 0.06829400360584259\n",
      "epoch: 4, batch: 749, loss: 0.08726522326469421\n",
      "epoch: 4, batch: 750, loss: 0.1619575470685959\n",
      "epoch: 4, batch: 751, loss: 0.20597825944423676\n",
      "epoch: 4, batch: 752, loss: 0.23026028275489807\n",
      "epoch: 4, batch: 753, loss: 0.14184898138046265\n",
      "epoch: 4, batch: 754, loss: 0.3826037645339966\n",
      "epoch: 4, batch: 755, loss: 0.05412127822637558\n",
      "epoch: 4, batch: 756, loss: 0.18450629711151123\n",
      "epoch: 4, batch: 757, loss: 0.08487191051244736\n",
      "epoch: 4, batch: 758, loss: 0.09633838385343552\n",
      "epoch: 4, batch: 759, loss: 0.16885225474834442\n",
      "epoch: 4, batch: 760, loss: 0.22665652632713318\n",
      "epoch: 4, batch: 761, loss: 0.11645101010799408\n",
      "epoch: 4, batch: 762, loss: 0.0947861298918724\n",
      "epoch: 4, batch: 763, loss: 0.3153424859046936\n",
      "epoch: 4, batch: 764, loss: 0.23103570938110352\n",
      "epoch: 4, batch: 765, loss: 0.06312908232212067\n",
      "epoch: 4, batch: 766, loss: 0.29422813653945923\n",
      "epoch: 4, batch: 767, loss: 0.23638883233070374\n",
      "epoch: 4, batch: 768, loss: 0.19971269369125366\n",
      "epoch: 4, batch: 769, loss: 0.2330624759197235\n",
      "epoch: 4, batch: 770, loss: 0.17568063735961914\n",
      "epoch: 4, batch: 771, loss: 0.2509380280971527\n",
      "epoch: 4, batch: 772, loss: 0.15447120368480682\n",
      "epoch: 4, batch: 773, loss: 0.17776642739772797\n",
      "epoch: 4, batch: 774, loss: 0.24766536056995392\n",
      "epoch: 4, batch: 775, loss: 0.20400266349315643\n",
      "epoch: 4, batch: 776, loss: 0.23299969732761383\n",
      "epoch: 4, batch: 777, loss: 0.24918881058692932\n",
      "epoch: 4, batch: 778, loss: 0.3484826982021332\n",
      "epoch: 4, batch: 779, loss: 0.20632410049438477\n",
      "epoch: 4, batch: 780, loss: 0.18199749290943146\n",
      "epoch: 4, batch: 781, loss: 0.20527620613574982\n",
      "epoch: 4, batch: 782, loss: 0.3230772018432617\n",
      "epoch: 4, batch: 783, loss: 0.20560301840305328\n",
      "epoch: 4, batch: 784, loss: 0.1893870085477829\n",
      "epoch: 4, batch: 785, loss: 0.27739816904067993\n",
      "epoch: 4, batch: 786, loss: 0.15411154925823212\n",
      "epoch: 4, batch: 787, loss: 0.17197217047214508\n",
      "epoch: 4, batch: 788, loss: 0.21244342625141144\n",
      "epoch: 4, batch: 789, loss: 0.09933803230524063\n",
      "epoch: 4, batch: 790, loss: 0.12172510474920273\n",
      "epoch: 4, batch: 791, loss: 0.08638465404510498\n",
      "epoch: 4, batch: 792, loss: 0.26467442512512207\n",
      "epoch: 4, batch: 793, loss: 0.26278990507125854\n",
      "epoch: 4, batch: 794, loss: 0.1858046054840088\n",
      "epoch: 4, batch: 795, loss: 0.15005911886692047\n",
      "epoch: 4, batch: 796, loss: 0.06666891276836395\n",
      "epoch: 4, batch: 797, loss: 0.09796904027462006\n",
      "epoch: 4, batch: 798, loss: 0.09168137609958649\n",
      "epoch: 4, batch: 799, loss: 0.17183619737625122\n",
      "epoch: 4, batch: 800, loss: 0.251202791929245\n",
      "epoch: 4, batch: 801, loss: 0.22946804761886597\n",
      "epoch: 4, batch: 802, loss: 0.2495318055152893\n",
      "epoch: 4, batch: 803, loss: 0.1372276097536087\n",
      "epoch: 4, batch: 804, loss: 0.2515103220939636\n",
      "epoch: 4, batch: 805, loss: 0.15491245687007904\n",
      "epoch: 4, batch: 806, loss: 0.19794557988643646\n",
      "epoch: 4, batch: 807, loss: 0.3739033341407776\n",
      "epoch: 4, batch: 808, loss: 0.09835712611675262\n",
      "epoch: 4, batch: 809, loss: 0.22866612672805786\n",
      "epoch: 4, batch: 810, loss: 0.11547374725341797\n",
      "epoch: 4, batch: 811, loss: 0.14534007012844086\n",
      "epoch: 4, batch: 812, loss: 0.2812756299972534\n",
      "epoch: 4, batch: 813, loss: 0.2740599513053894\n",
      "epoch: 4, batch: 814, loss: 0.31975695490837097\n",
      "epoch: 4, batch: 815, loss: 0.1965065896511078\n",
      "epoch: 4, batch: 816, loss: 0.23559904098510742\n",
      "epoch: 4, batch: 817, loss: 0.28161364793777466\n",
      "epoch: 4, batch: 818, loss: 0.19345879554748535\n",
      "epoch: 4, batch: 819, loss: 0.23549818992614746\n",
      "epoch: 4, batch: 820, loss: 0.07766203582286835\n",
      "epoch: 4, batch: 821, loss: 0.07394031435251236\n",
      "epoch: 4, batch: 822, loss: 0.29196807742118835\n",
      "epoch: 4, batch: 823, loss: 0.06463424116373062\n",
      "epoch: 4, batch: 824, loss: 0.3275667130947113\n",
      "epoch: 4, batch: 825, loss: 0.18026778101921082\n",
      "epoch: 4, batch: 826, loss: 0.3610883355140686\n",
      "epoch: 4, batch: 827, loss: 0.2629857063293457\n",
      "epoch: 4, batch: 828, loss: 0.20697708427906036\n",
      "epoch: 4, batch: 829, loss: 0.20509015023708344\n",
      "epoch: 4, batch: 830, loss: 0.3492284119129181\n",
      "epoch: 4, batch: 831, loss: 0.24705758690834045\n",
      "epoch: 4, batch: 832, loss: 0.11739533394575119\n",
      "epoch: 4, batch: 833, loss: 0.1649911105632782\n",
      "epoch: 4, batch: 834, loss: 0.28670167922973633\n",
      "epoch: 4, batch: 835, loss: 0.22862231731414795\n",
      "epoch: 4, batch: 836, loss: 0.2303357571363449\n",
      "epoch: 4, batch: 837, loss: 0.14832960069179535\n",
      "epoch: 4, batch: 838, loss: 0.2754826247692108\n",
      "epoch: 4, batch: 839, loss: 0.22758589684963226\n",
      "epoch: 4, batch: 840, loss: 0.20428216457366943\n",
      "epoch: 4, batch: 841, loss: 0.15663695335388184\n",
      "epoch: 4, batch: 842, loss: 0.1720963567495346\n",
      "epoch: 4, batch: 843, loss: 0.23545020818710327\n",
      "epoch: 4, batch: 844, loss: 0.12730145454406738\n",
      "epoch: 4, batch: 845, loss: 0.3858529031276703\n",
      "epoch: 4, batch: 846, loss: 0.10490389168262482\n",
      "epoch: 4, batch: 847, loss: 0.12229440361261368\n",
      "epoch: 4, batch: 848, loss: 0.20669418573379517\n",
      "epoch: 4, batch: 849, loss: 0.2160431146621704\n",
      "epoch: 4, batch: 850, loss: 0.10499696433544159\n",
      "epoch: 4, batch: 851, loss: 0.46438321471214294\n",
      "epoch: 4, batch: 852, loss: 0.19678790867328644\n",
      "epoch: 4, batch: 853, loss: 0.11496355384588242\n",
      "epoch: 4, batch: 854, loss: 0.24043220281600952\n",
      "epoch: 4, batch: 855, loss: 0.09292951971292496\n",
      "epoch: 4, batch: 856, loss: 0.06741738319396973\n",
      "epoch: 4, batch: 857, loss: 0.13796086609363556\n",
      "epoch: 4, batch: 858, loss: 0.11559046804904938\n",
      "epoch: 4, batch: 859, loss: 0.17837736010551453\n",
      "epoch: 4, batch: 860, loss: 0.23697634041309357\n",
      "epoch: 4, batch: 861, loss: 0.05834096670150757\n",
      "epoch: 4, batch: 862, loss: 0.17641998827457428\n",
      "epoch: 4, batch: 863, loss: 0.2395298182964325\n",
      "epoch: 4, batch: 864, loss: 0.1591240018606186\n",
      "epoch: 4, batch: 865, loss: 0.3942951261997223\n",
      "epoch: 4, batch: 866, loss: 0.29798752069473267\n",
      "epoch: 4, batch: 867, loss: 0.19710487127304077\n",
      "epoch: 4, batch: 868, loss: 0.36910369992256165\n",
      "epoch: 4, batch: 869, loss: 0.1498742401599884\n",
      "epoch: 4, batch: 870, loss: 0.15936633944511414\n",
      "epoch: 4, batch: 871, loss: 0.27865299582481384\n",
      "epoch: 4, batch: 872, loss: 0.1452820599079132\n",
      "epoch: 4, batch: 873, loss: 0.2231980264186859\n",
      "epoch: 4, batch: 874, loss: 0.1363592892885208\n",
      "epoch: 4, batch: 875, loss: 0.2686697840690613\n",
      "epoch: 4, batch: 876, loss: 0.16058072447776794\n",
      "epoch: 4, batch: 877, loss: 0.07946104556322098\n",
      "epoch: 4, batch: 878, loss: 0.19365936517715454\n",
      "epoch: 4, batch: 879, loss: 0.2411784827709198\n",
      "epoch: 4, batch: 880, loss: 0.19808614253997803\n",
      "epoch: 4, batch: 881, loss: 0.22099295258522034\n",
      "epoch: 4, batch: 882, loss: 0.21517083048820496\n",
      "epoch: 4, batch: 883, loss: 0.10555762052536011\n",
      "epoch: 4, batch: 884, loss: 0.12433043122291565\n",
      "epoch: 4, batch: 885, loss: 0.4375828504562378\n",
      "epoch: 4, batch: 886, loss: 0.1405998170375824\n",
      "epoch: 4, batch: 887, loss: 0.20395010709762573\n",
      "epoch: 4, batch: 888, loss: 0.0731470063328743\n",
      "epoch: 4, batch: 889, loss: 0.03489148989319801\n",
      "epoch: 4, batch: 890, loss: 0.2578999996185303\n",
      "epoch: 4, batch: 891, loss: 0.14965090155601501\n",
      "epoch: 4, batch: 892, loss: 0.22833199799060822\n",
      "epoch: 4, batch: 893, loss: 0.13133229315280914\n",
      "epoch: 4, batch: 894, loss: 0.16352348029613495\n",
      "epoch: 4, batch: 895, loss: 0.2952776253223419\n",
      "epoch: 4, batch: 896, loss: 0.33732539415359497\n",
      "epoch: 4, batch: 897, loss: 0.2621939778327942\n",
      "epoch: 4, batch: 898, loss: 0.2777126729488373\n",
      "epoch: 4, batch: 899, loss: 0.13744023442268372\n",
      "epoch: 4, batch: 900, loss: 0.08744646608829498\n",
      "epoch: 4, batch: 901, loss: 0.11132010817527771\n",
      "epoch: 4, batch: 902, loss: 0.26254090666770935\n",
      "epoch: 4, batch: 903, loss: 0.2614600956439972\n",
      "epoch: 4, batch: 904, loss: 0.14924302697181702\n",
      "epoch: 4, batch: 905, loss: 0.1065797358751297\n",
      "epoch: 4, batch: 906, loss: 0.26147228479385376\n",
      "epoch: 4, batch: 907, loss: 0.3192794919013977\n",
      "epoch: 4, batch: 908, loss: 0.1154051274061203\n",
      "epoch: 4, batch: 909, loss: 0.20064173638820648\n",
      "epoch: 4, batch: 910, loss: 0.22335785627365112\n",
      "epoch: 4, batch: 911, loss: 0.12137646228075027\n",
      "epoch: 4, batch: 912, loss: 0.10196282714605331\n",
      "epoch: 4, batch: 913, loss: 0.1244928166270256\n",
      "epoch: 4, batch: 914, loss: 0.17908036708831787\n",
      "epoch: 4, batch: 915, loss: 0.2882879972457886\n",
      "epoch: 4, batch: 916, loss: 0.21045935153961182\n",
      "epoch: 4, batch: 917, loss: 0.3902316987514496\n",
      "epoch: 4, batch: 918, loss: 0.2257775515317917\n",
      "epoch: 4, batch: 919, loss: 0.3008563220500946\n",
      "epoch: 4, batch: 920, loss: 0.31191644072532654\n",
      "epoch: 4, batch: 921, loss: 0.2642422318458557\n",
      "epoch: 4, batch: 922, loss: 0.07271239906549454\n",
      "epoch: 4, batch: 923, loss: 0.08687742054462433\n",
      "epoch: 4, batch: 924, loss: 0.21381473541259766\n",
      "epoch: 4, batch: 925, loss: 0.10344453155994415\n",
      "epoch: 4, batch: 926, loss: 0.11783242225646973\n",
      "epoch: 4, batch: 927, loss: 0.25632694363594055\n",
      "epoch: 4, batch: 928, loss: 0.1874755173921585\n",
      "epoch: 4, batch: 929, loss: 0.1997448205947876\n",
      "epoch: 4, batch: 930, loss: 0.2956128716468811\n",
      "epoch: 4, batch: 931, loss: 0.14149174094200134\n",
      "epoch: 4, batch: 932, loss: 0.19921627640724182\n",
      "epoch: 4, batch: 933, loss: 0.1893577128648758\n",
      "epoch: 4, batch: 934, loss: 0.19132372736930847\n",
      "epoch: 4, batch: 935, loss: 0.3737030625343323\n",
      "epoch: 4, batch: 936, loss: 0.1964137852191925\n",
      "epoch: 4, batch: 937, loss: 0.21970359981060028\n",
      "CPU times: total: 1min 3s\n",
      "Wall time: 58.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# get model to cuda if possible\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, target) in enumerate((train_datloader)):\n",
    "\n",
    "        # get data to cuda if possible\n",
    "        data = data.to(device)\n",
    "        # Squeeze the dim of data from (B X 1 X 28 X 28) to (B X 28 X 28)\n",
    "        data = data.squeeze(dim=1)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # feed forward the data to model\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, target)\n",
    "        print(f\"epoch: {epoch}, batch: {batch_idx}, loss: {loss}\")\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad() # This will flush the gradients from the last iteration\n",
    "        loss.backward()\n",
    "\n",
    "        # optimise the loss (gradient descent or Adam step)\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check accuracy on train and test data (Validate model accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy on training & test to see how good our model\n",
    "def check_accuracy(loader, model):\n",
    "    \"\"\"\n",
    "    Check accuracy of our trained model given a loader and a model\n",
    "\n",
    "    Parameters:\n",
    "        loader: torch.utils.data.DataLoader\n",
    "            A loader for the dataset you want to check accuracy on\n",
    "        model: nn.Module\n",
    "            The model you want to check accuracy on\n",
    "\n",
    "    Returns:\n",
    "        acc: float\n",
    "            The accuracy of the model on the dataset given by the loader\n",
    "    \"\"\"\n",
    "\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "\n",
    "    # We don't need to keep track of gradients here so we wrap it in torch.no_grad()\n",
    "    with torch.no_grad():\n",
    "        # Loop through the data\n",
    "        for x, y in loader:\n",
    "            # Move data to device\n",
    "            x = x.to(device=device)\n",
    "            # Squeeze the dim of x from (B X 1 X 28 X 28) to (B X 28 X 28)\n",
    "            x = x.squeeze(dim=1)\n",
    "            y = y.to(device=device)\n",
    "\n",
    "            # Forward pass\n",
    "            scores = model(x)\n",
    "            predictions = scores.argmax(1)\n",
    "\n",
    "            # Check how many we got correct\n",
    "            num_correct += (predictions == y).sum()\n",
    "\n",
    "            # Keep track of number of samples\n",
    "            num_samples += predictions.size(0)\n",
    "\n",
    "    model.train()\n",
    "    return num_correct / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 93.92\n",
      "Accuracy on test set: 93.88\n"
     ]
    }
   ],
   "source": [
    "# Check accuracy on training & test to see how good our model\n",
    "print(f\"Accuracy on training set: {check_accuracy(train_datloader, model)*100:.2f}\")\n",
    "print(f\"Accuracy on test set: {check_accuracy(test_datloader, model)*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
